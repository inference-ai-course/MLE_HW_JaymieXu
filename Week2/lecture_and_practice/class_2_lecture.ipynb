{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Practice: From Transformers to Alignment\n",
    "### Learning Objectives:\n",
    "* Understand attention mechanisms through NumPy code\n",
    "* Build a simple transformer block\n",
    "* Predict next token using a pretrained LLM\n",
    "* Analyze hallucinations\n",
    "* Explore supervised fine-tuning logic\n",
    "* Understand how DPO works via preference modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Package Introduction\n",
    "\n",
    "In this notebook, we will use several important Python libraries:\n",
    "\n",
    "- **[NumPy](https://education.launchcode.org/data-analysis-curriculum/eda-with-pandas/reading/numpy-intro/index.html?utm_term=launchcode&utm_campaign=&utm_source=bing&utm_medium=ppc&hsa_acc=4368208516&hsa_cam=568518766&hsa_grp=1173180668353233&hsa_ad=&hsa_src=o&hsa_tgt=dat-2325123495982042:loc-190&hsa_kw=launchcode&hsa_mt=b&hsa_net=adwords&hsa_ver=3&msclkid=f69fad9bed3f18d44e31c5a6703d580b&utm_content=Group%202)**: The fundamental package for scientific computing with Python. We use it for matrix operations and to demonstrate the attention mechanism.\n",
    "- **[PyTorch](https://www.geeksforgeeks.org/start-learning-pytorch-for-beginners/)**: A popular deep learning framework. We use it to build and train neural network models, including transformer blocks.\n",
    "- **[Hugging Face Transformers](https://github.com/huggingface/transformers)**: Provides state-of-the-art pre-trained models and tools for natural language processing. We use it to load and interact with large language models (LLMs).\n",
    "- **[huggingface-cli](https://huggingface.co/docs/huggingface_hub/main/en/guides/cli)**: A command-line tool for managing Hugging Face models and datasets. Useful for downloading models or checking your authentication.\n",
    "\n",
    "Make sure you have these packages installed. You can install them using pip if needed:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mDEPRECATION: Loading egg at /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages/python_autocite-0.0.4-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0mRequirement already satisfied: numpy in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: torch in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (2.6.0)\n",
      "Requirement already satisfied: transformers in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (4.44.0)\n",
      "Requirement already satisfied: huggingface_hub in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (0.30.2)\n",
      "Requirement already satisfied: filelock in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from torch) (4.13.0)\n",
      "Requirement already satisfied: networkx in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from requests->transformers) (2025.7.9)\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n",
      "usage: huggingface-cli <command> [<args>]\n",
      "\n",
      "positional arguments:\n",
      "  {download,upload,repo-files,env,login,whoami,logout,auth,repo,lfs-enable-largefiles,lfs-multipart-upload,scan-cache,delete-cache,tag,version,upload-large-folder}\n",
      "                        huggingface-cli command helpers\n",
      "    download            Download files from the Hub\n",
      "    upload              Upload a file or a folder to a repo on the Hub\n",
      "    repo-files          Manage files in a repo on the Hub\n",
      "    env                 Print information about the environment.\n",
      "    login               Log in using a token from\n",
      "                        huggingface.co/settings/tokens\n",
      "    whoami              Find out which huggingface.co account you are logged\n",
      "                        in as.\n",
      "    logout              Log out\n",
      "    auth                Other authentication related commands\n",
      "    repo                {create} Commands to interact with your huggingface.co\n",
      "                        repos.\n",
      "    lfs-enable-largefiles\n",
      "                        Configure your repository to enable upload of files >\n",
      "                        5GB.\n",
      "    scan-cache          Scan cache directory.\n",
      "    delete-cache        Delete revisions from the cache directory.\n",
      "    tag                 (create, list, delete) tags for a repo in the hub\n",
      "    version             Print information about the huggingface-cli version.\n",
      "    upload-large-folder\n",
      "                        Upload a large folder to a repo on the Hub\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy torch transformers huggingface_hub\n",
    "\n",
    "# For `huggingface-cli`, it is included with `huggingface_hub` or `transformers`. You can check your installation with:\n",
    "\n",
    "! huggingface-cli --help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Attention Mechanism (Self-Attention)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:03:47.272623Z",
     "start_time": "2025-07-31T20:03:47.086222Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "# Random Q, K, V matrices\n",
    "def generate_random_qkv(seq_len=4, d_model=8):\n",
    "    return [np.random.rand(seq_len, d_model) for _ in range(3)]\n",
    "\n",
    "# Scaled dot-product attention\n",
    "def self_attention(Q, K, V):\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
    "    weights = softmax(scores)\n",
    "    output = np.dot(weights, V)\n",
    "    return output, weights\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "Q, K, V = generate_random_qkv()\n",
    "out, attn_weights = self_attention(Q, K, V)\n",
    "print(\"Attention Output:\\n\", out)\n",
    "print(\"Attention Weights:\\n\", attn_weights)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output:\n",
      " [[0.39274111 0.59418926 0.47698978 0.50069981 0.51285445 0.60852918\n",
      "  0.39332058 0.3780168 ]\n",
      " [0.36783478 0.62047584 0.45975957 0.45719961 0.53931    0.62264516\n",
      "  0.37558194 0.36647449]\n",
      " [0.34315919 0.58401885 0.48155679 0.49389943 0.50849903 0.59738132\n",
      "  0.38574058 0.37847254]\n",
      " [0.34654938 0.60201105 0.47516725 0.46919386 0.52055684 0.60331041\n",
      "  0.3765756  0.37624634]]\n",
      "Attention Weights:\n",
      " [[0.31211522 0.27484315 0.20330494 0.20973669]\n",
      " [0.37199334 0.23784345 0.19493512 0.1952281 ]\n",
      " [0.30315369 0.22066121 0.22837718 0.24780791]\n",
      " [0.3381275  0.22045625 0.20835747 0.23305878]]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Discussion: Walk students through the QK^T score computation, scaling, and softmax. Explain how this captures relationships between tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Mini Transformer Block in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:03:58.445910Z",
     "start_time": "2025-07-31T20:03:54.585388Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MiniTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads=2, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_output)\n",
    "        return x\n",
    "\n",
    "x = torch.randn(1, 5, 16)  # batch_size=1, seq_len=5, embed_dim=16\n",
    "model = MiniTransformerBlock(embed_dim=16)\n",
    "out = model(x)\n",
    "print(out.shape)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 16])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Goal: Show how self-attention and FFN work with residual and norm in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Next Token Prediction using HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: Use a Publicly Available Model\n",
    "Use a non-gated model such as TinyLlama or mistralai/Mistral-7B-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `infrenceAI` has been saved to /Users/scottlai/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /Users/scottlai/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `infrenceAI`\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Login via CLI\n",
    "! pip install --upgrade huggingface_hub[cli]\n",
    "# To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
    "! huggingface-cli login --token YOUR_HUGGINGFACE_TOKEN\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:46:20.693933Z",
     "start_time": "2025-07-31T20:45:46.460308Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def get_my_key(key_file):\n",
    "    if not os.path.exists(key_file):\n",
    "        print(f\"ERROR: '{key_file}' not found. Please create this file and put your API key in it. Or directly replace api_key=get_my_key\")\n",
    "        sys.exit(1)  # Exit with error code\n",
    "\n",
    "    with open(key_file) as f:\n",
    "        return f.read().strip()\n",
    "\n",
    "login(token=get_my_key('../../Keys/hf.txt'))\n",
    "\n",
    "# you have to visit https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1 to sign the agreement in order to use this model\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=True, device_map=\"auto\", load_in_4bit=True)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaymi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\jaymi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:999: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jaymi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:27<00:00, 13.67s/it]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using a Mac with M1/M2/M3 and have this line working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "! python -c \"import torch; print(torch.backends.mps.is_available())\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it returns True, then you can run like this:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T21:24:30.848264Z",
     "start_time": "2025-07-31T21:24:21.135097Z"
    }
   },
   "source": [
    "import sys\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "\n",
    "# Optional if already logged in via CLI\n",
    "# login(token=\"your_hf_token\")\n",
    "\n",
    "# Check device for MacBook (MPS if available, else CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"‚úÖ Using CUDA (GPU)\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"üü° Using MPS (Apple Silicon GPU)\")\n",
    "else:\n",
    "    print(\"Error: CUDA is not available. This isn't gonna happen on my pc.\")\n",
    "    print(\"Exiting...\")\n",
    "    sys.exit()\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "# Load tokenizer and model with Hugging Face gated repo access\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=True, device_map=\"auto\", quantization_config=quantization_config)\n",
    "\n",
    "# Prepare input prompt\n",
    "prompt = \"The Eiffel Tower is located in\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Run generation\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using CUDA (GPU)\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaymi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:999: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jaymi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.27s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Eiffel Tower is located in Paris, France, and is one of the most\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not sure about your GPU in your device, selects the best available device in the order: CUDA ‚Üí MPS ‚Üí CPU and also prints which one it chose:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T21:36:15.452432Z",
     "start_time": "2025-07-31T21:35:53.812269Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "\n",
    "# Optional if already logged in via CLI\n",
    "# login(token=\"your_hf_token\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# üì¶ Device Selection: CUDA > MPS > CPU\n",
    "# ------------------------------------------\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"‚úÖ Using CUDA (GPU)\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"üü° Using MPS (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"üî¥ Using CPU\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# üß† Load Model from Hugging Face\n",
    "# ------------------------------------------\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    use_auth_token=True,\n",
    "    quantization_config=quantization_config\n",
    ").to(device)\n",
    "\n",
    "# ------------------------------------------\n",
    "# üìù Prompt + Inference\n",
    "# ------------------------------------------\n",
    "prompt = \"The Eiffel Tower is located in\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "    print(\"üìù Generated Output:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using CUDA (GPU)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:18<00:00,  9.13s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Generated Output: The Eiffel Tower is located in Paris, France, and is one of the most\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: DPO vs PPO ‚Äì Side-by-Side Educational Example\n",
    "#### DPO: Direct Preference Optimization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T21:36:50.407896Z",
     "start_time": "2025-07-31T21:36:50.404487Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Simulated log-probs of chosen vs rejected completions\n",
    "chosen_logp = torch.tensor([[-1.0]])\n",
    "rejected_logp = torch.tensor([[-2.0]])\n",
    "\n",
    "# model should have more good answer than rejected ones otherwise we have a problem\n",
    "def dpo_loss(chosen_logp, rejected_logp, beta=0.1):\n",
    "    return -F.logsigmoid((chosen_logp - rejected_logp) / beta).mean()\n",
    "\n",
    "print(\"DPO Loss:\", dpo_loss(chosen_logp, rejected_logp).item())\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPO Loss: 4.5398901420412585e-05\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PPO: Proximal Policy Optimization (simplified for in-class demo)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T21:59:29.332158Z",
     "start_time": "2025-07-31T21:59:29.287952Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Simulated old and new policy log-probs (log œÄ_Œ∏(a|s) and log œÄ_Œ∏_old(a|s))\n",
    "old_log_prob = torch.tensor([[-1.0]])  # from reference policy (e.g. GPT-4 before PPO step)\n",
    "new_log_prob = torch.tensor([[-0.8]])  # from updated policy\n",
    "reward = torch.tensor([[1.0]])         # reward from human or reward model\n",
    "epsilon = 0.2                          # PPO clipping parameter\n",
    "\n",
    "# Compute ratio of new to old policy\n",
    "log_ratio = new_log_prob - old_log_prob\n",
    "ratio = torch.exp(log_ratio)\n",
    "\n",
    "# Unclipped and clipped advantages\n",
    "advantage = reward  # assume reward ~ advantage for simplicity\n",
    "clipped_ratio = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
    "\n",
    "# PPO loss (negative of the clipped surrogate objective)\n",
    "ppo_loss = -torch.min(ratio * advantage, clipped_ratio * advantage).mean()\n",
    "print(\"PPO Loss:\", ppo_loss.item())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO Loss: -1.2000000476837158\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç DPO vs PPO: Alignment Loss Comparison\n",
    "\n",
    "| Criterion         | DPO (Direct Preference Optimization)     | PPO (Proximal Policy Optimization)        |\n",
    "|------------------|-------------------------------------------|--------------------------------------------|\n",
    "| üß† Origin         | Preference modeling (UnfoldAI 2023)        | Reinforcement Learning (OpenAI 2017)       |\n",
    "| ‚úÖ Rejection Signal | Yes ‚Äî uses chosen vs rejected pairs       | No ‚Äî requires scalar reward                |\n",
    "| üèÜ Reward Signal   | Implicit via logit difference              | Explicit reward model needed               |\n",
    "| üìê Loss Function   | `-log(sigmoid((chosen - rejected)/Œ≤))`     | `-min(ratio * A, clipped_ratio * A)`       |\n",
    "| üîß Optimization    | Binary classification over preferences     | Policy gradient with clipped surrogate     |\n",
    "| üéØ Application     | DPO-tuned models like LLaMA 3              | RLHF-tuned models like InstructGPT         |\n",
    "| ‚öôÔ∏è Complexity      | Simpler (no reward model needed)           | More complex (needs reward model + sampling) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain how aligning models toward human preference uses logit differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Inference with Quantization (O1 & O3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  5.32s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run model with torch_dtype=torch.float16 for O1\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concept: Explain how FP16/O1 optimizes memory and speed at inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Concept Breakdown: How FP16 & O1 Optimize Inference\n",
    "\n",
    "#### üìå What is FP16?\n",
    "\n",
    "- **FP16** = *16-bit floating point*, also called **half precision**.\n",
    "- It uses **less memory** than FP32 (standard 32-bit float), with:\n",
    "  - 1 sign bit\n",
    "  - 5 exponent bits\n",
    "  - 10 mantissa bits\n",
    "- Typical FP32 values: `0.123456789`\n",
    "- FP16 representation: `0.1234` (lower precision but good enough for inference)\n",
    "\n",
    "---\n",
    "\n",
    "#### üìâ Why Use FP16?\n",
    "\n",
    "| Feature        | FP32                     | FP16                    |\n",
    "|----------------|--------------------------|-------------------------|\n",
    "| Memory usage   | 4 bytes per value         | 2 bytes per value       |\n",
    "| Compute speed  | Slower on GPUs            | Much faster on GPUs (especially A100/H100) |\n",
    "| Energy usage   | Higher                    | Lower                   |\n",
    "| Precision      | High                      | Slightly reduced (acceptable for inference) |\n",
    "\n",
    "üß† FP16 helps run **large models** on GPUs with limited memory (e.g., 24GB vs 80GB cards).\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚öôÔ∏è What Is O1 Optimization?\n",
    "\n",
    "`O1` is a setting from **[DeepSpeed](https://www.deepspeed.ai/)** and **[Accelerate](https://huggingface.co/docs/accelerate)** used for **mixed-precision inference/training**.\n",
    "\n",
    "| Optimization Level | Description                           |\n",
    "|--------------------|---------------------------------------|\n",
    "| O0                 | Full precision (FP32)                 |\n",
    "| **O1**             | **Mixed precision (auto FP16 + FP32 fallback)** |\n",
    "| O2                 | Pure FP16                             |\n",
    "| O3                 | Advanced optimizations (e.g., quantization, kernel fusion) |\n",
    "\n",
    "##### üîß What Does O1 Do?\n",
    "- Automatically **casts compatible operations** (like matmul) to FP16\n",
    "- **Keeps numerically sensitive ops** (e.g., layer norm, softmax) in FP32\n",
    "- Result: **Best balance** between speed and stability\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö° Benefits at Inference Time\n",
    "\n",
    "| Metric            | Before (FP32 / O0) | After (FP16 / O1) |\n",
    "|------------------|---------------------|--------------------|\n",
    "| VRAM usage       | High                | ~2x lower          |\n",
    "| Batch size limit | Smaller             | Larger             |\n",
    "| Latency          | Higher              | Lower              |\n",
    "| Throughput       | Lower               | Higher             |\n",
    "\n",
    "**Example:** Running LLaMA-7B in FP32 might require ~30GB VRAM, while FP16 can bring that down to ~16GB.\n",
    "\n",
    "---\n",
    "\n",
    "#### üí° Code Example (Hugging Face Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:20<00:00, 26.91s/it]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:11<00:00,  3.96s/it]\n",
      "/Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"openchat/openchat-3.5-1210\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # üëà Enable half-precision\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üß™ Bonus: Combine with O3\n",
    "O3 goes further with quantization, sparse attention, and custom kernels\n",
    "\n",
    "Supported by tools like DeepSpeed, vLLM, AWQ, and TensorRT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
