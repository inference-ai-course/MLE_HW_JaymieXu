[
  {
    "url": "http://arxiv.org/abs/2507.21028v1",
    "title": "Multi-Agent-as-Judge: Aligning LLM-Agent-Based Automated Evaluation with Multi-Dimensional Human Evaluation",
    "abstract": "Nearly all human work is collaborative; thus, the evaluation of\nreal-world NLP applications often requires multiple dimensions that align with diverse human\nperspectives. As real human evaluator resources are often scarce and costly, the emerging \"LLM-as-a-\nJudge” paradigm sheds light on a promising approach to leverage LLM agents to believably simulate\nhuman evaluators. Yet, to date, existing LLM-as-a-judge approaches face two limitations: persona\ndescriptions of agents are often arbitrarily designed, and the frameworks are not generalizable to\nother tasks. To address these challenges, we propose MAJ-EVAL, a Multi-Agent-as-Judge evaluation\nframework that can automatically construct multiple evaluator personas with distinct dimensions from\nrelevant text documents (e.g., research papers), instantiate LLM agents with the personas, and engage\nin-group debates with multi-agents to Generate multi-dimensional feedback. Our evaluation experiments\nin both the educational and medical domains demonstrate that MAJ-EVAL can generate evaluation results\n‘that better align with human experts’ ratings compared with conventional automated evaluation metrics\nand existing LLM-as-a~judge methods.",
    "authors": "Chen; Jiaju; Lu; Yuxuan; Wang; Xiaojie; Zeng; Huimin; Huang; Jing; Gesi; Jiri; Xu; Ying; Yao; Bingsheng; Dakuo",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.21009v1",
    "title": "Memorization in Fine-Tuned Large Language Models",
    "abstract": "This study investigates the mechanisms and factors influencing\nmemorization in fine-tuned large language models (LLMs), with a focus on the medical domain due to its\nprivacy-sensitive nature. We examine how different aspects of the fine-tuning process affect a model's\npropensity to memorize training data, using the PHEE dataset of pharmacovigilance events.\n\nOur research employs two main approaches: a membership inference attack to detect memorized data, and\n‘a generation task with prompted prefixes to assess verbatim reproduction. We analyze the impact of\nadapting different weight matrices in the transformer architecture, the relationship between\nperplexity and memorization, and the effect of increasing the rank in low-rank adaptation (LORA) fine-\n‘tuning.\n\nKey findings include: (1) Value and Output matrices contribute more significantly to memorization\ncompared to Query and Key matrices; (2) Lower perplexity in the fine-tuned model correlates with\nincreased memorization; (3) Higher LoRA ranks lead to increased memorization, but with diminishing\nreturns at higher ranks.\n\nThese results provide insights into the trade-offs between model performance and privacy risks in\nfine-tuned LLMs. Our findings have implications for developing more effective and responsible\nstrategies for adapting large language models while managing data privacy concerns.",
    "authors": "Savine; Danil; Pydi; Muni Sreenivas; Atif; Jamal; Cappé; Olivier",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20999v1",
    "title": "LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient LLM Fine-Tuning",
    "abstract": "Large-scale generative models like DeepSeek-R1 and OpenAI-O1\nbenefit substantially from chain-of-thought (CoT) reasoning, yet pushing their performance typically\nrequires vast data, large model sizes, and full-parameter fine-tuning. While parameter-efficient fine-\n‘tuning (PEFT) helps reduce cost, most existing approaches primarily address domain adaptation or\nlayer-wise allocation rather than explicitly tailoring data and parameters to different response\ndemands. Inspired by \"Thinking, Fast and Slow,” which characterizes two distinct modes of thought-\nsystem 1 (fast, intuitive, often automatic) and System 2 (slower, more deliberative and analytic)-we\ndraw an analogy that different “subregions” of an LLM's parameters might similarly specialize for\n‘tasks that demand quick, intuitive responses versus those requiring multi-step logical reasoning.\n‘Therefore, we propose LORA-PAR, a dual-system LoRA framework that partitions both data and parameters\nby System 1 or System 2 demands, using fewer yet more focused parameters for each task. Specifically,\nwe classify task data via multi-model role-playing and voting, and partition parameters based on\nimportance scoring, then adopt a two-stage fine-tuning strategy of training System 1 tasks with\nsupervised fine-tuning (SFT) to enhance knowledge and intuition and refine System 2 tasks with\nreinforcement learning (RL) to reinforce deeper logical deliberation next. Extensive experiments show\n‘that the tuo-stage fine-tuning strategy, SFT and RL, lowers active parameter usage while matching or\nsurpassing SOTA PEFT baselines.",
    "authors": "Huang; Yining; Li; Bin; Tang; Keke; Chen; Meilian",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20957v1",
    "title": "Your AI, Not Your View: The Bias of LLMs in Investment Analysis",
    "abstract": "In finance, Large Language Models (LLMs) face frequent knowledge\nconflicts due to discrepancies between pre-trained parametric knowledge and real-time market data.\n‘These conflicts become particularly problematic when LLMs are deployed in real-world investment\nservices, where misalignment between a model's embedded preferences and those of the financial\ninstitution can lead to unreliable recommendations. Yet little research has examined what investment\nviews LLMs actually hold. We propose an experimental framework to investigate such conflicts, offering\n‘the first quantitative analysis of confirmation bias in LLM-based investment analysis. Using\nhypothetical scenarios with balanced and imbalanced arguments, we extract models’ latent preferences\nand measure their persistence. Focusing on sector, size, and momentum, our analysis reveals distinct,\nmodel-specific tendencies. In particular, we observe a consistent preference for large-cap stocks and\ncontrarian strategies across most models. These preferences often harden into confirmation bias, with\nmodels clinging to initial judgments despite counter-evidence.\n\nCurrent browse context:\n\nq-fin.PM,",
    "authors": "Lee; Hoyoung; Seo; Junhyuk; Park; Suhwan; Junhyeong; Ahn; Wonbin; Choi; Chanyeol; Lopez-Lira; Alejandro; Yongjae",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20956v1",
    "title": "Mind the Gap: Conformative Decoding to Improve Output Diversity of Instruction-Tuned Large Language Models",
    "abstract": "Instruction-tuning large language models (LLMs) reduces the\ndiversity of their outputs, which has implications for many tasks, particularly for creative tasks.\nThis paper investigates the ~“diversity gap'' for a writing prompt narrative generation task. This gap\nemerges as measured by current diversity metrics for various open-weight and open-source LLMs. The\nresults show significant decreases in diversity due to instruction-tuning. We explore the diversity\nloss at each fine-tuning stage for the OLMo and OLMo 2 models to further understand how output\ndiversity is affected. The results indicate that DPO has the most substantial impact on diversity.\nMotivated by these findings, we present a new decoding strategy, conformative decoding, which guides\nan instruct model using its more diverse base model to reintroduce output diversity. We show that\nconformative decoding typically increases diversity and even maintains or improves quality.",
    "authors": "Peeperkorn; Max; Kouwenhoven; Tom; Brown; Dan; Jordanous; Anna",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20936v1",
    "title": "Dissecting Persona-Driven Reasoning in Language Models via Activation Patching",
    "abstract": "Large language models (LLMs) exhibit remarkable versatility in\nadopting diverse personas. In this study, we examine how assigning a persona influences a model's\nreasoning on an objective task. Using activation patching, we take a first step toward understanding\nhow key components of the model encode persona-specific information. Our findings reveal that the\nearly Multi-Layer Perceptron (MLP) layers attend not only to the syntactic structure of the input but\nalso process its semantic content. These layers transform persona tokens into richer representations,\nwhich are then used by the middle Multi-Head Attention (MHA) layers to shape the model's output.\nAdditionally, we identify specific attention heads that disproportionately attend to racial and color-\nbased identities.\n\nCurrent browse context:\n\n¢s.L6",
    "authors": "Poonia; Ansh; Jain; Maeghal",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20930v1",
    "title": "FRED: Financial Retrieval-Enhanced Detection and Editing of Hallucinations in Language Models",
    "abstract": "Hallucinations in large language models pose a critical challenge for applications\nrequiring factual reliability, particularly in high-stakes domains such as finance. This work presents\nan effective approach for detecting and editing factually incorrect content in model-generated\nresponses based on the provided context. Given a user-defined domain-specific error taxonomy, we\nconstruct a synthetic dataset by inserting tagged errors into financial question-answering corpora and\n‘then fine-tune four language models, Phi-4, Phi-4-mini, Quen3-48, and Quen3-148, to detect and edit\n‘these factual inaccuracies. Our best-performing model, fine-tuned Phi-4, achieves an 8% improvement in\nbinary F1 score and a 30% gain in overall detection performance compared to OpenAl-o3. Notably, our\nfine-tuned Phi-4-mini model, despite having only 4 billion parameters, maintains competitive\nperformance with just a 2% drop in binary detection and a 0.1% decline in overall detection compared\n‘to OpenAI-o3. Our work provides a practical solution for detecting and editing factual inconsistencies\nin financial text generation while introducing a generalizable framework that can enhance the\n‘trustworthiness and alignment of large language models across diverse applications beyond finance. Our\ncode and data are available at this https URL.\n\nCurrent browse context:\n\ncs.cL",
    "authors": "Tan; Likun; Huang; Kuan-Wei; Wu; Kevin",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20924v1",
    "title": "FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech Concept Bottleneck Models",
    "abstract": "",
    "authors": "Labadie-Tamayo; Roberto; Böck; Adrian Jaques; Slijepčević; Djordje; Chen; Xihui; Babic; Andreas; Zeppelzauer; Matthias",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20917v1",
    "title": "MediQAl: A French Medical Question Answering Dataset for Knowledge and Reasoning Evaluation",
    "abstract": "This work introduces MediQAl, a French medical question answering\ndataset designed to evaluate the capabilities of language models in factual medical recall and\nreasoning over real-world clinical scenarios. MediQAl contains 32,603 questions sourced from French\nmedical examinations across 41 medical subjects. The dataset includes three tasks: (i) Multiple-Choice\nQuestion with Unique answer, (ii) Multiple-Choice Question with Multiple answer, and (iii) Open-Ended\nQuestion with Short-Answer. Each question is labeled as Understanding or Reasoning, enabling a\ndetailed analysis of models’ cognitive capabilities. We validate the MediQAl dataset through extensive\nevaluation with 14 large language models, including recent reasoning-augmented models, and observe a\nsignificant performance gap between factual recall and reasoning tasks. Our evaluation provides a\ncomprehensive benchmark for assessing language models’ performance on French medical question\nanswering, addressing @ crucial gap in multilingual resources for the medical domain.",
    "authors": "Bazoge; Adrien",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20906v1",
    "title": "Soft Injection of Task Embeddings Outperforms Prompt-Based In-Context Learning",
    "abstract": "In-Context Learning (ICL) enables Large Language Models (LLMs) to\nperform tasks by conditioning on input-output examples in the prompt, without requiring any update in\nmodel parameters. While widely adopted, it remains unclear whether prompting with multiple examples is\n‘the most effective and efficient way to convey task information. In this work, we propose Soft\nInjection of task embeddings. The task embeddings are constructed only once using few-shot ICL prompts\nand repeatedly used during inference. Soft injection is performed by softly mixing task embeddings\nwith attention head activations using pre-optimized mixing parameters, referred to as soft head-\nselection parameters. This method not only allows a desired task to be performed without in-prompt\ndemonstrations but also significantly outperforms existing ICL approaches while reducing memory usage\n‘and compute cost at inference time. An extensive evaluation is performed across 57 tasks and 12 LLMs,\nspanning four model families of sizes from 48 to 708. Averaged across 57 tasks, our method outperforms\n1-shot ICL by 10.1%-13.9% across 12 LLMs. Additional analyses show that our method also serves as an\ninsightful tool for analyzing task-relevant roles of attention heads, revealing that task-relevant\nhead positions selected by our method transfer across similar tasks but not across dissimilar ones --\nunderscoring the task-specific nature of head functionality. Our soft injection method opens a new\nparadigm for reducing prompt length and improving task performance by shifting task conditioning from\n‘the prompt space to the activation space.",
    "authors": "Park; Jungwon; Rhee; Wonjong",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20890v1",
    "title": "$A^2R^2$: Advancing Img2LaTeX Conversion via Visual Reasoning with Attention-Guided Refinement",
    "abstract": "Ing2LaTex is a practically significant task that involves\nconverting mathematical expressions or tabular data from images into LaTeX code. In recent years,\nvision-language models (VLMs) have demonstrated strong performance across a variety of visual\nunderstanding tasks, owing to their generalization capabilities. lihile some studies have explored the\nuse of Vis for the ImgLaTeX task, their performance often falls short of expectations. Empirically,\nVUis sometimes struggle with fine-grained visual elements, leading to inaccurate LaTeX predictions. To\naddress this challenge, we propose $A°2R\"2$: Advancing Img2LaTeX Conversion via Visual Reasoning with\nAttention-Guided Refinement, a framework that effectively integrates attention localization and\niterative refinement within 2 visual reasoning framework, enabling VLMs to perform self-correction and\nprogressively improve prediction quality. For effective evaluation, we introduce a new dataset,\nImg2LaTex-Hard-1K, consisting of 1,100 carefully curated and challenging examples designed to\nrigorously evaluate the capabilities of VMs within this task domain. Extensive experimental results\ndemonstrate that: (1) $A°2R\"2$ significantly improves model performance across six evaluation metrics\nspanning both textual and visual levels, consistently outperforming other baseline methods; (2)\nIncreasing the number of inference rounds yields notable performance gains, underscoring the potential\nof $A*2R°2$ in test-time scaling scenarios; (3) Ablation studies and human evaluations validate the\npractical effectiveness of our approach, as well as the strong synergy among its core components\nduring inference.",
    "authors": "Li; Zhecheng; Song; Guoxian; Wang; Yiwei; Xiong; Zhen; Yuan; Junsong; Cai; Yujun",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20888v1",
    "title": "Enhancing Project-Specific Code Completion by Inferring Internal API Information",
    "abstract": "Project-specific code completion is a critical task that\nleverages context from a project to generate accurate code. State-of-the-art methods use retrieval-\naugmented generation (RAG) with large language models (LLMs) and project information for code\ncompletion. However, they often struggle to incorporate internal API information, which is crucial for\naccuracy, especially when APIs are not explicitly imported in the file.\n\nTo address this, we propose a method to infer internal API information without relying on imports. Our\nmethod extends the representation of APIs by constructing usage examples and semantic descriptions,\nbuilding a knowledge base for LLMs to generate relevant completions. We also introduce Projgench, &\nbenchmark that avoids leaked imports and consists of large-scale real-world projects.\n\nExperiments on ProjBench and CrossCodetval show that our approach significantly outperforms existing\nmethods, improving code exact match by 22.72% and identifier exact match by 18.31%. Additionally,\nintegrating our method with existing baselines boosts code match by 47.80% and identifier match by\n35.55%.",
    "authors": "Deng; Le; Ren; Xiaoxue; Ni; Chao; Liang; Ming; Lo; David; Liu; Zhongxin",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20884v1",
    "title": "The Importance of Facial Features in Vision-based Sign Language Recognition: Eyes, Mouth or Full Face?",
    "abstract": "Non-manual facial features play @ crucial role in sign language\ncommunication, yet their importance in automatic sign language recognition (ASLR) remains\nunderexplored. While prior studies have shown that incorporating facial features can improve\nrecognition, related work often relies on hand-crafted feature extraction and fails to go beyond the\ncomparison of manual features versus the combination of manual and facial features. In this work, we\nsystematically investigate the contribution of distinct facial regionseyes, mouth, and full faceusing\ntwo different deep learning models (a CNN-based model and a transformer-based model) trained on an SLR\ndataset of isolated signs with randomly selected classes. Through quantitative performance and\nqualitative saliency map evaluation, we reveal that the mouth is the most important non-manual facial\nfeature, significantly improving accuracy. Our findings highlight the necessity of incorporating\nfacial features in ASLR.\n\nCurrent browse context:\n\ncs.V",
    "authors": "Pham; Dinh Nam; Avramidis; Eleftherios",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20859v1",
    "title": "Leveraging Open-Source Large Language Models for Clinical Information Extraction in Resource-Constrained Settings",
    "abstract": "Medical reports contain rich clinical information but are often\nunstructured and written in domain-specific language, posing challenges for information extraction.\nWhile proprietary large language models (LLMs) have shown promise in clinical natural language\nprocessing, their lack of transparency and data privacy concerns limit their utility in healthcare.\nThis study therefore evaluates nine open-source generative LLMs on the DRAGON benchmark, which\nincludes 28 clinical information extraction tasks in Dutch. We developed \\texttt{1lm\\_extractinator},\na publicly available framework for information extraction using open-source generative LLMs, and used\nit to assess model performance in a zero-shot setting. Several 14 billion parameter models, Phi-4-148,\nQuen-2.5-148, and DeepSeek-R1-148, achieved competitive results, while the bigger Llama-3.3-708 model\nachieved slightly higher performance at greater computational cost. Translation to English prior to\ninference consistently degraded performance, highlighting the need of native-language processing.\n‘These findings demonstrate that open-source LLMs, when used with our framework, offer effective,\nscalable, and privacy-conscious solutions for clinical information extraction in low-resource\nsettings.",
    "authors": "Builtjes; Luc; Bosma; Joeran; Prokop; Mathias; Van Ginneken; Bram; Hering; Alessa",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20858v1",
    "title": "A survey of diversity quantification in natural language processing: The why, what, where and how",
    "abstract": "The concept of diversity has received increased consideration in\nNatural Language Processing (NLP) in recent years. This is due to various motivations like promoting\nand inclusion, approximating human linguistic behavior, and increasing systems’ performance. Diversity\nhas however often been addressed in an ad hoc manner in NLP, and with few explicit links to other\ndomains where this notion is better theorized. We survey articles in the ACL Anthology from the past 6\nyears, with “diversity” or “diverse” in their title. We find a wide range of settings in which\ndiversity is quantified, often highly specialized and using inconsistent terminology. We put forward a\nunified taxonomy of why, what on, where, and how diversity is measured in NLP. Diversity measures are\ncast upon a unified framework from ecology and economy (Stirling, 2007) with 3 dimensions of\ndiversity: variety, balance and disparity. We discuss the trends which emerge due to this systematized\napproach. We believe that this study paves the way towards a better formalization of diversity in NLP,\nwhich should bring a better understanding of this notion and a better comparability between various\napproaches.",
    "authors": "Estève; Louis; De Marneffe; Marie-Catherine; Melnik; Nurit; Savary; Agata; Kanishcheva; Olha",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20849v1",
    "title": "Latent Inter-User Difference Modeling for LLM Personalization",
    "abstract": "Large language models (LLMs) are increasingly integrated into\nusers’ daily lives, leading to @ growing demand for personalized outputs. Previous work focuses on\nleveraging a user’s own history, overlooking inter-user differences that are crucial for effective\npersonalization. While recent work has attempted to model such differences, the reliance on language-\nbased prompts often hampers the effective extraction of meaningful distinctions. To address these\nissues, we propose Difference-auare Embedding-based Personalization (DEP), 2 framework that models\ninter-user differences in the latent space instead of relying on language prompts. DEP constructs soft\nprompts by contrasting a user's embedding with those of peers who engaged with similar content,\nhighlighting relative behavioral signals. A sparse autoencoder then filters and compresses both user-\nspecific and difference-aware embeddings, preserving only task-relevant features before injecting them\ninto a frozen LLM. Experiments on personalized review generation show that DEP consistently\n‘outperforms baseline methods across multiple metrics. Our code is available at this https URL.",
    "authors": "Qiu; Yilun; Shi; Tianhao; Zhao; Xiaoyan; Zhu; Fengbin; Zhang; Yang; Feng; Fuli",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20786v1",
    "title": "Automating Thematic Review of Prevention of Future Deaths Reports: Replicating the ONS Child Suicide Study using Large Language Models",
    "abstract": "Prevention of Future Deaths (PFD) reports, issued by coroners in England and Wales,\nflag systemic hazards that may lead to further loss of life. Analysis of these reports has previously\nbeen constrained by the manual effort required to identify and code relevant cases. In 2025, the\nOffice for National Statistics (ONS) published a national thematic review of child-suicide PFD reports\n($\\leq$ 18 years), identifying 37 cases from January 2015 to November 2023 - a process based entirely\n‘on manual curation and coding. We evaluated whether a fully automated, open source \"text-to-table”\nLanguage-model pipeline (PFD Toolkit) could reproduce the ONS's identification and thematic analysis\nof child-suicide PFD reports, and assessed gains in efficiency and reliability. All 4,249 PFD reports\npublished from July 2013 to November 2023 were processed via PFD Toolkit's large language model\npipelines. Automated screening identified cases where the coroner attributed death to suicide in\nindividuals aged 18 or younger, and eligible reports were coded for recipient category and 23 concern\nsub-themes, replicating the ONS coding frame. PFD Toolkit identified 72 child-suicide PFD reports -\nalmost twice the ONS count. Three blinded clinicians adjudicated a stratified sample of 144 reports to\nvalidate the child-suicide screening. Against the post-consensus clinical annotations, the LLM-based\nworkflow showed substantial to almost-perfect agreement (Cohen's $\\kappa$ = 0.82, 95% CI: 0.66-0.98,\nraw agreement = 91%). The end-to-end script runtime was 8m 16s, transforming @ process that previously\n‘took months into one that can be completed in minutes. This demonstrates that automated LLM analysis\ncan reliably and efficiently replicate manual thematic reviews of coronial data, enabling scalable,\nreproducible, and timely insights for public health and safety. The PFD Toolkit is openly available\nfor future research.",
    "authors": "Osian; Sam; Dutta; Arpan; Bhandari; Sahil; Buchan; Iain E; Joyce; Dan W",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20783v1",
    "title": "On The Role of Pretrained Language Models in General-Purpose Text Embeddings: A Survey",
    "abstract": "Text embeddings have attracted growing interest due to their effectiveness across a\nwide range of natural language processing (NLP) tasks, such as retrieval, classification, clustering,\nbitext mining, and summarization. With the emergence of pretrained language models (PLMs), general-\npurpose text embeddings (GPTE) have gained significant traction for their ability to produce rich,\n‘transferable representations. The general architecture of GPTE typically leverages PLIls to derive\ndense text representations, which are then optimized through contrastive learning on large-scale\npairwise datasets. In this survey, we provide a comprehensive overview of GPTE in the era of PLMs,\nfocusing on the roles PLis play in driving its development. We first examine the fundamental\narchitecture and describe the basic roles of PLMs in GPTE, i.e., embedding extraction, expressivity\nenhancement, training strategies, learning objectives, and data construction. Then, we describe\nadvanced roles enabled by PLMs, such as multilingual support, multimodal integration, code\nunderstanding, and scenario-specific adaptation. Finally, we highlight potential future research\ndirections that move beyond traditional improvement goals, including ranking integration, safety\nconsiderations, bias mitigation, structural information incorporation, and the cognitive extension of\n‘embeddings. This survey aims to serve as a valuable reference for both newcomers and established\nresearchers seeking to understand the current state and future potential of GPTE.",
    "authors": "Zhang; Meishan; Xin; Zhao; Huang; Shouzheng; Hu; Baotian; Min",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20752v1",
    "title": "Multilingual Self-Taught Faithfulness Evaluators",
    "abstract": "The growing use of large language models (LLMs) has increased the need for automatic\nevaluation systems, particularly to address the challenge of information hallucination. Although\nexisting faithfulness evaluation approaches have shown promise, they are predominantly English-focused\nand often require expensive human-labeled training data for fine-tuning specialized models. As LLMs\nsee increased adoption in multilingual contexts, there is a need for accurate faithfulness evaluators\n‘that can operate across languages without extensive labeled data. This paper presents Self-Taught\nEvaluators for Multilingual Faithfulness, @ framework that learns exclusively from synthetic\nmultilingual summarization data while leveraging cross-lingual transfer learning. Through experiments\ncomparing language-specific and mixed-language fine-tuning approaches, we demonstrate a consistent\nrelationship between an LLM's general language capabilities and its performance in language-specific\nevaluation tasks. Our framework shows improvements over existing baselines, including state-of-the-art\nEnglish evaluators and machine translation-based approaches.",
    "authors": "Alfano; Carlo; Marjani; Aymen Al; Jonke; Zeno; Mantrach; Amin; Mansour; Saab; Federico; Marcello",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20749v1",
    "title": "Investigating Structural Pruning and Recovery Techniques for Compressing Multimodal Large Language Models: An Empirical Study",
    "abstract": "hhile Multimodal Large Language Models (MLLMs) demonstrate impressive capabilities,\n‘their substantial computational and memory requirements pose significant barriers to practical\ndeployment. Current parameter reduction techniques primarily involve training MLLMs from Small\nLanguage Models (SLMs), but these methods offer limited flexibility and remain computationally\nintensive. To address this gap, we propose to directly compress existing MLLMs through structural\npruning combined with efficient recovery training. Specifically, we investigate two structural pruning\nparadigms--layerwise and widthwise pruning--applied to the language model backbone of MLLMs, alongside\nsupervised finetuning and knowledge distillation. Additionally, we assess the feasibility of\nconducting recovery training with only a small fraction of the available data. Our results show that\nwidthwise pruning generally maintains better performance in low-resource scenarios with limited\ncomputational resources or insufficient finetuning data. As for the recovery training, finetuning only\n‘the multimodal projector is sufficient at small compression levels (< 20%). Furthermore, a combination\nof supervised finetuning and hidden-state distillation yields optimal recovery across various pruning\nlevels. Notably, effective recovery can be achieved with as little as 5% of the original training\ndata, while retaining over 95% of the original performance. Through empirical study on tuo\nrepresentative LLMs, i.e., LLaVA-v1.5-78 and Bunny-v1.0-38, this study offers actionable insights for\npractitioners aiming to compress LLMs effectively without extensive computation resources or\nsufficient data.",
    "authors": "Huang; Yiran; Thede; Lukas; Mancini; Massimiliano; Xu; Wenjia; Akata; Zeynep",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20704v1",
    "title": "Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models",
    "abstract": "The increasing integration of Visual Language Models (VLMs) into AI systems\nnecessitates robust model alignment, especially when handling multimodal content that combines text\nand images. Existing evaluation datasets heavily lean towards text-only prompts, leaving visual\nvulnerabilities under evaluated. To address this gap, we propose \\textbf{Text2VLM}, @ novel multi-\nstage pipeline that adapts text-only datasets into multimodal formats, specifically designed to\nevaluate the resilience of VLMs against typographic prompt injection attacks. The Text2VLM pipeline\nidentifies harmful content in the original text and converts it into a typographic image, creating a\nmultimodal prompt for VLMs. Also, our evaluation of open-source VLMs highlights their increased\nsusceptibility to prompt injection when visual inputs are introduced, revealing critical weaknesses in\n‘the current models’ alignment. This is in addition to a significant performance gap compared to\nclosed-source frontier models. We validate Text2VLM through human evaluations, ensuring the alignment\nof extracted salient concepts; text summarization and output classification align with human\n‘expectations. Text2VLM provides a scalable tool for comprehensive safety assessment, contributing to\n‘the development of more robust safety mechanisms for VLls. By enhancing the evaluation of multimodal\nvulnerabilities, Text2ViM plays a role in advancing the safe deployment of VLMs in diverse, real-world\napplications.\n\nCurrent browse context:\n\ncs.cL",
    "authors": "Downer; Gabriel; Craven; Sean; Ruck; Damian; Thomas; Jake",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20700v1",
    "title": "When Scale Meets Diversity: Evaluating Language Models on Fine-Grained Multilingual Claim Verification",
    "abstract": "The rapid spread of multilingual misinformation requires robust automated fact\nverification systems capable of handling fine-grained veracity assessments across diverse languages.\nWhile large language models have shown remarkable capabilities across many NLP tasks, their\neffectiveness for multilingual claim verification with nuanced classification schemes remains\nunderstudied. We conduct a comprehensive evaluation of five state-of-the-art language models on the X-\nFact dataset, which spans 25 languages with seven distinct veracity categories. Our experiments\ncompare small language models (encoder-based XLM-R and mT5) with recent decoder-only LLMs (Llama 3.1,\nQuen 2.5, Mistral Nemo) using both prompting and fine-tuning approaches. Surprisingly, we find that\nXLU-R (270M parameters) substantially outperforms all tested LLMs (7-128 parameters), achieving 57.7%\nmacro-Fl compared to the best LLM performance of 16.9%. This represents @ 15.8% improvement over the\nprevious state-of-the-art (41.9%), establishing new performance benchmarks for multilingual fact\nverification. Our analysis reveals problematic patterns in LLM behavior, including systematic\ndifficulties in leveraging evidence and pronounced biases toward frequent categories in imbalanced\ndata settings. These findings suggest that for fine-grained multilingual fact verification, smaller\nspecialized models may be more effective than general-purpose large models, with important\nimplications for practical deployment of fact-checking systems.\n\nSubmission history\n\nFrom: Hanna Shcharbakova [view email][v1] Mon, 28 Jul 2025 10:49:04 UTC (1,921 KB)",
    "authors": "Shcharbakova; Hanna; Anikina; Tatiana; Skachkova; Natalia; Van Genabith; Josef",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20673v1",
    "title": "Geometric-Mean Policy Optimization",
    "abstract": "Recent advancements, such as Group Relative Policy Optimization\n(GRPO), have enhanced the reasoning capabilities of large language models by optimizing the arithmetic\nmean of token-level rewards. However, GRPO suffers from unstable policy updates when processing tokens\nwith outlier importance-weighted rewards, which manifests as extreme importance sampling ratios during\n‘training, i.e., the ratio between the sampling probabilities assigned to a token by the current and\nold policies. In this work, we propose Geometric-Mean Policy Optimization (GHPO), a stabilized variant\nof GRPO. Instead of optimizing the arithmetic mean, GMPO maximizes the geometric mean of token-level\nrewards, which is inherently less sensitive to outliers and maintains a more stable range of\nimportance sampling ratio. In addition, we provide comprehensive theoretical and experimental analysis\nto justify the design and stability benefits of GMPO. Beyond improved stability, GYPO-78 outperforms\nGRPO by an average of 4.1% on multiple mathematical benchmarks and 1.4% on multimodal reasoning\nbenchmark, including AIME24, AMC, MATHS®0, OlympiadBench, Minerva, and Geometry3K. Code is available\nat this https URL.",
    "authors": "Zhao; Yuzhong; Liu; Yue; Junpeng; Chen; Jingye; Wu; Xun; Hao; Yaru; Lv; Tengchao; Huang; Shaohan; Cui; Lei; Ye; Qixiang; Wan; Fang; Wei; Furu",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20643v1",
    "title": "Ontology-Enhanced Knowledge Graph Completion using Large Language Models",
    "abstract": "Large Language Models (LLMs) have been extensively adopted in Knowledge Graph\nCompletion (KGC), showcasing significant research advancements. However, as black-box models driven by\ndeep neural architectures, current LLi-based KGC methods rely on implicit knowledge representation\nwith parallel propagation of erroneous knowledge, thereby hindering their ability to produce\nconclusive and decisive reasoning outcomes. We aim to integrate neural-perceptual structural\ninformation with ontological knowledge, leveraging the powerful capabilities of LLMs to achieve a\ndeeper understanding of the intrinsic logic of the knowledge. We propose an ontology enhanced KGC\nmethod using LLMs -- OL-KGC. It first leverages neural perceptual mechanisms to effectively embed\nstructural information into the textual space, and then uses an automated extraction algorithm to\nretrieve ontological knowledge from the knowledge graphs (KGs) that needs to be completed, which is\nfurther transformed into a textual format comprehensible to LLMs for providing logic guidance. We\nconducted extensive experiments on three widely-used benchmarks -- FB1SK-237, UMLS and WNIBRR. The\n‘experimental results demonstrate that OL-KGC significantly outperforms existing mainstream KGC methods\nacross multiple evaluation metrics, achieving state-of-the-art performance.",
    "authors": "Guo; Wenbin; Wang; Xin; Chen; Jiaoyan; Li; Zhao; Zirui",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20614v1",
    "title": "Before the Outrage: Challenges and Advances in Predicting Online Antisocial Behavior",
    "abstract": "Antisocial behavior (ASB) on social media-including hate speech, harassment, and\ntrolling-poses growing challenges for platform safety and societal wellbeing. While prior work has\nprimarily focused on detecting harmful content after it appears, predictive approaches aim to forecast\nfuture harmful behaviors-such as hate speech propagation, conversation derailment, or user recidivism\nbefore they fully unfold. Despite increasing interest, the field remains fragmented, lacking a unified\n‘taxonomy or clear synthesis of existing methods. This paper presents @ systematic review of over 49\nstudies on ASB prediction, offering a structured taxonomy of five core task types: early harm\ndetection, harm emergence prediction, harm propagation prediction, behavioral risk prediction, and\nproactive moderation support. We analyze how these tasks differ by temporal framing, prediction\ngranularity, and operational goals. In addition, we examine trends in modeling techniques-from\nClassical machine learning to pre-trained language models-and assess the influence of dataset\ncharacteristics on task feasibility and generalization. Our review highlights methodological\nchallenges, such as dataset scarcity, temporal drift, and limited benchmarks, while outlining emerging\nresearch directions including multilingual modeling, cross-platform generalization, and human-in-the-\nloop systems. By organizing the field around a coherent framework, this survey aims to guide future\nwork toward more robust and socially responsible ASB prediction.\n\nSubmission history\n\nFrom: Anais ollagnier [view email] [via CCSD proxy][v2] Mon, 28 Jul 2025 08:27:58 UTC (1,059 KB)",
    "authors": "Ollagnier; Anaïs",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20564v1",
    "title": "ZSE-Cap: A Zero-Shot Ensemble for Image Retrieval and Prompt-Guided Captioning",
    "abstract": "ble present Z5E-Cap (Zero-Shot Ensemble for Captioning), our 4th place system in\nEvent-Enriched Image Analysis (EVENTA) shared task on article-grounded image retrieval and captioning.\nOur zero-shot approach requires no finetuning on the competition's data. For retrieval, we ensemble\nsimilarity scores from CLIP, SiglIP, and DINOv2. For captioning, we leverage a carefully engineered\nprompt to guide the Gemma 3 model, enabling it to link high-level events from the article to the\nvisual content in the image. Our system achieved a final score of 0.42002, securing a top-4 position\non the private test set, demonstrating the effectiveness of combining foundation models through\n‘ensembling and prompting. Our code is available at this https URL.",
    "authors": "Dinh; Duc-Tai; Duc Anh Khoa",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20546v1",
    "title": "Enhancing Hallucination Detection via Future Context",
    "abstract": "Large Language Models (LLMs) are widely used to generate plausible text on online\nplatforms, without revealing the generation process. As users increasingly encounter such black-box\noutputs, detecting hallucinations has become a critical challenge. To address this challenge, we focus\n‘on developing a hallucination detection framework for black-box generators. Motivated by the\nobservation that hallucinations, once introduced, tend to persist, we sample future contexts. The\nsampled future contexts provide valuable clues for hallucination detection and can be effectively\nintegrated with various sampling-based methods. We extensively demonstrate performance improvements\nacross multiple methods using our proposed sampling approach.",
    "authors": "Lee; Joosung; Park; Cheonbok; Jo; Hwiyeol; Kim; Jeonghoon; Yoo; Kang Min",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20534v1",
    "title": "Kimi K2: Open Agentic Intelligence",
    "abstract": "We introduce Kimi K2, a Mixture-of-Experts (MoE) large language\nmodel with 32 billion activated parameters and 1 trillion total parameters. We propose the MuonClip\noptimizer, which improves upon Muon with a novel Qk-clip technique to address training instability\nwhile enjoying the advanced token efficiency of Muon. Based on MuonClip, K2 was pre-trained on 15.5\n‘trillion tokens with zero loss spike. During post-training, K2 undergoes a multi-stage post-training\nprocess, highlighted by a large-scale agentic data synthesis pipeline and a joint reinforcement\nlearning (RL) stage, where the model improves its capabilities through interactions with real and\nsynthetic environments.\n\nKimi K2 achieves state-of-the-art performance among open-source non-thinking models, with strengths in\nagentic capabilities. Notably, K2 obtains 66.1 on Tau2-Bench, 76.5 on ACEBench (En), 65.8 on SWE-Bench\nVerified, and 47.3 on SWE-Bench Multilingual -- surpassing most open and closed-sourced baselines in\nnon-thinking settings. It also exhibits strong capabilities in coding, mathematics, and reasoning\n‘tasks, with a score of 53.7 on LiveCodeBench v6, 49.5 on AIME 2025, 75.1 on GPQA-Diamond, and 27.1 on\n038ench, all without extended thinking. These results position Kimi K2 as one of the most capable\nopen-source large language models to date, particularly in software engineering and agentic tasks. We\nrelease our base and post-trained model checkpoints to facilitate future research and applications of\nagentic intelligence.\n\nCurrent browse context\n¢s.L6",
    "authors": "Kimi Team; Bai; Yifan; Bao; Yiping; Chen; Guanduo; Jiahao; Ningxin; Ruijue; Yanru; Yuankun; Yutian; Zhuofu; Cui; Jialei; Ding; Hao; Dong; Mengnan; Du; Angang; Dikang; Yulun; Fan; Yu; Feng; Yichen; Fu; Kelin; Gao; Bofei; Hongcheng; Peizhong; Tong; Gu; Xinran; Longyu; Haiqing; Jianhang; Hu; Xiaoru; He; Tianhong; Weiran; Wenyang; Hong; Chao; Yangyang; Zhenxing; Weixiao; Zhiqi; Zihao; Jiang; Tao; Zhejun; Jin; Xinyi; Kang; Yongsheng; Lai; Li; Ming; Wentao; Yanhao; Yiwei; Zhaowei; Zheming; Xiaohan; Zongyu; Junqi; Shaowei; T Y; Tianwei; Weizhou; Yibo; Zhengying; Lu; Enzhe; Ma; Shengling; Xinyu; Yingwei; Shaoguang; Mei; Jie; Men; Xin; Miao; Pan; Siyuan; Peng; Yebo; Qin; Ruoyu; Qu; Bowen; Shang; Zeyu; Shi; Shengyuan; Song; Feifan; Su; Jianlin; Zhengyuan; Flood; Tang; Jiawen; Teng; Qifeng; Wang; Haiming; Jianzhou; Jiaxing; Shengjie; Shuyi; Yao; Yejie; Yiqin; Zhaoji; Zhengtao; Zhexu; Wei; Chu; Qianqian; Wu; Wenhao; Xiao; Xie; Xiong; Xu; Boyu; L H; Yangchuan; Ziyao; Yan; Junjie; Ying; Zhen; Zhilin; Zonghan; Ye; Wenjie; Zhuorui; Yin; Bohong; Longhui; Enming; Zhan; Dehao; Wanlu; Yizhi; Yongting; Zhao; Yikai; Shaojie; Zhou; Jianren; Zaida; Zhu; Zu",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20528v1",
    "title": "Dialogues of Dissent: Thematic and Rhetorical Dimensions of Hate and Counter-Hate Speech in Social Media Conversations",
    "abstract": "We introduce a novel multi-labeled scheme for joint annotation of hate and counter-\nhate speech in social media conversations, categorizing hate and counter-hate messages into thematic\nand rhetorical dimensions. The thematic categories outline different discursive aspects of each type\nof speech, while the rhetorical dimension captures how hate and counter messages are communicated,\ndrawing on Aristotle's Logos, Ethos and Pathos. We annotate a sample of 92 conversations, consisting\nof 720 tweets, and conduct statistical analyses, incorporating public metrics, to explore patterns of\ninteraction between the thematic and rhetorical dimensions within and between hate and counter-hate\nspeech. Our findings provide insights into the spread of hate messages on social media, the strategies\nused to counter them, and their potential impact on online behavior.",
    "authors": "Levi; Effi; Ron; Gal; Oshri; Odelia; Shenhav; Shaul R",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20527v1",
    "title": "SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers",
    "abstract": "The demand for Large Language Models (LLMs) capable of sophisticated mathematical\nreasoning is growing across industries. However, the development of performant mathematical LLMs is\ncritically bottlenecked by the scarcity of difficult, novel training data. We introduce \\textbf{SAND-\nMath} (Synthetic Augmented Novel and Difficult Mathematics problems and solutions), a pipeline that\naddresses this by first generating high-quality problems from scratch and then systematically\nelevating their complexity via a new \\textbf{Difficulty Hiking} step. We demonstrate the effectiveness\n‘of our approach through two key findings. First, augmenting a strong baseline with SAND-Math data\nsignificantly boosts performance, outperforming the next-best synthetic dataset by \\textbf{$\\uparrou$\n17.85 absolute points} on the AINE25 benchmark. Second, in a dedicated ablation study, we show our\nDifficulty Hiking process is highly effective: by increasing average problem difficulty from 5.02 to\n5.98, this step lifts AINE25 performance from 46.38\\% to 49.23\\%. The full generation pipeline, final\ndataset, and a fine-tuned model form a practical and scalable toolkit for building more capable and\nefficient mathematical reasoning LLMs. SAND-Math dataset is released here: \\href{this https URL}{this\nhttps URL}",
    "authors": "Manem; Chaitanya; Brahma; Pratik Prabhanjan; Mishra; Prakamya; Liu; Zicheng; Barsoum; Emad",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20526v1",
    "title": "Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition",
    "abstract": "Recent advances have enabled LLM-powered AI agents to autonomously execute complex\n‘tasks by combining language model reasoning with tools, memory, and web access. But can these systems\nbe trusted to follow deployment policies in realistic environments, especially under attack? To\ninvestigate, we ran the largest public red-teaming competition to date, targeting 22 frontier AI\nagents across 44 realistic deployment scenarios. Participants submitted 1.8 million prompt-injection\nattacks, with over 60,000 successfully eliciting policy violations such as unauthorized data access,\nillicit financial actions, and regulatory noncompliance. We use these results to build the Agent Red\n‘Teaming (ART) benchmark - a curated set of high-impact attacks - and evaluate it across 19 state-of-\n‘the-art models. Nearly all agents exhibit policy violations for most behaviors within 10-100 queries,\nwith high attack transferability across models and tasks. Importantly, we find limited correlation\nbetween agent robustness and model size, capability, or inference-time compute, suggesting that\nadditional defenses are needed against adversarial misuse. Our findings highlight critical and\npersistent vulnerabilities in today's AI agents. By releasing the ART benchmark and accompanying\nevaluation framework, we aim to support more rigorous security assessment and drive progress toward\nsafer agent deployment.\n\nCurrent browse context:\n\ncs.AL",
    "authors": "Zou; Andy; Lin; Maxwell; Jones; Eliot; Nowak; Micha; Dziemian; Mateusz; Winter; Nick; Grattan; Alexander; Nathanael; Valent; Croft; Ayla; Davies; Xander; Patel; Jai; Kirk; Robert; Burnikell; Nate; Gal; Yarin; Hendrycks; Dan; Kolter; J Zico; Fredrikson; Matt",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20520v1",
    "title": "AQUA: A Large Language Model for Aquaculture & Fisheries",
    "abstract": "Aquaculture plays a vital role in global food security and coastal economies by\nproviding sustainable protein sources. As the industry expands to meet rising demand, it faces growing\nchallenges such as disease outbreaks, inefficient feeding practices, rising labor costs, logistical\ninefficiencies, and critical hatchery issues, including high mortality rates and poor water quality\ncontrol. Although artificial intelligence has made significant progress, existing machine learning\nmethods fall short of addressing the domain-specific complexities of aquaculture. To bridge this gap,\nwe introduce AQUA, the first large language model (LLM) tailored for aquaculture, designed to support\nfarmers, researchers, and industry practitioners. Central to this effort is AQUADAPT (Data\nAcquisition, Processing and Tuning), an Agentic Framework for generating and refining high-quality\nsynthetic data using a combination of expert knowledge, largescale language models, and automated\nevaluation techniques. Our work lays the foundation for LLM-driven innovations in aquaculture\nresearch, advisory systems, and decision-making tools.\n\nSubmission history\n\nFron: Praneeth Narisetty [view email][v1] Mon, 28 Jul 2025 05:06:07 UTC (689 KB)\n\nCurrent browse context:\n\n¢s.cL",
    "authors": "Narisetty; Praneeth; Kattamanchi; Uday Kumar Reddy; Nimma; Lohit Akshant; Karnati; Sri Ram Kaushik; Kore; Shiva Nagendra Babu; Golamari; Mounika; Nageshreddy; Tejashree",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20503v1",
    "title": "Customize Multi-modal RAI Guardrails with Precedent-based predictions",
    "abstract": "A multi-modal guardrail must effectively filter image content based on user-defined\npolicies, identifying material that may be hateful, reinforce harmful stereotypes, contain explicit\nmaterial, or spread misinformation. Deploying such guardrails in real-world applications, however,\nposes significant challenges. Users often require varied and highly customizable policies and\n‘typically cannot provide abundant examples for each custom policy. Consequently, an ideal guardrail\nshould be scalable to the multiple policies and adaptable to evolving user standards with minimal\nretraining. Existing fine-tuning methods typically condition predictions on pre-defined policies,\nrestricting their generalizability to new policies or necessitating extensive retraining to adapt.\nConversely, training-free methods struggle with limited context lengths, making it difficult to\nincorporate all the policies comprehensively. To overcome these limitations, we propose to condition\nmodel's judgment on “precedents”, which are the reasoning processes of prior data points similar to\n‘the given input. By leveraging precedents instead of fixed policies, our approach greatly enhances the\nflexibility and adaptability of the guardrail. In this paper, we introduce a critique-revise mechanism\nfor collecting high-quality precedents and two strategies that utilize precedents for robust\nprediction. Experimental results demonstrate that our approach outperforms previous methods across\nboth few-shot and full-dataset scenarios and exhibits superior generalization to novel policies.\nCurrent browse context:\n\n¢s.L6",
    "authors": "Yang; Cheng-Fu; Tran; Thanh; Christodoulopoulos; Christos; Ruan; Weitong; Gupta; Rahul; Chang; Kai-Wei",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20491v1",
    "title": "Speaking in Words, Thinking in Logic: A Dual-Process Framework in QA Systems",
    "abstract": "",
    "authors": "Bui; Tuan; Le; Trong; Thai; Phat; Nguyen; Sang; Hua; Minh; Pham; Ngan; Thang; Quan; Tho",
    "date": "28 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20423v1",
    "title": "CodeNER: Code Prompting for Named Entity Recognition",
    "abstract": "Recent studies have explored various approaches for treating\ncandidate named entity spans as both source and target sequences in named entity recognition (NER) by\nleveraging large language models (LLMs). Although previous approaches have successfully generated\ncandidate named entity spans with suitable labels, they rely solely on input context information when\nusing LLMs, particularly, ChatGPT. However, NER inherently requires capturing detailed labeling\nrequirements with input context information. To address this issue, we propose a novel method that\nleverages code-based prompting to improve the capabilities of LLMs in understanding and performing\nNER. By embedding code within prompts, we provide detailed B10 schema instructions for labeling,\n‘thereby exploiting the ability of LLMs to comprehend long-range scopes in programming languages.\nExperimental results demonstrate that the proposed code-based prompting method outperforms\nconventional text-based prompting on ten benchmarks across English, Arabic, Finnish, Danish, and\nGerman datasets, indicating the effectiveness of explicitly structuring NER instructions. We also\nverify that combining the proposed code-based prompting method with the chain-of-thought prompting\nfurther improves performance.",
    "authors": "Han; Sungwoo; Kim; Hyeyeon; Kwon; Jingun; Kamigaito; Hidetaka; Okumura; Manabu",
    "date": "27 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20419v1",
    "title": "Survey of NLU Benchmarks Diagnosing Linguistic Phenomena: Why not Standardize Diagnostics Benchmarks?",
    "abstract": "",
    "authors": "Jallad; Khloud AL; Ghneim; Nada; Rebdawi; Ghaida",
    "date": "27 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20411v1",
    "title": "CONCAP: Seeing Beyond English with Concepts Retrieval-Augmented Captioning",
    "abstract": "Multilingual vision-language models have made significant strides\nin image captioning, yet they still lag behind their English counterparts due to limited multilingual\n‘training data and costly large-scale model parameterization. Retrieval-augmented generation (RAG)\noffers a promising alternative by conditioning caption generation on retrieved examples in the target\nLanguage, reducing the need for extensive multilingual training. However, multilingual RAG captioning\nmodels often depend on retrieved captions translated from English, which can introduce mismatches and\nLinguistic biases relative to the source language. We introduce CONCAP, a multilingual image\ncaptioning model that integrates retrieved captions with image-specific concepts, enhancing the\ncontextualization of the input image and grounding the captioning process across different languages.\nExperiments on the X\"3600 dataset indicate that CONCAP enables strong performance on low- and mid-\nresource languages, with highly reduced data requirements. Our findings highlight the effectiveness of\nconcept-aware retrieval augmentation in bridging multilingual performance gaps.",
    "authors": "Ibrahim; George; Ramos; Rita; Kementchedjhieva; Yova",
    "date": "27 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20409v1",
    "title": "Cognitive Chain-of-Thought: Structured Multimodal Reasoning about Social Situations",
    "abstract": "Chain-of-Thought (CoT) prompting helps models think step by step.\nBut what happens when they must see, understand, and judge-all at once? In visual tasks grounded in\nsocial context, where bridging perception with norm-grounded judgments is essential, flat CoT often\nbreaks down. We introduce Cognitive Chain-of-Thought (CoCoT), @ prompting strategy that scaffolds VLM\nreasoning through three cognitively inspired stages: perception, situation, and norm. Our experiments\nshow that, across multiple multimodal benchmarks (including intent disambiguation, commonsense\nreasoning, and safety), CoCoT consistently outperforms CoT and direct prompting (+8\\% on average). Our\nfindings demonstrate that cognitively grounded reasoning stages enhance interpretability and social\nawareness in VLMs, paving the way for safer and more reliable multimodal systems.\n\nCurrent browse context:\n\ncs.cL",
    "authors": "Park; Eunkyu; Deng; Wesley Hanwen; Kim; Gunhee; Eslami; Motahhare; Sap; Maarten",
    "date": "27 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20398v1",
    "title": "Length Representations in Large Language Models",
    "abstract": "Large language models (LLMs) have shown remarkable capabilities\nacross various tasks, that are learned from massive amounts of text-based data. Although LLMs can\ncontrol output sequence length, particularly in instruction-based settings, the internal mechanisms\nbehind this control have been unexplored yet. In this study, we provide empirical evidence on how\noutput sequence length information is encoded within the internal representations in LLMs. In\nparticular, our findings show that multi-head attention mechanisms are critical in determining output\nsequence length, which can be adjusted in a disentangled manner. By scaling specific hidden units\nwithin the model, we can control the output sequence length without losing the informativeness of the\ngenerated text, thereby indicating that length information is partially disentangled from semantic\ninformation. Moreover, some hidden units become increasingly active as prompts become more length-\nspecific, thus reflecting the model's internal awareness of this attribute. Our findings suggest that\nLLMs have learned robust and adaptable internal mechanisms for controlling output length without any\nexternal control.",
    "authors": "Moon; Sangjun; Choi; Dasom; Kwon; Jingun; Kamigaito; Hidetaka; Okumura; Manabu",
    "date": "27 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20352v1",
    "title": "RMTBench: Benchmarking LLMs Through Multi-Turn User-Centric Role-Playing",
    "abstract": "Recent advancements in Large Language Models (LLMs) have shown\noutstanding potential for role-playing applications. Evaluating these capabilities is becoming crucial\nyet remains challenging. Existing benchmarks mostly adopt a \\textbf{character-centric} approach,\nsimplify user-character interactions to isolated Q&A tasks, and fail to reflect real-world\napplications. To address this limitation, we introduce RMTBench, a comprehensive \\textbf{user-centric}\nbilingual role-playing benchmark featuring 80 diverse characters and over 8,000 dialogue rounds.\nRMTBench includes custom characters with detailed backgrounds and abstract characters defined by\nsimple traits, enabling evaluation across various user scenarios. Our benchmark constructs dialogues\nbased on explicit user motivations rather than character descriptions, ensuring alignment with\npractical user applications. Furthermore, we construct an authentic multi-turn dialogue simulation\nmechanism. With carefully selected evaluation dimensions and LLM-based scoring, this mechanism\ncaptures the complex intention of conversations between the user and the character. By shifting focus\nfrom character background to user intention fulfillment, RMTBench bridges the gap between academic\nevaluation and practical deployment requirements, offering a more effective framework for assessing\nrole-playing capabilities in LLMs. All code and datasets will be released soon.",
    "authors": "Xiang; Hao; Tang; Tianyi; Su; Yang; Yu; Bowen; An; Huang; Fei; Zhang; Yichang; Lu; Yaojie; Lin; Hongyu; Han; Xianpei; Zhou; Jingren; Junyang; Le",
    "date": "27 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20343v1",
    "title": "DYNARTmo: A Dynamic Articulatory Model for Visualization of Speech Movement Patterns",
    "abstract": "We present DYNARTmo, @ dynamic articulatory model designed to\nvisualize speech articulation processes in a two-dimensional midsagittal plane. The model builds upon\n‘the UK-DYNANO framework and integrates principles of articulatory underspecification, segmental and\ngestural control, and coarticulation. DYNARTmo simulates six key articulators based on ten continuous\nand six discrete control parameters, allowing for the generation of both vocalic and consonantal\narticulatory configurations. The current implementation is embedded in 2 web-based application\n(SpeecharticulationTrainer) that includes sagittal, glottal, and palatal views, making it suitable for\nUse in phonetics education and speech therapy. While this paper focuses on the static modeling\naspects, future work will address dynamic movement generation and integration with articulatory-\nacoustic modules.\n\nSubmission history\n\nFrom: Bernd-Joachim Kréger [view email][v1] Sun, 27 Jul 2025 16:19:46 UTC (5,070 KB)",
    "authors": "Kröger; Bernd J",
    "date": "27 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20301v1",
    "title": "Advancing Dialectal Arabic to Modern Standard Arabic Machine Translation",
    "abstract": "Dialectal Arabic (DA) poses a persistent challenge for natural\nLanguage processing (NLP), as most everyday communication in the Arab world occurs in dialects that\ndiverge significantly from Modern Standard Arabic (HSA). This linguistic divide limits access to\ndigital services and educational resources and impedes progress in Arabic machine translation. This\npaper presents two core contributions to advancing DA-MSA translation for the Levantine, Egyptian, and\nGulf dialects, particularly in low-resource and computationally constrained settings: a comprehensive\nevaluation of training-free prompting techniques, and the development of a resource-efficient fine-\n‘tuning pipeline. Our evaluation of prompting strategies across six large language models (LLMs) found\n‘that few-shot prompting consistently outperformed zero-shot, chain-of-thought, and our proposed Ara-\nTEaR method. GPT-4o achieved the highest performance across all prompting settings. For fine-tuning, a\nquantized Genma2-98 model achieved 3 CHrF++ score of 49.88, outperforming zero-shot GPT-40 (44.58).\nJoint multi-dialect trained models outperformed single-dialect counterparts by over 10% CHrF++, and 4-\nbit quantization reduced memory usage by 60% with less than 1% performance loss. The results and\ninsights of our experiments offer a practical blueprint for improving dialectal inclusion in Arabic\nNLP, showing that high-quality DA-MSA machine translation is achievable even with limited resources\nand paving the way for more inclusive language technologies.\n\nSubmission history\n\nFrom: Abdullah Alabdullah [view email][v1] Sun, 27 Jul 2025 14:37:53 UTC (1,168 KB)",
    "authors": "Alabdullah; Abdullah; Han; Lifeng; Lin; Chenghua",
    "date": "27 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20280v1",
    "title": "SciToolAgent: A Knowledge Graph-Driven Scientific Agent for Multi-Tool Integration",
    "abstract": "Scientific research increasingly relies on specialized\ncomputational tools, yet effectively utilizing these tools demands substantial domain expertise. while\nLarge Language Models (LLMs) show promise in tool automation, they struggle to seamlessly integrate\nand orchestrate multiple tools for complex scientific workflows. Here, we present SciToolagent, an\nLLM-powered agent that automates hundreds of scientific tools across biology, chemistry, and materials\nscience. At its core, SciToolagent leverages a scientific tool knowledge graph that enables\nintelligent tool selection and execution through graph-based retrieval-augmented generation. The agent\nalso incorporates a comprehensive safety-checking module to ensure responsible and ethical tool usage.\nExtensive evaluations on a curated benchmark demonstrate that SciToolagent significantly outperforms\nexisting approaches. Case studies in protein engineering, chemical reactivity prediction, chemical\nsynthesis, and metal-organic framework screening further demonstrate SciToolagent's capability to\nautomate complex scientific workflows, making advanced research tools accessible to both experts and\nnon-experts.",
    "authors": "Ding; Keyan; Yu; Jing; Huang; Junjie; Yang; Zhang; Qiang; Chen; Huajun",
    "date": "27 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20279v1",
    "title": "What Language(s) Does Aya-23 Think In? How Multilinguality Affects Internal Language Representations",
    "abstract": "Large language models (LLMs) excel at multilingual tasks, yet\n‘their internal language processing remains poorly understood. We analyze how Aya-23-88, a decoder-only\nLLM trained on balanced multilingual data, handles code-mixed, cloze, and translation tasks compared\n‘to predominantly monolingual models like Llama 3 and Chinese-LLaMA-2. Using logit lens and neuron\nspecialization analyses, we find: (1) Aya-23 activates typologically related language representations\nduring translation, unlike English-centric models that rely on a single pivot language; (2) code-mixed\nneuron activation patterns vary with mixing rates and are shaped more by the base language than the\nmixed-in one; and (3) Aya-23\"s languagespecific neurons for code-mixed inputs concentrate in final\nlayers, diverging from prior findings on decoder-only models. Neuron overlap analysis further shows\n‘that script similarity and typological relations impact processing across model types. These findings\nreveal how multilingual training shapes LLM internals and inform future cross-lingual transfer\nresearch.\n\n‘Submission history\n\nFrom: Katharina Trinley [view email][v1] Sun, 27 Jul 2025 13:53:45 UTC (12,000 KB)",
    "authors": "Trinley; Katharina; Nakai; Toshiki; Anikina; Tatiana; Baeumel; Tanja",
    "date": "27 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20278v1",
    "title": "MoL-RL: Distilling Multi-Step Environmental Feedback into LLMs for Feedback-Independent Reasoning",
    "abstract": "Large language models (LLMs) face significant challenges in\neffectively leveraging sequential environmental feedback (EF) signals, such as natural language\nevaluations, for feedback-independent chain-of-thought (CoT) reasoning. Existing approaches either\nconvert &F into scalar rewards, losing rich contextual information, or employ refinement datasets,\nfailing to exploit the multi-step and discrete nature of EF interactions. To address these\nLimitations, we propose MoL-RL, a novel training paradigm that integrates multi-step EF signals into\nLLMs through a dual-objective optimization framework. Our method combines MoL (Mixture-of-Losses)\ncontinual training, which decouples domain-specific EF signals (optimized via cross-entropy loss) and\ngeneral language capabilities (preserved via Kullback-Leibler divergence), with GRPO-based post-\n‘training to distill sequential EF interactions into single-step inferences. This synergy enables\nrobust feedback-independent reasoning without relying on external feedback loops. Experimental results\non mathematical reasoning (MATH-500, AIME24/AIME25) and code generation (CodeAgent-Test) benchmarks\ndemonstrate that MoL-RL achieves state-of-the-art performance with the Quen3-88 model, while\nmaintaining strong generalization across model scales (Quen3-48). This work provides a promising\napproach for leveraging multi-step textual feedback to enhance LLMs’ reasoning capabilities in diverse\ndomains.",
    "authors": "Yang; Kang; Chen; Jingxue; Tang; Qingkun; Zhang; Tianxiang; Lu; Qianchun",
    "date": "27 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20264v1",
    "title": "EMBRACE: Shaping Inclusive Opinion Representation by Aligning Implicit Conversations with Social Norms",
    "abstract": "Shaping inclusive representations that embrace diversity and\n‘ensure fair participation and reflections of values is at the core of many conversation-based models.\nHowever, many existing methods rely on surface inclusion using mention of user demographics or\nbehavioral attributes of social groups. Such methods overlook the nuanced, implicit expression of\nopinion embedded in conversations. Furthermore, the over-reliance on overt cues can exacerbate\nmisalignment and reinforce harmful or stereotypical representations in model outputs. Thus, we took a\nstep back and recognized that equitable inclusion needs to account for the implicit expression of\n‘opinion and use the stance of responses to validate the normative alignment. This study aims to\nevaluate how opinions are represented in NLP or computational models by introducing an alignment\nevaluation framework that foregrounds implicit, often overlooked conversations and evaluates the\nnormative social views and discourse. Our approach models the stance of responses as a proxy for the\nunderlying opinion, enabling @ considerate and reflective representation of diverse social viewpoints.\nWe evaluate the framework using both (i) positive-unlabeled (PU) online learning with base\nclassifiers, and (ii) instruction-tuned language models to assess post-training alignment. Through\n‘this, we provide a lens on how implicit opinions are (mis)represented and offer a pathway toward more\ninclusive model behavior.",
    "authors": "Aldayel; Abeer; Alokaili; Areej",
    "date": "27 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20252v1",
    "title": "Post-Completion Learning for Language Models",
    "abstract": "Current language model training paradigms typically terminate\nlearning upon reaching the end-of-sequence (}) token, overlooking the potential learning opportunities\nin the post-completion space. We propose Post-Completion Learning (PCL), a novel training framework\n‘that systematically utilizes the sequence space after model output completion, to enhance both the\nreasoning and self-evaluation abilities. PCL enables models to continue generating self-assessment\nand reward predictions during training, while maintaining efficient inference by stopping at the\ncompletion point.\n\nTo fully utilize this post-completion space, we design a white-box reinforcement learning method: let\n‘the model evaluate the output content according to the reward rules, then calculate and align the\nscore with the reward functions for supervision. We implement dual-track SFT to optimize both\nreasoning and evaluation capabilities, and mixed it with RL training to achieve multi-objective hybrid\noptimization.\n\nExperimental results on different datasets and models demonstrate consistent improvements over:\n‘traditional SFT and RL methods. Our method provides a new technical path for language model training\n‘that enhances output quality while preserving deployment efficiency.",
    "authors": "Fei; Xiang; Wang; Siqi; Wei; Shu; Nie; Yuxiang; Shi; Feng; Hao; Huang; Can",
    "date": "27 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20249v1",
    "title": "Modeling Professionalism in Expert Questioning through Linguistic Differentiation",
    "abstract": "Professionalism is a crucial yet underexplored dimension of\nexpert communication, particularly in high-stakes domains like finance. This paper investigates how\nLinguistic features can be leveraged to model and evaluate professionalism in expert questioning. We\nintroduce @ novel annotation framework to quantify structural and pragmatic elements in financial\nanalyst questions, such as discourse regulators, prefaces, and request types. Using both human-\nauthored and large language model (LLM)-generated questions, we construct two datasets: one annotated\nfor perceived professionalism and one labeled by question origin. We show that the same linguistic\nfeatures correlate strongly with both human judgments and authorship origin, suggesting a shared\nstylistic foundation. Furthermore, a classifier trained solely on these interpretable features\noutperforms gemini-2.0 and SVM baselines in distinguishing expert-authored questions. Our findings\ndemonstrate that professionalism is @ learnable, domain-general construct that can be captured through\nlinguistically grounded modeling.",
    "authors": "D'Agostino; Giulia; Chen; Chung-Chi",
    "date": "27 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20241v1",
    "title": "Reframe Your Life Story: Interactive Narrative Therapist and Innovative Moment Assessment with Large Language Models",
    "abstract": "Recent progress in large language models (LLMs) has opened new\npossibilities for mental health support, yet current approaches lack realism in simulating specialized\npsychotherapy and fail to capture therapeutic progression over time. Narrative therapy, which helps\nindividuals transform problematic life stories into empowering alternatives, remains underutilized due\nto limited access and social stigma. We address these limitations through a comprehensive framework\nwith two core components. First, INT (Interactive Narrative Therapist) simulates expert narrative\n‘therapists by planning therapeutic stages, guiding reflection levels, and generating contextually\nappropriate expert-like responses. Second, IMA (Innovative Moment Assessment) provides a therapy-\ncentric evaluation method that quantifies effectiveness by tracking \"Innovative Moments” (IMs),\ncritical narrative shifts in client speech signaling therapy progress. Experimental results on 260\nsimulated clients and 230 human participants reveal that INT consistently outperforms standard LLMs in\n‘therapeutic quality and depth. We further demonstrate the effectiveness of INT in synthesizing high-\nquality support conversations to facilitate social applications.",
    "authors": "Feng; Yi; Wang; Jiaqi; Zhang; Wenxuan; Chen; Zhuang; Shen; Yutong; Xiao; Xiyao; Huang; Minlie; Jing; Liping; Yu; Jian",
    "date": "27 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20210v1",
    "title": "Co-NAML-LSTUR: A Combined Model with Attentive Multi-View Learning and Long- and Short-term User Representations for News Recommendation",
    "abstract": "News recommendation systems play a vital role in mitigating\ninformation overload by delivering personalized news content. A central challenge is to effectively\nmodel both multi-view news representations and the dynamic nature of user interests, which often span\nboth short- and long-term preferences. Existing methods typically rely on single-view features of news\narticles (e.g., titles or categories) or fail to comprehensively capture user preferences across time\nscales. In this work, we propose Co-NAML-LSTUR, a hybrid news recommendation framework that integrates\nNAML for attentive multi-view news modeling and LSTUR for capturing both long- and short-term user\nrepresentations. Our model also incorporates BERT-based word embeddings to enhance semantic feature\nextraction. We evaluate Co-NAML-LSTUR on two widely used benchmarks, MIND-small and MIND-large.\nExperimental results show that Co-NAML-LSTUR achieves substantial improvements over most state-of-the-\nart baselines on MIND-small and MIND-large, respectively. These results demonstrate the effectiveness\nof combining multi-view news representations with dual-scale user modeling. The implementation of our\nmodel is publicly available at this https URL.",
    "authors": "Nguyen; Minh Hoang; Thuat Thien; Ta; Minh Nhat",
    "date": "27 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20208v1",
    "title": "IQ Test for LLMs: An Evaluation Framework for Uncovering Core Skills in LLMs",
    "abstract": "Current evaluations of large language models (LLMs) rely on\nbenchmark scores, but it is difficult to interpret what these individual scores reveal about a model's\noverall skills. Specifically, as a comunity we lack understanding of how tasks relate to one another,\nwhat they measure in common, how they differ, or which ones are redundant. As a result, models are\noften assessed via a single score averaged across benchmarks, an approach that fails to capture the\nmodels’ wholistic strengths and limitations. Here, we propose a new evaluation paradigm that uses\nfactor analysis to identify latent skills driving performance across benchmarks. We apply this method\n‘to a comprehensive new leaderboard showcasing the performance of 69 LLMs on 44 tasks, and identify a\nsmall set of latent skills that largely explain performance. Finally, we turn these insights into\npractical tools that identify redundant tasks, aid in model selection, and profile models along each\nlatent skill.",
    "authors": "Maimon; Aviya; Cohen; Amir DN; Vishne; Gal; Ravfogel; Shauli; Tsarfaty; Reut",
    "date": "27 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20187v1",
    "title": "Diversity-Enhanced Reasoning for Subjective Questions",
    "abstract": "Large reasoning models (LRM) with long chain-of-thought (CoT)\ncapabilities have shown strong performance on objective tasks, such as math reasoning and coding.\nHowever, their effectiveness on subjective questions that may have different responses from different\nperspectives is still limited by a tendency towards homogeneous reasoning, introduced by the reliance\n‘on a single ground truth in supervised fine-tuning and verifiable reward in reinforcement learning.\nMotivated by the finding that increasing role perspectives consistently improves performance, we\npropose MultiRole-R1, a diversity-enhanced framework with multiple role perspectives, to improve the\naccuracy and diversity in subjective reasoning tasks. MultiRole-R1 features an unsupervised data\nconstruction pipeline that generates reasoning chains that incorporate diverse role perspectives. We\nfurther employ reinforcement learning via Group Relative Policy Optimization (GRPO) with reward\nshaping, by taking diversity as a reward signal in addition to the verifiable reward. With specially\ndesigned reward functions, we successfully promote perspective diversity and lexical diversity,\nuncovering a positive relation between reasoning diversity and accuracy. Our experiment on six\nbenchmarks demonstrates MultiRole-R1's effectiveness and generalizability in enhancing both subjective\nand objective reasoning, showcasing the potential of diversity-enhanced training in LRMS.",
    "authors": "Wang; Yumeng; Fan; Zhiyuan; Liu; Jiayu; Fung; Yi R",
    "date": "27 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20185v1",
    "title": "SessionIntentBench: A Multi-task Inter-session Intention-shift Modeling Benchmark for E-commerce Customer Behavior Understanding",
    "abstract": "Session history is @ common way of recording user interacting\nbehaviors throughout a browsing activity with multiple products. For example, if an user clicks a\nproduct webpage and then leaves, it might because there are certain features that don't satisfy the\nuser, which serve as an important indicator of on-the-spot user preferences. However, all prior works\nfail to capture and model customer intention effectively because insufficient information exploitation\nand only apparent information like descriptions and titles are used. There is also a lack of data and\ncorresponding benchmark for explicitly modeling intention in E-commerce product purchase sessions. To\naddress these issues, we introduce the concept of an intention tree and propose a dataset curation\npipeline. Together, we construct a sibling multimodal benchmark, SessionIntentBench, that evaluates\nL(V)LMs* capability on understanding inter-session intention shift with four subtasks. With 1,952,177\nintention entries, 1,132,145 session intention trajectories, and 13,003,664 available tasks mined\nusing 10,905 sessions, we provide a scalable way to exploit the existing session data for customer\nintention understanding. We conduct human annotations to collect ground-truth label for a subset of\ncollected data to form an evaluation gold set. Extensive experiments on the annotated data further\nconfirm that current L(V)LMs fail to capture and utilize the intention across the complex session\nsetting. Further analysis show injecting intention enhances LLMs’ performances.",
    "authors": "Yang; Yuqi; Wang; Weiqi; Xu; Baixuan; Fan; Wei; Zong; Qing; Chan; Chunkit; Deng; Zheye; Liu; Xin; Gao; Yifan; Yu; Luo; Chen; Li; Zheng; Yin; Bing; Song",
    "date": "27 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20181v1",
    "title": "SGPO: Self-Generated Preference Optimization based on Self-Improver",
    "abstract": "Large language models (LLMs), despite their extensive pretraining\non diverse datasets, require effective alignment to human preferences for practical and reliable\ndeployment. Conventional alignment methods typically employ off-policy learning and depend on human-\nannotated datasets, which limits their broad applicability and introduces distribution shift issues\nduring training. To address these challenges, we propose Self-Generated Preference Optimization based\n‘on Self-Improver (SGPO), an innovative alignment framework that leverages an on-policy self-improving\nmechanism. Specifically, the improver refines responses from a policy model to self-generate\npreference data for direct preference optimization (DPO) of the policy model. Here, the improver and\npolicy are unified into a single model, and in order to generate higher-quality preference data, this\nself-improver learns to make incremental yet discernible improvements to the current responses by\nreferencing supervised fine-tuning outputs. Experimental results on Alpacafval 2.@ and Arena-Hard show\n‘that the proposed SGPO significantly improves performance over DPO and baseline self-improving methods\nwithout using external preference data.",
    "authors": "Lee; Hyeonji; Jo; Daejin; Yun; Seohwan; Kim; Sungwoong",
    "date": "27 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20152v1",
    "title": "Goal Alignment in LLM-Based User Simulators for Conversational AI",
    "abstract": "User simulators are essential to conversational AI, enabling scalable agent\ndevelopment and evaluation through simulated interactions. While current Large Language Models (LLMs)\nhave advanced user simulation capabilities, we reveal that they struggle to consistently demonstrate\ngoal-oriented behavior across multi-turn conversations--a critical limitation that compromises their\nreliability in downstream applications. We introduce User Goal State Tracking (UGST), @ novel\nframework that tracks user goal progression throughout conversations. Leveraging UGST, we present @\n‘three-stage methodology for developing user simulators that can autonomously track goal progression\nand reason to generate goal-aligned responses. Moreover, we establish comprehensive evaluation metrics\nfor measuring goal alignment in user simulators, and demonstrate that our approach yields substantial\nimprovements across two benchmarks (MultiNOZ 2.4 and {\\tau}-Bench). Our contributions address a\ncritical gap in conversational AI and establish UGST as an essential framework for developing goal-\naligned user simulators.",
    "authors": "Mehri; Shuhaib; Yang; Xiaocheng; Kim; Takyoung; Tur; Gokhan; Shikib; Hakkani-Tür; Dilek",
    "date": "27 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20150v1",
    "title": "The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large Language Models",
    "abstract": "Reinforcement learning (RL) plays @ crucial role in shaping the\nbehavior of large language and reasoning models (LLMs/LRMs). However, it often produces brittle and\nunstable policies, leading to critical failures such as spurious reasoning, deceptive alignment, and\ninstruction disobedience that undermine the trustworthiness and safety of LLMs/LRMs. Currently, these\nissues lack a unified theoretical explanation and are typically addressed using ad-hoc heuristics.\n‘This paper presents a rigorous mathematical framework for analyzing the stability of the mapping from\na reward function to the optimal policy. We show that policy brittleness often stems from non-unique\noptimal actions, a common occurrence when multiple valid traces exist in a reasoning task. This\n‘theoretical lens provides a unified explanation for a range of seemingly disparate failures, reframing\n‘them as rational outcomes of optimizing rewards that may be incomplete or noisy, especially in the\npresence of action degeneracy. We extend this analysis from the fundamental single-reward setting to\n‘the more realistic multi-reward RL across diverse domains, showing how stability is governed by an\n“effective reward” aggregation mechanism. We also prove that entropy regularization restores policy\nstability at the cost of increased stochasticity. Our framework provides a unified explanation for\nrecent empirical findings on deceptive reasoning, instruction-following trade-offs, and RLHF-induced\nsophistry, and is further validated through perturbation experiments in multi-reward RL. This work\nadvances policy-stability analysis from empirical heuristics towards @ principled theory, offering\nessential insights for designing safer and more trustworthy AI systems.\n\nCurrent browse context:\n\ncs.AL",
    "authors": "Xu; Xingcheng",
    "date": "27 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20145v1",
    "title": "Multi-Agent Interactive Question Generation Framework for Long Document Understanding",
    "abstract": "Document Understanding (DU) in long-contextual scenarios with\ncomplex layouts remains a significant challenge in vision-language research. Although Large Vision-\nLanguage Models (LVLMs) excel at short-context DU tasks, their performance declines in long-context\nsettings. A key limitation is the scarcity of fine-grained training data, particularly for low-\nresource languages such as Arabic. Existing state-of-the-art techniques rely heavily on human\nannotation, which is costly and inefficient. We propose a fully automated, multi-agent interactive\nframework to generate long-context questions efficiently. Our approach efficiently generates high-\nquality single- and multi-page questions for extensive English and Arabic documents, covering hundreds\nof pages across diverse domains. This facilitates the development of LVLMs with enhanced long-context\nunderstanding ability. Experimental results in this work have shown that our generated English and\nArabic questions (\\textbf{AaratngLongBench}) are quite challenging to major open- and close-source\nLVLMs. The code and data proposed in this work can be found in this https URL. Sample Question and\nAnswer (QA) pairs and structured system prompts can be found in the Appendix.",
    "authors": "Wang; Kesen; Toibazar; Daulet; Alfulayt; Abdulrahman; Albadawi; Abdulaziz S; Alkahtani; Ranya A; Ibrahim; Asma A; Alhomoud; Haneen A; Mohamed; Sherif; Moreno; Pedro J",
    "date": "27 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20136v1",
    "title": "Multi-Stage Verification-Centric Framework for Mitigating Hallucination in Multi-Modal RAG",
    "abstract": "This paper presents the technical solution developed by team CRUISE for the KDD Cup\n2025 Meta Comprehensive RAG Benchmark for Multi-modal, Multi-turn (CRAG-MH) challenge. The challenge\naims to address a critical limitation of modern Vision Language Models (VLMs): their propensity to\nhallucinate, especially when faced with egocentric imagery, long-tail entities, and complex, multi-hop\nquestions. This issue is particularly problematic in real-world applications where users pose fact-\nseeking queries that demand high factual accuracy across diverse modalities. To tackle this, we\npropose a robust, multi-stage framework that prioritizes factual accuracy and truthfulness over\ncompleteness. Our solution integrates a lightweight query router for efficiency, a query-aware\nretrieval and summarization pipeline, a dual-pathways generation and a post-hoc verification. This\nconservative strategy is designed to minimize hallucinations, which incur a severe penalty in the\ncompetition's scoring metric. Our approach achieved 3rd place in Task 1, demonstrating the\neffectiveness of prioritizing answer reliability in complex multi-modal RAG systems. Our\nimplementation is available at this https URL .\n\nCurrent browse context:\n\ncs.cL",
    "authors": "Chen; Baiyu; Wongso; Wilson; Hu; Xiaoqian; Tan; Yue; Salim; Flora",
    "date": "27 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20133v1",
    "title": "Sem-DPO: Mitigating Semantic Inconsistency in Preference Optimization for Prompt Engineering",
    "abstract": "Generative AI can now synthesize strikingly realistic images from\n‘text, yet output quality remains highly sensitive to how prompts are phrased. Direct Preference\nOptimization (DPO) offers a lightweight, off-policy alternative to RL for automatic prompt\nengineering, but its token-level regularization leaves semantic inconsistency unchecked as prompts\n‘that win higher preference scores can still drift away from the user's intended meaning.\n\nWe introduce Sem-DPO, 2 variant of DPO that preserves semantic consistency yet retains its simplicity\nand efficiency. Sem-DPO scales the DPO loss by an exponential weight proportional to the cosine\ndistance between the original prompt and winning candidate in embedding space, softly down-weighting\n‘training signals that would otherwise reward semantically mismatched prompts. We provide the first\nanalytical bound on semantic drift for preference-tuned prompt generators, showing that Sem-DPO keeps\nlearned prompts within a provably bounded neighborhood of the original text. On three standard text-\n‘to-image prompt-optimization benchmarks and two language models, Sem-DPO achieves 8-12% higher CLIP\nsimilarity and 5-9% higher human-preference scores (HPSv2.1, PickScore) than DPO, while also\noutperforming state-of-the-art baselines. These findings suggest that strong flat baselines augmented\nwith semantic weighting should become the new standard for prompt-optimization studies and lay the\ngroundwork for broader, semantics-aware preference optimization in language models.\n\nSubmission history\n\nFrom: Azal Ahmad Khan [view email][v1] Sun, 27 Jul 2025 05:20:13 UTC (10,692 KB)\n\nCurrent browse context:\n\ncs.cL",
    "authors": "Mohamed; Anas; Khan; Azal Ahmad; Wang; Xinran; Ahmad Faraz; Ge; Shuwen; Saman Bahzad; Ahmad; Ayaan; Anwar; Ali",
    "date": "27 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20111v1",
    "title": "AI-Driven Generation of Old English: A Framework for Low-Resource Languages",
    "abstract": "Preserving ancient languages is essential for understanding\nhumanity's cultural and linguistic heritage, yet Old English remains critically under-resourced,\nLimiting its accessibility to modern natural language processing (NLP) techniques. We present a\nscalable framework that uses advanced large language models (LLMs) to generate high-quality Old\nEnglish texts, addressing this gap. Our approach combines parameter-efficient fine-tuning (Low-Rank\nAdaptation, LORA), data augmentation via backtranslation, and a dual-agent pipeline that separates the\n‘tasks of content generation (in English) and translation (into Old English). Evaluation with automated\nmetrics (BLEU, METEOR, and CHRF) shows significant improvements over baseline models, with BLEU scores\nincreasing from 26 to over 65 for English-to-Old English translation. Expert human assessment also\nconfirms high grammatical accuracy and stylistic fidelity in the generated texts. Beyond expanding the\nOld English corpus, our method offers @ practical blueprint for revitalizing other endangered\nlanguages, effectively uniting AI innovation with the goals of cultural preservation.\n\nSubmission history\n\nFrom: Rodrigo Gabriel Salazar Alva [view email][v1] Sun, 27 Jul 2025 03:2:",
    "authors": "Alva; Rodrigo Gabriel Salazar; Nuñez; Matías; López; Cristian; Arista; Javier Martín",
    "date": "27 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20096v1",
    "title": "EcoTransformer: Attention without Multiplication",
    "abstract": "The Transformer, with its scaled dot-product attention mechanism,\nhas become a foundational architecture in modern AI. However, this mechanism is computationally\nintensive and incurs substantial energy costs. We propose a new Transformer architecture\nEcoTransformer, in which the output context vector is constructed as the convolution of the values\nusing a Laplacian kernel, where the distances are measured by the Li metric between the queries and\nkeys. Compared to dot-product based attention, the new attention score calculation is free of matrix\nmultiplication. It performs on par with, or even surpasses, scaled dot-product attention in NLP,\nbioinformatics, and vision tasks, while consuming significantly less energy.\n\nCurrent browse context:\n\n¢s.L6",
    "authors": "Gao; Xin; Xu",
    "date": "27 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20091v1",
    "title": "ProsodyLM: Uncovering the Emerging Prosody Processing Capabilities in Speech Language Models",
    "abstract": "Speech language models refer to language models with speech\nprocessing and understanding capabilities. One key desirable capability for speech language models is\n‘the ability to capture the intricate interdependency between content and prosody. The existing\nmainstream paradigm of training speech language models, which converts speech into discrete tokens\nbefore feeding them into LLMs, is sub-optimal in learning prosody information -- we find that the\nresulting LLMs do not exhibit obvious emerging prosody processing capabilities via pre-training alone.\nTo overcome this, we propose ProsodyLM, which introduces a simple tokenization scheme amenable to\nlearning prosody. Each speech utterance is first transcribed into text, followed by a sequence of\nword-level prosody tokens. Compared with conventional speech tokenization schemes, the proposed\n‘tokenization scheme retains more complete prosody information, and is more understandable to text-\nbased LLMs. We find that ProsodylM can learn surprisingly diverse emerging prosody processing\ncapabilities through pre-training alone, ranging from harnessing the prosody nuances in generated\nspeech, such as contrastive focus, understanding emotion and stress in an utterance, to maintaining\nprosody consistency in long contexts.\n\nCurrent browse context:\n\ncs.cL",
    "authors": "Qian; Kaizhi; Fan; Xulin; Ni; Junrui; Shechtman; Slava; Hasegawa-Johnson; Mark; Gan; Chuang; Zhang; Yang",
    "date": "27 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20077v1",
    "title": "The Devil is in the EOS: Sequence Training for Detailed Image Captioning",
    "abstract": "Despite significant advances in vision-language models (VLMs),\nimage captioning often suffers from a lack of detail, with base models producing short, generic\ncaptions. This limitation persists even though VLMs are equipped with strong vision and language\nbackbones. While supervised data and complex reward functions have been proposed to improve detailed\nimage captioning, we identify @ simpler underlying issue: a bias towards the end-of-sequence (EOS)\n‘token, which is introduced during cross-entropy training. We propose an unsupervised method to debias\n‘the model's tendency to predict the EOS token prematurely. By reducing this bias, we encourage the\ngeneration of longer, more detailed captions without the need for intricate reward functions or\nSupervision. Our approach is straightforward, effective, and easily applicable to any pretrained\nmodel. We demonstrate its effectiveness through experiments with three VLMs and on three detailed\ncaptioning benchmarks. Our results show a substantial increase in caption length and relevant details,\nalbeit with an expected increase in the rate of hallucinations.\n\nSubmission history\n\nFrom: Abdelrahman Mohamed [view email][v1] Sat, 26 Jul 2025 23:00:43 UTC (1,048 KB)",
    "authors": "Mohamed; Abdelrahman; Kementchedjhieva; Yova",
    "date": "26 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20067v1",
    "title": "PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training",
    "abstract": "Inference-time alignment enables large language models (LLMs) to\ngenerate outputs aligned with end-user preferences without further training. Recent post-training\nmethods achieve this by using small guidance models to modify token generation during inference. These\nmethods typically optimize a reward function KL-regularized by the original LLM taken as the reference\npolicy. A critical limitation, however, is their dependence on a pre-trained reward model, which\nrequires fitting to human preference feedback--a potentially unstable process. In contrast, we\nintroduce PITA, @ novel framework that integrates preference feedback directly into the LLI's token\ngeneration, eliminating the need for a reward model. PITA learns a small preference-based guidance\npolicy to modify token probabilities at inference time without LLM fine-tuning, reducing computational\ncost and bypassing the pre-trained reward model dependency. The problem is framed as identifying an\nunderlying preference distribution, solved through stochastic search and iterative refinement of the\npreference-based guidance model. We evaluate PITA across diverse tasks, including mathematical\nreasoning and sentiment classification, demonstrating its effectiveness in aligning LLM outputs with\nuser preferences.\n\nSubmission history\n\nFrom: Sarat Chandra Bobbili [view email][vi] Sat, 26 Jul 2025 21:\nCurrent browse context:\n\ncs.AL",
    "authors": "Bobbili; Sarat Chandra; Dinesha; Ujwal; Narasimha; Dheeraj; Shakkottai; Srinivas",
    "date": "26 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20059v1",
    "title": "RAG in the Wild: On the (In)effectiveness of LLMs with Mixture-of-Knowledge Retrieval Augmentation",
    "abstract": "Retrieval-augmented generation (RAG) enhances large language\nmodels (LLMs) by integrating external knowledge retrieved at inference time. hile RAG demonstrates\nstrong performance on benchmarks largely derived from general-domain corpora like Wikipedia, its\neffectiveness under realistic, diverse retrieval scenarios remains underexplored. We evaluated RAG\nsystems using MassiveDS, a large-scale datastore with mixture of knowledge, and identified critical\nLimitations: retrieval mainly benefits smaller models, rerankers add minimal value, and no single\nretrieval source consistently excels. Moreover, current LLMs struggle to route queries across\nheterogeneous knowledge sources. These findings highlight the need for adaptive retrieval strategies\nbefore deploying RAG in real-world settings. Our code and data can be found at this https URL.\nCurrent browse context:\n\ncs.cL",
    "authors": "Xu; Ran; Zhuang; Yuchen; Yu; Wang; Haoyu; Shi; Wenqi; Yang; Carl",
    "date": "26 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20055v1",
    "title": "A Tensor-Based Compiler and a Runtime for Neuron-Level DNN Certifier Specifications",
    "abstract": "The uninterpretability of DNNs has led to the adoption of abstract interpretation-\nbased certification as a practical means to establish trust in real-world systems that rely on DNNs.\nHowever, the current landscape supports only a limited set of certifiers, and developing new ones or\nmodifying existing ones for different applications remains difficult. This is because the mathematical\ndesign of certifiers is expressed at the neuron level, while their implementations are optimized and\n‘executed at the tensor level. This mismatch creates a semantic gap between design and implementation,\nmaking manual bridging both complex and expertise-intensive -- requiring deep knowledge in formal\nmethods, high-performance computing, etc.\n\nWe propose @ compiler framework that automatically translates neuron-level specifications of DNN\ncertifiers into tensor-based, layer-level implementations. This is enabled by tuo key innovations: 3\nnovel stack-based intermediate representation (IR) and a shape analysis that infers the implicit\n‘tensor operations needed to simulate the neuron-level semantics. During lifting, the shape analysis\ncreates tensors in the minimal shape required to perform the corresponding operations. The IR also\nenables domain-specific optimizations as rewrites. At runtime, the resulting tensor computations\nexhibit sparsity tied to the DNN architecture. This sparsity does not align well with existing\nformats. To address this, we introduce g-BCSR, a double-compression format that represents tensors as\ncollections of blocks of varying sizes, each possibly internally sparse.\n\nUsing our compiler and g-BCSR, we make it easy to develop new certifiers and analyze their utility\nacross diverse DNNs. Despite its flexibility, the compiler achieves performance comparable to hand-\n‘optimized implementations.",
    "authors": "Singh; Avaljot; Sarita; Yamin Chandini; Mishra; Aditya; Goyal; Ishaan; Gagandeep; Mendis; Charith",
    "date": "26 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20051v1",
    "title": "$K^4$: Online Log Anomaly Detection Via Unsupervised Typicality Learning",
    "abstract": "Existing Log Anomaly Detection (LogAD) methods are often slow,\ndependent on error-prone parsing, and use unrealistic evaluation protocols. We introduce $K\"4$, an\nunsupervised and parser-independent framework for high-performance online detection. $k°4$ transforms\narbitrary log embeddings into compact four-dimensional descriptors (Precision, Recall, Density,\nCoverage) using efficient k-nearest neighbor (k-NN) statistics. These descriptors enable lightweight\ndetectors to accurately score anomalies without retraining. Using a more realistic online evaluation\nprotocol, $k*4$ sets a new state-of-the-art (AUROC: 0.995-0.999), outperforming baselines by large\nmargins while being orders of magnitude faster, with training under 4 seconds and inference as low as\n4 s\\muss.\n\nCurrent browse context:\n\nc5.LG",
    "authors": "Chen; Weicong; Singh; Vikash; Rahmani; Zahra; Ganguly; Debargha; Hariri; Mohsen; Chaudhary; Vipin",
    "date": "26 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20046v1",
    "title": "Infogen: Generating Complex Statistical Infographics from Documents",
    "abstract": "Statistical infographics are powerful tools that simplify complex\ndata into visually engaging and easy-to-understand formats. Despite advancements in AI, particularly\nwith LLMs, existing efforts have been limited to generating simple charts, with no prior work\naddressing the creation of complex infographics from text-heavy documents that demand a deep\nunderstanding of the content. We address this gap by introducing the task of generating statistical\ninfographics composed of multiple sub-charts (e.g-, line, bar, pie) that are contextually accurate,\ninsightful, and visually aligned. To achieve this, we define infographic metadata that includes its\ntitle and textual insights, along with sub-chart-specific details such as their corresponding data and\nalignment. We also present Infodat, the first benchmark dataset for text-to-infographic metadata\ngeneration, where each sample links a document to its metadata. We propose Infogen, a two-stage\nframework where fine-tuned LLMs first generate metadata, which is then converted into infographic\ncode. Extensive evaluations on Infodat demonstrate that Infogen achieves state-of-the-art performance,\n‘outperforming both closed and open-source LLMs in text-to-statistical infographic generation.",
    "authors": "Ghosh; Akash; Garimella; Aparna; Ramu; Pritika; Bandyopadhyay; Sambaran; Saha; Sriparna",
    "date": "26 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20030v1",
    "title": "FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache Compression",
    "abstract": "The efficacy of Large Language Models (LLMs) in long-context\ntasks is often hampered by the substantial memory footprint and computational demands of the Key-Value\n(Kv) cache. Current compression strategies, including token eviction and learned projections,\nfrequently lead to biased representations -- either by overemphasizing recent/high-attention tokens or\nby repeatedly degrading information from earlier context -- and may require costly model retraining.\nWe present FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), @ novel, training-free KV cache\ncompression framework that ensures unbiased information retention. FAEDKV operates by transforming the\nKv cache into the frequency domain using a proposed Infinite-Window Fourier Transform (INDFT). This\napproach allows for the equalized contribution of all tokens to the compressed representation,\neffectively preserving both early and recent contextual information. A preliminary frequency ablation\nstudy identifies critical spectral components for layer-wise, targeted compression. Experiments on\nLongBench benchmark demonstrate FAEDKV's superiority over existing methods by up to 22\\%. In addition,\n‘our method shows superior, position-agnostic retrieval accuracy on the Needle-In-A-Haystack task\ncompared to compression based approaches.",
    "authors": "Li; Runchao; Fu; Yao; Sheng; Mu; Long; Xianxuan; Yu; Haotian; Pan",
    "date": "26 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20019v1",
    "title": "Anomaly Detection in Human Language via Meta-Learning: A Few-Shot Approach",
    "abstract": "We propose a meta learning framework for detecting anomalies in human language across\ndiverse domains with limited labeled data. Anomalies in language ranging from spam and fake news to\nhate speech pose a major challenge due to their sparsity and variability. We treat anomaly detection\nas a few shot binary classification problem and leverage meta-learning to train models that generalize\nacross tasks. Using datasets from domains such as SMS spam, COVID-19 fake news, and hate speech, we\nevaluate model generalization on unseen tasks with minimal labeled anomalies. Our method combines\nepisodic training with prototypical networks and domain resampling to adapt quickly to new anomaly\ndetection tasks. Empirical results show that our method outperforms strong baselines in Fl and AUC\nscores. We also release the code and benchmarks to facilitate further research in few-shot text\nanomaly detection.",
    "authors": "Singla; Saurav; Aarav; Gupta; Advik; Parnika",
    "date": "26 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.20018v1",
    "title": "The Carbon Cost of Conversation, Sustainability in the Age of Language Models",
    "abstract": "Large language models (LLMs) like GPT-3 and BERT have revolutionized natural language\nprocessing (NLP), yet their environmental costs remain dangerously overlooked. This article critiques\nthe sustainability of LLMs, quantifying their carbon footprint, water usage, and contribution to e-\nwaste through case studies of models such as GPT-4 and energy-efficient alternatives like Mistral 78.\nTraining @ Single LLM can emit carbon dioxide equivalent to hundreds of cars driven annually, while\ndata centre cooling exacerbates water scarcity in vulnerable regions. Systemic challenges corporate\ngreenwashing, redundant model development, and regulatory voids perpetuate harm, disproportionately\nburdening marginalized communities in the Global South. However, pathways exist for sustainable NL\ntechnical innovations (.g., model pruning, quantum computing), policy reforms (carbon taxes,\nmandatory emissions reporting), and cultural shifts prioritizing necessity over novelty. By analysing\nindustry leaders (Google, Microsoft) and laggards (Amazon), this work underscores the urgency of\nethical accountability and global cooperation. Without immediate action, AIs ecological toll risks\noutpacing its societal benefits. The article concludes with a call to align technological progress\nwith planetary boundaries, advocating for equitable, transparent, and regenerative AI systems that\nprioritize both human and environmental well-being.\n\nSubmission history\n\nFron: Sayed Mahbub Hasan Amiri [view enail][v1] Sat, 26 Jul 2025 17:\nCurrent browse context:\n\ncs.c¥",
    "authors": "Amiri; Sayed Mahbub Hasan; Goswami; Prasun; Islam; Md Mainul; Hossen; Mohammad Shakhawat; Sayed Majhab Hasan; Akter; Naznin",
    "date": "26 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19995v1",
    "title": "VLQA: The First Comprehensive, Large, and High-Quality Vietnamese Dataset for Legal Question Answering",
    "abstract": "The advent of large language models (LLMs) has led to significant\nachievements in various domains, including legal text processing. Leveraging LLMs for legal tasks is a\nnatural evolution and an increasingly compelling choice. However, their capabilities are often\nportrayed as greater than they truly are. Despite the progress, we are still far from the ultimate\ngoal of fully automating legal tasks using artificial intelligence (AI) and natural language\nProcessing (NLP). Moreover, legal systems are deeply domain-specific and exhibit substantial variation\nacross different countries and languages. The need for building legal text processing applications for\ndifferent natural languages is, therefore, large and urgent. However, there is a big challenge for\nLegal NLP in low-resource languages such as Vietnamese due to the scarcity of resources and annotated\ndata. The need for labeled legal corpora for supervised training, validation, and supervised fine-\n‘tuning is critical. In this paper, we introduce the VLQA dataset, a comprehensive and high-quality\nresource tailored for the Vietnamese legal domain. We also conduct a comprehensive statistical\nanalysis of the dataset and evaluate its effectiveness through experiments with state-of-the-art\nmodels on legal information retrieval and question-answering tasks.",
    "authors": "Nguyen; Tan-Minh; Hoang-Trung; Dao; Trong-Khoi; Phan; Xuan-Hieu; Ha-Thanh; Vuong; Thi-Hai-Yen",
    "date": "26 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19990v1",
    "title": "Improving the Performance of Sequential Recommendation Systems with an Extended Large Language Model",
    "abstract": "Recently, competition in the field of artificial intelligence\n(AI) has intensified among major technological companies, resulting in the continuous release of new\nlarge-language models (LLMs) that exhibit improved language understanding and context-based reasoning\ncapabilities. It is expected that these advances will enable more efficient personalized\nrecommendations in LLM-based recommendation systems through improved quality of training data and\narchitectural design. However, many studies have not considered these recent developments. In this\nstudy, it was proposed to improve LLM-based recommendation systems by replacing Llama2 with Llama3 in\n‘the LlamaRec framework. To ensure a fair comparison, random seed values were set and identical input\ndata was provided during preprocessing and training. The experimental results show average performance\nimprovements of 38.65\\%, 8.69\\%, and 8.19\\% for the ML-100K, Beauty, and Games datasets, respectively,\n‘thus confirming the practicality of this method. Notably, the significant improvements achieved by\nmodel replacement indicate that the recommendation quality can be improved cost-effectively without\n‘the need to make structural changes to the system. Based on these results, it is our contention that\n‘the proposed approach is a viable solution for improving the performance of current recommendation\nsystems.\n\nCurrent browse context:\n\ncs.IR",
    "authors": "Choi; Sinnyum; Kim; Woong",
    "date": "26 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19980v1",
    "title": "Exploring LLM Autoscoring Reliability in Large-Scale Writing Assessments Using Generalizability Theory",
    "abstract": "This study investigates the estimation of reliability for large language models\n(LLMs) in scoring writing tasks from the AP Chinese Language and Culture Exam. Using generalizability\n‘theory, the research evaluates and compares score consistency between human and AI raters across two\n‘types of AP Chinese free-response writing tasks: story narration and email response. These essays were\nindependently scored by two trained human raters and seven AI raters. Each essay received four scores:\none holistic score and three analytic scores corresponding to the domains of task completion,\ndelivery, and language use. Results indicate that although human raters produced more reliable scores\noverall, LLMs demonstrated reasonable consistency under certain conditions, particularly for story\nnarration tasks. Composite scoring that incorporates both human and AI raters improved reliability,\nwhich supports that hybrid scoring models may offer benefits for large-scale writing assessments.",
    "authors": "Song; Dan; Lee; Won-Chan; Jiao; Hong",
    "date": "26 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19973v1",
    "title": "Leveraging Fine-Tuned Large Language Models for Interpretable Pancreatic Cystic Lesion Feature Extraction and Risk Categorization",
    "abstract": "",
    "authors": "Rasromani; Ebrahim; Kang; Stella K; Xu; Yanqi; Liu; Beisong; Luhadia; Garvit; Chui; Wan Fung; Pasadyn; Felicia L; Hung; Yu Chih; An; Julie Y; Mathieu; Edwin; Gu; Zehui; Fernandez-Granda; Carlos; Javed; Ammar A; Sacks; Greg D; Gonda; Tamas; Huang; Chenchan; Shen; Yiqiu",
    "date": "26 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19969v1",
    "title": "Text2Vis: A Challenging and Diverse Benchmark for Generating Multimodal Visualizations from Text",
    "abstract": "Automated data visualization plays a crucial role in simplifying\ndata interpretation, enhancing decision-making, and improving efficiency. While large language models\n(LLMs) have shown promise in generating visualizations from natural language, the absence of\ncomprehensive benchmarks limits the rigorous evaluation of their capabilities. We introduce Text2Vis,\n‘a benchmark designed to assess text-to-visualization models, covering 20+ chart types and diverse data\nscience queries, including trend analysis, correlation, outlier detection, and predictive analytics.\nIt comprises 1,985 samples, each with a data table, natural language query, short answer,\nvisualization code, and annotated charts. The queries involve complex reasoning, conversational turns,\nand dynamic data retrieval. We benchmark 11 open-source and closed-source models, revealing\nsignificant performance gaps, highlighting key challenges, and offering insights for future\nadvancements. To close this gap, we propose the first cross-modal actor-critic agentic framework that\nJointly refines the textual answer and visualization code, increasing GPT-4o\"s pass rate from 26% to\n42% over the direct approach and improving chart quality. We also introduce an automated LLM-based\nevaluation framework that enables scalable assessment across thousands of samples without human\nannotation, measuring answer correctness, code execution success, visualization readability, and chart\naccuracy. We release Text2Vis at this https URL.\n\nSubmission history\n\nFrom: Md Tahmid Rahman Laskar [view email][v1] Sat, 26 Jul 2025 14:59:04 UTC (2,727 KB)",
    "authors": "Rahman; Mizanur; Laskar; Joty; Shafiq; Hoque; Enamul",
    "date": "26 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19962v1",
    "title": "KLAAD: Refining Attention Mechanisms to Reduce Societal Bias in Generative Language Models",
    "abstract": "Large language models (LLMs) often exhibit societal biases in\ntheir outputs, prompting ethical concerns regarding fairness and harm. In this work, we propose KLAAD\n(KL-attention Alignment Debiasing), an attention-based debiasing framework that implicitly aligns\nattention distributions between stereotypical and anti-stereotypical sentence pairs without directly\nmodifying model weights. KLAAD introduces a composite training objective combining Cross-Entropy, KL\ndivergence, and Triplet losses, guiding the model to consistently attend across biased and unbiased\ncontexts while preserving fluency and coherence. Experimental evaluation of KLAAD denonstrates\nimproved bias mitigation on both the B&Q and BOLD benchnarks, with minimal impact on language modeling\nquality. The results indicate that attention-level alignment offers a principled solution for\nmitigating bias in generative language models.",
    "authors": "Kim; Seorin; Lee; Dongyoung; Jaejin",
    "date": "26 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19947v1",
    "title": "Spatial Language Likelihood Grounding Network for Bayesian Fusion of Human-Robot Observations",
    "abstract": "Fusing information from human observations can help robots\novercome sensing limitations in collaborative tasks. However, an uncertainty-aware fusion framework\nrequires a grounded likelihood representing the uncertainty of human inputs. This paper presents a\nFeature Pyramid Likelihood Grounding Network (FP-LGN) that grounds spatial language by learning\nrelevant map image features and their relationships with spatial relation semantics. The model is\n‘trained as @ probability estimator to capture aleatoric uncertainty in human language using three-\nstage curriculum learning. Results showed that FP-LGN matched expert-designed rules in mean Negative\nLog-Likelihood (NLL) and demonstrated greater robustness with lower standard deviation. Collaborative\nsensing results demonstrated that the grounded likelihood successfully enabled uncertainty-aware\nfusion of heterogeneous human language observations and robot sensor measurements, achieving\nsignificant improvements in human-robot collaborative task performance.\n\nCurrent browse context:\n\n€s.RO",
    "authors": "Sitdhipol; Supawich; Sukprasongdee; Waritwong; Chuangsuwanich; Ekapol; Tse; Rina",
    "date": "26 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19909v1",
    "title": "The Impact of Fine-tuning Large Language Models on Automated Program Repair",
    "abstract": "Automated Program Repair (APR) uses various tools and techniques\nto help developers achieve functional and error-free code faster. In recent years, Large Language\nModels (LLMs) have gained popularity as components in APR tool chains because of their performance and\nflexibility. However, training such models requires a significant amount of resources. Fine-tuning\n‘techniques have been developed to adapt pre-trained LLMs to specific tasks, such as APR, and enhance\n‘their performance at far lower computational costs than training from scratch. In this study, we\n‘empirically investigate the impact of various fine-tuning techniques on the performance of LIMs used\nfor APR. Our experiments provide insights into the performance of a selection of state-of-the-art LLMs\npre-trained on code. The evaluation is done on three popular APR benchmarks (i.e., QuixBugs, Defects4]\nand HumanEval-Java) and considers six different LLMs with varying parameter sizes (resp. CodeGen,\nCodeTS, StarCoder, DeepSeekCoder, Bloom, and CodeLlama-2). We consider three training regimens: no\nfine-tuning, full fine-tuning, and parameter-efficient fine-tuning (PEFT) using LoRA and IA3. We\nobserve that full fine-tuning techniques decrease the benchmarking performance of various models due\nto different data distributions and overfitting. By using parameter-efficient fine-tuning methods, we\nrestrict models in the amount of trainable parameters and achieve better results.\n\nKeywords: large language models, automated program repair, parameter-efficient fine-tuning, Al4Code,\nAIASE, MLASE.\n\nCurrent browse context:\n\ncs.SE",
    "authors": "Macháček; Roman; Grishina; Anastasiia; Hort; Max; Moonen; Leon",
    "date": "26 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19906v1",
    "title": "CaliDrop: KV Cache Compression with Calibration",
    "abstract": "",
    "authors": "Su; Yi; Qiu; Quantong; Zhou; Yuechi; Li; Juntao; Xia; Qingrong; Ping; Duan; Xinyu; Wang; Zhefeng; Zhang; Min",
    "date": "26 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19899v1",
    "title": "A Gold Standard Dataset and Evaluation Framework for Depression Detection and Explanation in Social Media using LLMs",
    "abstract": "Early detection of depression from online social media posts holds promise for\nproviding timely mental health interventions. In this work, we present a high-quality, expert-\nannotated dataset of 1,017 social media posts labeled with depressive spans and mapped to 12\ndepression symptom categories. Unlike prior datasets that primarily offer coarse post-level labels\n\\cite{cohan-etal-2018-smhd}, our dataset enables fine-grained evaluation of both model predictions and\ngenerated explanations.\n\nWe develop an evaluation framework that leverages this clinically grounded dataset to assess the\nfaithfulness and quality of natural language explanations generated by large language models (LLMs).\nThrough carefully designed prompting strategies, including zero-shot and few-shot approaches with\ndomain-adapted examples, we evaluate state-of-the-art proprietary LLMs including GPT-4.1, Gemini 2.5\nPro, and Claude 3.7 Sonnet.\n\nOur comprehensive empirical analysis reveals significant differences in how these models perform on\nclinical explanation tasks, with zero-shot and few-shot prompting. Our findings underscore the value\nof human expertise in guiding LLM behavior and offer a step toward safer, more transparent AI systems\nfor psychological well-being.\n\nSubmission history\n\nFrom: Prajval Bolegave [view email][v1] Sat, 26 Jul 2025 10:01:55 UTC (9,671 KB)",
    "authors": "Bolegave; Prajval; Bhattacharya; Pushpak",
    "date": "26 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19885v1",
    "title": "Zero-shot Performance of Generative AI in Brazilian Portuguese Medical Exam",
    "abstract": "",
    "authors": "Truyts; Cesar Augusto Madid; Rabelo; Amanda Gomes; De Souza; Gabriel Mesquita; Lages; Daniel Scaldaferri; Pereira; Adriano Jose; Flato; Uri Adrian Prync; Reis; Eduardo Pontes dos; Vieira; Joaquim Edson; Silveira; Paulo Sergio Panse; Junior; Edson Amaro",
    "date": "26 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19869v1",
    "title": "The Polish Vocabulary Size Test: A Novel Adaptive Test for Receptive Vocabulary Assessment",
    "abstract": "he present the Polish Vocabulary Size Test (PVST), a novel tool for assessing the\nreceptive vocabulary size of both native and non-native Polish speakers. Based on Item Response Theory\nand Computerized Adaptive Testing, PVST dynamically adjusts to each test-taker's proficiency level,\n‘ensuring high accuracy while keeping the test duration short. To validate the test, a pilot study was\nconducted with 1.475 participants. Native Polish speakers demonstrated significantly larger\nvocabularies compared to non-native speakers. For native speakers, vocabulary size showed a strong\npositive correlation with age. The PVST is available online at this http URL.",
    "authors": "Fokin; Danil; Płużyczka; Monika; Golovin; Grigory",
    "date": "26 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19867v1",
    "title": "DRIVE: Disfluency-Rich Synthetic Dialog Data Generation Framework for Intelligent Vehicle Environments",
    "abstract": "In-car conversational AI is becoming increasingly critical as\nautonomous vehicles and smart assistants gain widespread adoption. Yet, existing datasets fail to\ncapture the spontaneous disfluencies such as hesitations, false starts, repetitions, and self-\ncorrections that characterize real driver-AI dialogs. To address this, we introduce DiscoDrive, a\nsynthetic corpus of 3500 multi-turn dialogs across seven automotive domains, generated using a two-\nstage, prompt-driven pipeline that dynamically integrates disfluencies during synthesis. We show that\nDiscoDrive is effective both as a training resource, enabling DialoGPT-Medium and T5-Base to match or\n‘exceed KVRET-trained models on the MultiNOZ 2.2 and Schema-Guided Dialogue (SGD) relevant test sets\n(BLEU-4 improvements of @.26 to 0.61; METEOR +2.10; ROUGE-L +3.433 BERTScore F1 improvements of 1.35\nto 3.43), and as a data augmentation resource in low-resource scenarios, delivering additional gains\nof up to BLEU-4 #0.38, METEOR +1.95, ROUGE-L 42.87, and BERTScore F1 +4.0 when combined with 10\npercent of KVRET. Human evaluations further confirm that dialogs sampled from DiscoDrive are rated\nhigher than KVRET’s human-collected dialogs in naturalness (3.8 vs 3.6) and coherence (4.1 vs 4.0),\nand are perceived as more context-appropriate than leading post-hoc methods (such as LARD), without\ncompromising clarity. DiscoDrive fills a critical gap in existing resources and serves as a versatile\ncorpus for both training and augmenting conversational AI, enabling robust handling of real-world,\ndisfluent in-car interactions.",
    "authors": "Chavda; Anshul; Jagadeesh; M; Kullayappa; Chintalapalli Raja; Jayaprakash; B; Sruthi; Pushpak",
    "date": "26 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19849v1",
    "title": "Agentic Reinforced Policy Optimization",
    "abstract": "Large-scale reinforcement learning with verifiable rewards (RLVR)\nhas demonstrated its effectiveness in harnessing the potential of large language models (LLMs) for\nsingle-turn reasoning tasks. In realistic reasoning scenarios, LLMs can often utilize external tools\nto assist in task-solving processes. However, current RL algorithms inadequately balance the models’\nintrinsic long-horizon reasoning capabilities and their proficiency in multi-turn tool interactions.\nTo bridge this gap, we propose Agentic Reinforced Policy Optimization (ARPO), a novel agentic RL\nalgorithm tailored for training multi-turn LLM-based agents. Through preliminary experiments, we\nobserve that LLMs tend to exhibit highly uncertain behavior, characterized by an increase in the\n‘entropy distribution of generated tokens, inmediately following interactions with external tools.\nMotivated by this observation, ARPO incorporates an entropy-based adaptive rollout mechanism,\ndynamically balancing global trajectory sampling and step-level sampling, thereby promoting\n‘exploration at steps with high uncertainty after tool usage. By integrating an advantage attribution\nestimation, ARPO enables LLMs to internalize advantage differences in stepwise tool-use interactions.\nOur experiments across 13 challenging benchmarks in computational reasoning, knowledge reasoning, and\ndeep search domains demonstrate ARPO's superiority over trajectory-level RL algorithms. Remarkably,\nARPO achieves improved performance using only half of the tool-use budget required by existing\nmethods, offering a scalable solution for aligning LLM\"-based agents with real-time dynamic\n‘environments. Our code and datasets are released at this https URL\n\nCurrent browse context:\n\n¢s.L6",
    "authors": "Dong; Guanting; Mao; Hangyu; Ma; Kai; Bao; Licheng; Chen; Yifei; Wang; Zhongyuan; Zhongxia; Du; Jiazhen; Huiyang; Zhang; Fuzheng; Zhou; Guorui; Zhu; Yutao; Wen; Ji-Rong; Dou; Zhicheng",
    "date": "26 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19840v1",
    "title": "AutoSign: Direct Pose-to-Text Translation for Continuous Sign Language Recognition",
    "abstract": "",
    "authors": "Johnny; Samuel Ebimobowei; Guda; Blessed; Stephen; Andrew Blayama; Gueye; Assane",
    "date": "26 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19823v1",
    "title": "HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs",
    "abstract": "Processing long-context inputs with large language models\npresents a significant challenge due to the enormous memory requirements of the Key-Value (KV) cache\nduring inference. Existing KV cache compression methods exhibit noticeable performance degradation\nwhen memory is reduced by more than 85%. Additionally, strategies that leverage GPU-CPU collaboration\nfor approximate attention remain underexplored in this setting. We propose HCAttention, a\nheterogeneous attention computation framework that integrates key quantization, value offloading, and\ndynamic KV eviction to enable efficient inference under extreme memory constraints. The method is\ncompatible with existing transformer architectures and does not require model fine-tuning.\nExperimental results on the LongBench benchmark demonstrate that our approach preserves the accuracy\nof full-attention model while shrinking the KV cache memory footprint to 25% of its original size.\nRemarkably, it stays competitive with only 12.5% of the cache, setting a new state-of-the-art in LLM\nKV cache compression. To the best of our knowledge, HCAttention is the first to extend the Llama-3-88\nmodel to process 4 million tokens on a single A100 GPU with 59GB memory.\n\nCurrent browse context:\n\ncs.cL",
    "authors": "Yang; Dongquan; Yifan; Yu; Xiaotian; Qi; Xianbiao; Xiao; Rong",
    "date": "26 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19786v1",
    "title": "Flora: Effortless Context Construction to Arbitrary Length and Scale",
    "abstract": "Effectively handling long contexts is challenging for Large Language Models (LLMs)\ndue to the rarity of long texts, high computational demands, and substantial forgetting of short-\ncontext abilities. Recent approaches have attempted to construct long contexts for instruction tuning,\nbut these methods often require LLMs or human interventions, which are both costly and limited in\nlength and diversity. Also, the drop in short-context performances of present long-context LLMs\nremains significant. In this paper, we introduce Flora, an effortless (human/LLM-free) long-context\nconstruction strategy. Flora can markedly enhance the long-context performance of LLMs by arbitrarily\nassembling short instructions based on categories and instructing LLMs to generate responses based on\nlong-context meta-instructions. This enables Flora to produce contexts of arbitrary length and scale\nwith rich diversity, while only slightly compromising short-context performance. Experiments on\nLlama3-88-Instruct and QuQ-328 show that LLMs enhanced by Flora excel in three long-context benchmarks\nwhile maintaining strong performances in short-context tasks. Our data-construction code is available\nat \\href{this https URL}{this https URL}.",
    "authors": "Chen; Tianxiang; Tan; Zhentao; Bo; Xiaofan; Wu; Yue; Gong; Tao; Chu; Qi; Ye; Jieping; Yu; Nenghai",
    "date": "26 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19766v1",
    "title": "UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models' Reasoning Abilities",
    "abstract": "Recent advances in large language models (LLMs) have highlighted\n‘the potential of reinforcement learning with verifiable rewards (RLVR) to enhance reasoning\ncapabilities through extended output sequences. However, traditional RL frameworks face inefficiencies\nwhen handling ultra-long outputs due to long-tail sequence distributions and entropy collapse during\n‘training. To address these challenges, we propose an Ultra-Long Output Reinforcement Learning (UloRL)\napproach for advancing large language models’ reasoning abilities. Specifically, we divide ultra long\n‘output decoding into short segments, enabling efficient training by mitigating delays caused by long-\ntail samples. Additionally, we introduce dynamic masking of well-Mastered Positive Tokens (MPTs) to\nprevent entropy collapse. Experimental results demonstrate the effectiveness of our approach. On the\nQuen3-30B-A3B model, RL with segment rollout achieved 2.06x increase in training speed, while RL\n‘training with 128k-token outputs improves the model's performance on AINE2025 from 70.9\\% to 85.1\\%\nand on BeyondAIME from 50.7\\% to 61.9\\%, even surpassing Quen3-2358-A22B with remarkable gains. These\nfindings underscore the potential of our methods to advance the reasoning capabilities of LLMs with\nultra-long sequence generation. We will release our code and model for further use by the community.",
    "authors": "Du; Dong; Liu; Shulin; Yang; Tao; Chen; Shaohua; Li",
    "date": "26 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19756v1",
    "title": "Are You There God? Lightweight Narrative Annotation of Christian Fiction with LMs",
    "abstract": "In addition to its more widely studied political activities, the\nAmerican Evangelical movement has a well-developed but less externally visible cultural and literary\nside. Christian Fiction, however, has been little studied, and what scholarly attention there is has\nfocused on the explosively popular Left Behind series. In this work, we use computational tools to\nprovide both a broad topical overview of Christian Fiction as a genre and a more directed exploration\nof how its authors depict divine acts. Working with human annotators we first developed definitions\nand a codebook for “acts of God.” We then adapted those instructions designed for human annotators for\nuse by a recent, lightweight LM with the assistance of a much larger model. The laptop-scale LM is\ncapable of matching human annotations, even when the task is subtle and challenging. Using these\nannotations, we show that significant and meaningful differences exist between the Left Behind books\nand Christian Fiction more broadly and between books by male and female authors.\n\nSubmission history\n\nFrom: Rebecca M. Ml. Hicke [view email][v1] Sat, 26 Jul 2025 93:01:59 UTC (403 KB)",
    "authors": "Hicke; Rebecca M M; Haggard; Brian; Ferrante; Mia; Khanna; Rayhan; Mimno; David",
    "date": "26 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19748v1",
    "title": "JT-Math: A Multi-Stage Framework for Advanced Mathematical Reasoning in Large Language Models",
    "abstract": "Mathematical reasoning is a cornerstone of artificial general\nintelligence and a primary benchmark for evaluating the capabilities of Large Language Models (LLMs).\nWhile state-of-the-art models show promise, they often falter when faced with complex problems that\ndemand deep conceptual understanding and intricate, multi-step deliberation. To address this\nchallenge, we introduce JT-Hath-8B, a series of open-source models comprising base, instruct, and\n‘thinking versions, built upon a systematic, multi-stage optimization framework. Our pre-training\ncorpus is a high-quality, 2108-token dataset curated through a dedicated data pipeline that uses\nmodel-based validation to ensure quality and diversity. The Instruct Model is optimized for direct,\nconcise answers through Supervised Fine-Tuning (SFT) and a GRPO-based reinforcement learning (RL)\nmethod. The Thinking Model is trained for complex problem-solving using @ Long Chain-of-Thought (Long\nCot) approach, combining SFT with a novel, multi-stage RL curriculum that progressively increases task\ndifficulty and context length up to 32K tokens. JT-Hath-8B achieves state-of-the-art results among\nopen-source models of similar size, surpassing prominent models like OpenAI's O1-mini and GPT-40 , and\ndemonstrating superior performance on competition-level mathematics.",
    "authors": "Hao; Yifan; Chao; Fangning; Yaqian; Cui; Zhaojun; Bai; Huan; Zhang; Haiyu; Liu; Yankai; Deng; Feng; Junlan",
    "date": "26 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19741v1",
    "title": "Basic Reading Distillation",
    "abstract": "Large language models (LLMs) have demonstrated remarkable\nabilities in various natural language processing areas, but they demand high computation resources\nwhich limits their deployment in real-world. Distillation is one technique to solve this problem\n‘through either knowledge distillation or task distillation. Both distillation approaches train small\nmodels to imitate specific features of LiMs, but they all neglect basic reading education for small\nmodels on generic texts that are \\emph{unrelated} to downstream tasks. In this paper, we propose basic\nreading distillation (BRD) which educates @ small model to imitate LLMs basic reading behaviors, such\n‘as named entity recognition, question raising and answering, on each sentence. After such basic\neducation, we apply the small model on various tasks including language inference benchmarks and BIG-\nbench tasks. It shows that the small model can outperform or perform comparable to over 20x bigger\nLLMs. Analysis reveals that BRD effectively influences the probability distribution of the small\nmodel, and has orthogonality to either knowledge distillation or task distillation.",
    "authors": "Zhou; Zhi; Miao; Sirui; Duan; Xiangyu; Yang; Hao; Zhang; Min",
    "date": "26 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19710v1",
    "title": "Ta-G-T: Subjectivity Capture in Table to Text Generation via RDF Graphs",
    "abstract": "In Table-to-Text (T2T) generation, existing approaches\npredominantly focus on providing objective descriptions of tabular data. However, generating text that\nincorporates subjectivity, where subjectivity refers to interpretations beyond raw numerical data,\nremains underexplored. To address this, we introduce a novel pipeline that leverages intermediate\nrepresentations to generate both objective and subjective text from tables. Our three-stage pipeline\nconsists of: 1) extraction of Resource Description Framework (RDF) triples, 2) aggregation of text\ninto coherent narratives, and 3) infusion of subjectivity to enrich the generated text. By\nincorporating RDFs, our approach enhances factual accuracy while maintaining interpretability. Unlike\nlarge language models (LLMs) such as GPT-3.5, Mistral-78, and Llama-2, our pipeline employs smaller,\nfine-tuned T5 models while achieving comparable performance to GPT-3.5 and outperforming Mistral-78\nand Llama-2 in several metrics. We evaluate our approach through quantitative and qualitative\nanalyses, demonstrating its effectiveness in balancing factual accuracy with subjective\ninterpretation. To the best of our knowledge, this is the first work to propose a structured pipeline\nfor T2T generation that integrates intermediate representations to enhance both factual correctness\nand subjectivity.",
    "authors": "Upasham; Ronak; Dey; Tathagata; Bhattacharyya; Pushpak",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19699v1",
    "title": "Towards Inclusive NLP: Assessing Compressed Multilingual Transformers across Diverse Language Benchmarks",
    "abstract": "Although LLMs have attained significant success in high-resource\nlanguages, their capacity in low-resource linguistic environments like Kannada and Arabic is not yet\nfully understood. This work benchmarking the performance of multilingual and monolingual Large\nLanguage Models (LLMs) across Arabic, English, and Indic languages, with particular emphasis on the\neffects of model compression strategies such as pruning and quantization. Findings shows significant\nperformance differences driven by linguistic diversity and resource availability on SOTA LHS as\nBLOOMZ, AceGPT, Jais, LLaltA-2, XGLM, and AraGPT2. We find that multilingual versions of the model\n‘outperform their language-specific counterparts across the board, indicating substantial cross-lingual\ntransfer benefits. Quantization (4-bit and 8-bit) is effective in maintaining model accuracy while\npromoting efficiency, but aggressive pruning significantly compromises performance, especially in\nbigger models. Our findings pinpoint key strategies to construct scalable and fair multilingual NLP\nsolutions and underscore the need for interventions to address hallucination and generalization errors\nin the low-resource setting.",
    "authors": "Alshehhi; Maitha; Sharshar; Ahmed; Guizani; Mohsen",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19684v1",
    "title": "Salsa as a Nonverbal Embodied Language -- The CoMPAS3D Dataset and Benchmarks",
    "abstract": "",
    "authors": "Burkanova; Bermet; Yazdian; Payam Jome; Zhang; Chuxuan; Evans; Trinity; Tuttösí; Paige; Lim; Angelica",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19666v1",
    "title": "RoD-TAL: A Benchmark for Answering Questions in Romanian Driving License Exams",
    "abstract": "The intersection of AI and legal systems presents a growing need for tools that\nsupport legal education, particularly in under-resourced languages such as Romanian. In this work, we\naim to evaluate the capabilities of Large Language Models (LLMs) and Vision-Language Models (VLMs) in\nunderstanding and reasoning about Romanian driving law through textual and visual question-answering\ntasks. To facilitate this, we introduce RoD-TAL, a novel multimodal dataset comprising Romanian\ndriving test questions, text-based and image-based, alongside annotated legal references and human\nexplanations. We implement and assess retrieval-augmented generation (RAG) pipelines, dense\nretrievers, and reasoning-optimized models across tasks including Information Retrieval (IR), Question\nAnswering (QA), Visual IR, and Visual QA. Our experiments demonstrate that domain-specific fine-tuning\nsignificantly enhances retrieval performance. At the same time, chain-of-thought prompting and\nspecialized reasoning models improve QA accuracy, surpassing the minimum grades required to pass\ndriving exams. However, visual reasoning remains challenging, highlighting the potential and the\nLimitations of applying LLMs and VLMs to legal education.\n\nSubmission history\n\nFron: R&zvan-Alexandru Smidu [view email][vi] Fri, 25 Jul 2025 20:40:39 UTC (651 KB)",
    "authors": "Man; Andrei Vlad; Smădu; Răzvan-Alexandru; Craciun; Cristian-George; Cercel; Dumitru-Clementin; Pop; Florin; Mihaela-Claudia",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19634v1",
    "title": "MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks",
    "abstract": "Recent advances in large language models have catalyzed the\ndevelopment of multimodal LLMs (MLLMs) that integrate text, speech, and vision within unified\nframeworks. As HLLMs evolve from narrow, monolingual, task-specific systems to general-purpose\ninstruction-following models, a key frontier lies in evaluating their multilingual and multimodal\ncapabilities over both long and short contexts. However, existing benchmarks fall short in evaluating\nthese dimensions jointly: they are often limited to English, mostly focus on one single modality at a\ntime, rely on short-form contexts, or lack human annotations--hindering comprehensive assessment of\nmodel performance across languages, modalities, and task complexity. To address these gaps, we\nintroduce MCIF (Multimodal Crosslingual Instruction Following), the first multilingual human-annotated\nbenchmark based on scientific talks that is designed to evaluate instruction-following in\ncrosslingual, multimodal settings over both short- and long-form inputs. MCIF spans three core\nmodalities--speech, vision, and text--and four diverse languages (English, German, Italian, and\nChinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret instructions across\nTanguages and combine them with multimodal contextual information. MCIF is released under a CC-BY 4.0\nlicense to encourage open research and progress in MLLMs development.\n\nCurrent browse context:\n\n¢s.cL",
    "authors": "Papi; Sara; Züfle; Maike; Gaido; Marco; Savoldi; Beatrice; Liu; Danni; Douros; Ioannis; Bentivogli; Luisa; Niehues; Jan",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19616v1",
    "title": "HITSZ's End-To-End Speech Translation Systems Combining Sequence-to-Sequence Auto Speech Recognition Model and Indic Large Language Model for IWSLT 2025 in Indic Track",
    "abstract": "This paper presents HITSZ's submission for the INSLT 2025 Indic\ntrack, focusing on speech-to-text translation (ST) for English-to-Indic and Indic-to-English language\npairs. To enhance translation quality in this low-resource scenario, we propose an end-to-end system\nintegrating the pre-trained Whisper automated speech recognition (ASR) model with Krutrim, an Indic-\nspecialized large language model (LLM). Experimental results demonstrate that our end-to-end system\nachieved average BLEU scores of $28.88 for English-to-Indic directions and $27.86$ for Indic-to-\nEnglish directions. Furthermore, we investigated the Chain-of-Thought (CoT) method. hile this method\nshowed potential for significant translation quality improvements on successfully parsed outputs (e.g.\n2 $13.84§ BLEU increase for Tamil-to-English), we observed challenges in ensuring the model\nconsistently adheres to the required CoT output format.",
    "authors": "Wei; Xuchen; Wu; Yangxin; Zhang; Yaoyin; Liu; Henglyu; Chen; Kehai; Bai; Xuefeng; Min",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19598v1",
    "title": "MOCHA: Are Code Language Models Robust Against Multi-Turn Malicious Coding Prompts?",
    "abstract": "Recent advancements in Large Language Models (LLMs) have\nsignificantly enhanced their code generation capabilities. However, their robustness against\nadversarial misuse, particularly through multi-turn malicious coding prompts, remains underexplored.\nIn this work, we introduce code decomposition attacks, where a malicious coding task is broken doun\ninto a series of seemingly benign subtasks across multiple conversational turns to evade safety\nfilters. To facilitate systematic evaluation, we introduce \\benchmarkname{}, a large-scale benchmark\ndesigned to evaluate the robustness of code LLMs against both single-turn and multi-turn malicious\npronpts. Empirical results across open- and closed-source models reveal persistent vulnerabilities,\nespecially under multi-turn scenarios. Fine-tuning on MOCHA improves rejection rates while preserving\ncoding ability, and importantly, enhances robustness on external adversarial datasets with up to 32.4%\nincrease in rejection rates without any additional supervision.\n\nCurrent browse context:\n\n¢s.cL",
    "authors": "Wahed; Muntasir; Zhou; Xiaona; Nguyen; Kiet A; Yu; Tianjiao; Diwan; Nirav; Wang; Gang; Hakkani-Tür; Dilek; Lourentzou; Ismini",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19595v1",
    "title": "Efficient Attention Mechanisms for Large Language Models: A Survey",
    "abstract": "Transformer-based architectures have become the prevailing\nbackbone of large language models. However, the quadratic time and memory complexity of self-attention\nremains a fundamental obstacle to efficient long-context modeling. To address this limitation, recent\nresearch has introduced two principal categories of efficient attention mechanisms. Linear attention\nmethods achieve linear complexity through kernel approximations, recurrent formulations, or fastweight\ndynamics, thereby enabling scalable inference with reduced computational overhead. Sparse attention\ntechniques, in contrast, limit attention computation to selected subsets of tokens based on fixed\npatterns, block-wise routing, or clustering strategies, enhancing efficiency while preserving\ncontextual coverage. This survey provides @ systematic and comprehensive overview of these\ndevelopments, integrating both algorithmic innovations and hardware-level considerations. In addition,\nwe analyze the incorporation of efficient attention into largescale pre-trained language models,\nincluding both architectures built entirely on efficient attention and hybrid designs that combine\nlocal and global components. By aligning theoretical foundations with practical deployment strategies,\n‘this work aims to serve as a foundational reference for advancing the design of scalable and efficient\nLanguage models.",
    "authors": "Sun; Yutao; Li; Zhenyu; Zhang; Yike; Pan; Tengyu; Dong; Bowen; Guo; Yuyi; Wang; Jianyong",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19586v1",
    "title": "Mitigating Geospatial Knowledge Hallucination in Large Language Models: Benchmarking and Dynamic Factuality Aligning",
    "abstract": "Large language models (LLMs) possess extensive world knowledge,\nincluding geospatial knowledge, which has been successfully applied to various geospatial tasks such\nas mobility prediction and social indicator prediction. However, LLMs often generate inaccurate\ngeospatial knowledge, leading to geospatial hallucinations (incorrect or inconsistent representations\nof geospatial information) that compromise their reliability. While the phenomenon of general\nknowledge hallucination in LLMs has been widely studied, the systematic evaluation and mitigation of\ngeospatial hallucinations remain largely unexplored. To address this gap, we propose a comprehensive\nevaluation framework for geospatial hallucinations, leveraging structured geospatial knowledge graphs\nfor controlled assessment. Through extensive evaluation across 20 advanced LLMs, we uncover the\nhallucinations in their geospatial knowledge. Building on these insights, we introduce a dynamic\nactuality aligning method based on Kahneman-Tversky Optimization (KTO) to mitigate geospatial\nhallucinations in LLMs, leading to a performance improvement of over 29.6% on the proposed benchmark.\nExtensive experimental results demonstrate the effectiveness of our benchmark and learning algorithm\nin enhancing the trustworthiness of LLMs in geospatial knowledge and reasoning tasks.\n\nCurrent browse context:\n\ncs.cL",
    "authors": "Wang; Shengyuan; Feng; Jie; Liu; Tianhui; Pei; Dan; Li; Yong",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19478v1",
    "title": "MMBench-GUI: Hierarchical Multi-Platform Evaluation Framework for GUI Agents",
    "abstract": "he introduce MlBench-GUI, a hierarchical benchmark for evaluating\nGUI automation agents across Windows, macOS, Linux, i0S, Android, and Web platforms. It comprises four\nlevels: GUI Content Understanding, Element Grounding, Task Automation, and Task Collaboration,\ncovering essential skills for GUI agents. In addition, we propose a novel Efficiency-Quality Area\n(EQA) metric to assess GUI agent execution efficiency in online automation scenarios. Through Bench-\nGUI, we identify accurate visual grounding as a critical determinant of overall task success,\n‘emphasizing the substantial benefits of modular frameworks that integrate specialized grounding\nmodules. Furthermore, to achieve reliable GUI automation, an agent requires strong task planning and\ncross-platform generalization abilities, with long-context memory, a broad action space, and long-term\nreasoning playing a critical role. More important, task efficiency remains a critically underexplored\ndimension, and all models suffer from substantial inefficiencies, with excessive redundant steps even\nwhen tasks are ultimately completed. The integration of precise localization, effective planning, and\nearly stopping strategies is indispensable to enable truly efficient and scalable GUI automation. Our\nbenchmark code, evaluation data, and running environment will be publicly available at this https URL.",
    "authors": "Wang; Xuehui; Wu; Zhenyu; Xie; JingJing; Ding; Zichen; Yang; Bowen; Li; Zehao; Zhaoyang; Qingyun; Dong; Xuan; Chen; Zhe; Weiyun; Zhao; Xiangyu; Jixuan; Duan; Haodong; Tianbao; Su; Shiqian; Yu; Huang; Yiqian; Zhang; Xiao; Yanting; Weijie; Zhu; Xizhou; Shen; Wei; Dai; Jifeng; Wenhai",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19477v1",
    "title": "Advancing Event Forecasting through Massive Training of Large Language Models: Challenges, Solutions, and Broader Impacts",
    "abstract": "Many recent papers have studied the development of\nsuperforecaster-level event forecasting LLMs. While methodological problems with early studies cast\ndoubt on the use of LLMs for event forecasting, recent studies with improved evaluation methods have\nshoun that state-of-the-art LLMs are gradually reaching superforecaster-level performance, and\nreinforcement learning has also been reported to improve future forecasting. Additionally, the\nunprecedented success of recent reasoning models and Deep Research-style models suggests that\n‘technology capable of greatly improving forecasting performance has been developed. Therefore, based\n‘on these positive recent trends, we argue that the time is ripe for research on large-scale training\nof superforecaster-level event forecasting LLMs. We discuss two key research directions: training\nmethods and data acquisition. For training, we first introduce three difficulties of LLI-based event\nforecasting training: noisiness-sparsity, knowledge cut-off, and simple reward structure problems.\nThen, we present related ideas to mitigate these problems: hypothetical event Bayesian networks,\nutilizing poorly-recalled and counterfactual events, and auxiliary reward signals. For data, we\npropose aggressive use of market, public, and crawling datasets to enable large-scale training and\nevaluation. Finally, we explain how these technical advances could enable AI to provide predictive\nintelligence to society in broader areas. This position paper presents promising specific paths and\nconsiderations for getting closer to superforecaster-level AI technology, aiming to call for\nresearchers’ interest in these directions.\n\nCurrent browse context:\n\n¢s.L6",
    "authors": "Lee; Sang-Woo; Yang; Sohee; Kwak; Donghyun; Siegel; Noah Y",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19470v1",
    "title": "Conversations Gone Awry, But Then? Evaluating Conversational Forecasting Models",
    "abstract": "We often rely on our intuition to anticipate the direction of a\nconversation. Endowing automated systems with similar foresight can enable them to assist human-human\ninteractions. Recent work on developing models with this predictive capacity has focused on the\nConversations Gone Awry (CGA) task: forecasting whether an ongoing conversation will derail. In this\nwork, we revisit this task and introduce the first uniform evaluation framework, creating a benchmark\n‘that enables direct and reliable comparisons between different architectures. This allows us to\npresent an up-to-date overview of the current progress in CGA models, in light of recent advancements\nin language modeling. Our framework also introduces a novel metric that captures a model's ability to\nrevise its forecast as the conversation progresses.",
    "authors": "Tran; Son Quoc; Gangavarapu; Tushaar; Chernogor; Nicholas; Chang; Jonathan P; Danescu-Niculescu-Mizil; Cristian",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19457v1",
    "title": "GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning",
    "abstract": "Large language models (LLMs) are increasingly adapted to downstream tasks via\nreinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), which often\nrequire thousands of rollouts to learn new tasks. We argue that the interpretable nature of language\ncan often provide a much richer learning medium for LLMs, compared with policy gradients derived from\nsparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), prompt optimizer that\n‘thoroughly incorporates natural language reflection to learn high-level rules from trial and error.\nGiven any AI system containing one or more LLM prompts, GEPA samples system-level trajectories (e.g.5\nreasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose\nproblems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier\nof its oun attempts. As 2 result of GEPA'S design, it can often turn even just a few rollouts into a\nlarge quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while\nusing up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over\n10% across two LLMs, and demonstrates promising results as an inference-time search strategy for code\noptimization.\n\nSubmission history\n\nFrom: Lakshya A Agrawal [view email][v1] Fri, 25 Jul 2025 1:\nCurrent browse context:\n\ncs.cL",
    "authors": "Agrawal; Lakshya A; Tan; Shangyin; Soylu; Dilara; Ziems; Noah; Khare; Rishi; Opsahl-Ong; Krista; Singhvi; Arnav; Shandilya; Herumb; Ryan; Michael J; Jiang; Meng; Potts; Christopher; Sen; Koushik; Dimakis; Alexandros G; Stoica; Ion; Klein; Dan; Zaharia; Matei; Khattab; Omar",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19419v1",
    "title": "TokenSmith: Streamlining Data Editing, Search, and Inspection for Large-Scale Language Model Training and Interpretability",
    "abstract": "Understanding the relationship between training data and model\nbehavior during pretraining is crucial, but existing workflows make this process cumbersome,\nfragmented, and often inaccessible to researchers. We present TokenSmith, an open-source library for\ninteractive editing, inspection, and analysis of datasets used in Megatron-style pretraining\nframeworks such as GPT-NeoX, Megatron, and NVIDIA Neto. TokenSmith supports a wide range of operations\nincluding searching, viewing, ingesting, exporting, inspecting, and sampling data, all accessible\n‘through a simple user interface and @ modular backend. It also enables structured editing of\npretraining data without requiring changes to training code, simplifying dataset debugging,\nvalidation, and experimentation.\n\nTokenSmith is designed as a plug and play addition to existing large language model pretraining\nworkflows, thereby democratizing access to production-grade dataset tooling. TokenSmith is hosted on\nGitHub1, with accompanying documentation and tutorials. A demonstration video is also available on\nYouTube.",
    "authors": "Khan; Mohammad Aflah; Godbole; Ameya; Wei; Johnny Tian-Zheng; Wang; Ryan; Flemings; James; Gummadi; Krishna; Neiswanger; Willie; Jia; Robin",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19407v1",
    "title": "Towards Domain Specification of Embedding Models in Medicine",
    "abstract": "Medical text embedding models are foundational to a wide array of\nhealthcare applications, ranging from clinical decision support and biomedical information retrieval\n‘to medical question answering, yet they remain hampered by two critical shortcomings. First, most\nmodels are trained on 2 narrow slice of medical and biological data, beside not being up to date in\n‘terms of methodology, making them ill suited to capture the diversity of terminology and semantics\n‘encountered in practice. Second, existing evaluations are often inadequate: even widely used\nbenchmarks fail to generalize across the full spectrum of real world medical tasks.\n\nTo address these gaps, we leverage MEDTE, a GTE model extensively fine-tuned on diverse medical\ncorpora through self-supervised contrastive learning across multiple data sources, to deliver robust\nmedical text embeddings.\n\nAlongside this model, we propose a comprehensive benchmark suite of 51 tasks spanning classification,\nclustering, pair classification, and retrieval modeled on the Massive Text Embedding Benchmark (MTEB)\nbut tailored to the nuances of medical text. Our results demonstrate that this combined approach not\nonly establishes a robust evaluation framework but also yields embeddings that consistently outperform\nstate of the art alternatives in different tasks.",
    "authors": "Khodadad; Mohammad; Shiraee; Ali; Astaraki; Mahdi; Mahyar; Hamidreza",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19396v2",
    "title": "Detection of Adverse Drug Events in Dutch clinical free text documents using Transformer Models: benchmark study",
    "abstract": "",
    "authors": "Murphy; Rachel M; Mishra; Nishant; De Keizer; Nicolette F; Dongelmans; Dave A; Jager; Kitty J; Abu-Hanna; Ameen; Klopotowska; Joanna E; Calixto; Iacer",
    "date": ""
  },
  {
    "url": "http://arxiv.org/abs/2507.19374v1",
    "title": "Data Augmentation for Spoken Grammatical Error Correction",
    "abstract": "hile there exist strong benchmark datasets for grammatical error\ncorrection (GEC), high-quality annotated spoken datasets for Spoken GEC (SGEC) are still under-\nresourced. In this paper, we propose a fully automated method to generate audio-text pairs with\ngrammatical errors and disfluencies. Moreover, we propose a series of objective metrics that can be\nused to evaluate the generated data and choose the more suitable dataset for SGEC. The goal is to\ngenerate an augmented dataset that maintains the textual and acoustic characteristics of the original\ndata while providing new types of errors. This augmented dataset should augment and enrich the\noriginal corpus without altering the language assessment scores of the second language (12) learners.\nWe evaluate the use of the augmented corpus both for written GEC (the text part) and for SGEC (the\naudio-text pairs). Our experiments are conducted on the S\\&I Corpus, the first publicly available\nspeech dataset with grammar error annotations.",
    "authors": "Karanasou; Penny; Qian; Mengjie; Bannò; Stefano; Gales; Mark J F; Knill; Kate M",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19362v1",
    "title": "LOTUS: A Leaderboard for Detailed Image Captioning from Quality to Societal Bias and User Preferences",
    "abstract": "Large Vision-Language Models (LVLMs) have transformed image\ncaptioning, shifting from concise captions to detailed descriptions. We introduce LOTUS, a leaderboard\nfor evaluating detailed captions, addressing three main gaps in existing evaluations: lack of\nstandardized criteria, bias-auare assessments, and user preference considerations. LOTUS\ncomprehensively evaluates various aspects, including caption quality (e.g., alignment,\ndescriptiveness), risks (\\eg, hallucination), and societal biases (e.g., gender bias) while enabling\npreference-oriented evaluations by tailoring criteria to diverse user preferences. Our analysis of\nrecent LVLMs reveals no single model excels across all criteria, while correlations emerge between\ncaption detail and bias risks. Preference-oriented evaluations demonstrate that optimal model\nselection depends on user priorities.\n\nCurrent browse context:\n\ncs.V",
    "authors": "Hirota; Yusuke; Li; Boyi; Hachiuma; Ryo; Wu; Yueh-Hua; Ivanovic; Boris; Nakashima; Yuta; Pavone; Marco; Choi; Yejin; Wang; Yu-Chiang Frank; Yang; Chao-Han Huck",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19361v1",
    "title": "SpeechIQ: Speech Intelligence Quotient Across Cognitive Levels in Voice Understanding Large Language Models",
    "abstract": "be introduce Speech-based Intelligence Quotient (SIQ) as a new\nform of human cognition-inspired evaluation pipeline for voice understanding large language models,\nLLM Voice, designed to assess their voice understanding ability. Moving beyond popular voice\nunderstanding metrics such as word error rate (WER), SIQ examines LLM Voice across three cognitive\nlevels motivated by Bloom's Taxonomy: (1) Remembering (i.e., WER for verbatim accuracy); (2)\nUnderstanding (i.e., similarity of LLM's interpretations); and (3) Application (i.e., QA accuracy for\nsimulating downstream tasks). We demonstrate that SIQ not only quantifies voice understanding\nabilities but also provides unified comparisons between cascaded methods (e.g., ASR LLM) and end-to-\nend models, identifies annotation errors in existing benchmarks, and detects hallucinations in LLM\nVoice. Our framework represents a first-of-its-kind intelligence examination that bridges cognitive\nprinciples with voice-oriented benchmarks, while exposing overlooked challenges in multi-modal\n‘training.\n\nCurrent browse context:\n\ncs.cL",
    "authors": "Wan; Zhen; Yang; Chao-Han Huck; Yu; Yahan; Tian; Jinchuan; Li; Sheng; Hu; Ke; Chen; Zhehuai; Watanabe; Shinji; Fei; Chu; Kurohashi; Sadao",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19356v1",
    "title": "Enhancing Speech Emotion Recognition Leveraging Aligning Timestamps of ASR Transcripts and Speaker Diarization",
    "abstract": "In this paper, we investigate the impact of incorporating\n‘timestamp-based alignment between Automatic Speech Recognition (ASR) transcripts and Speaker\nDiarization (SD) outputs on Speech Emotion Recognition (SER) accuracy. Misalignment between these two\nmodalities often reduces the reliability of multimodal emotion recognition systems, particularly in\nconversational contexts. To address this issue, we introduce an alignment pipeline utilizing pre-\n‘trained ASR and speaker diarization models, systematically synchronizing timestamps to generate\naccurately labeled speaker segments. Our multimodal approach combines textual embeddings extracted via\nROBERTa with audio embeddings from Wav2Vec, leveraging cross-attention fusion enhanced by a gating\nmechanism. Experimental evaluations on the IEMOCAP benchmark dataset demonstrate that precise\n‘timestamp alignment improves SER accuracy, outperforming baseline methods that lack synchronization.\nThe results highlight the critical importance of temporal alignment, demonstrating its effectiveness\nin enhancing overall emotion recognition accuracy and providing a foundation for robust multimodal\n‘emotion analysis.\n\nCurrent browse context:\n\ncs.cL",
    "authors": "Wang; Hsuan-Yu; Lee; Pei-Ying; Chen; Berlin",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19353v1",
    "title": "Smooth Reading: Bridging the Gap of Recurrent LLM to Self-Attention LLM on Long-Context Tasks",
    "abstract": "Recently, recurrent large language models (Recurrent LLMs) with\nLinear computational complexity have re-emerged as efficient alternatives to self-attention-based LLMs\n(Self-attention LLMs), which have quadratic complexity. However, Recurrent LLMs often underperform on\nlong-context tasks due to their limited fixed-size memory. Previous research has primarily focused on\nenhancing the memory capacity of Recurrent LLMs through architectural innovations, but these\napproaches have not yet enabled Recurrent LLMs to match the performance of Self-Attention LLMs on\nlong-context tasks. We argue that this limitation arises because processing the entire context at once\nis not well-suited for Recurrent LLMs. In this paper, we propose Smooth Reading, a chunk-wise\ninference method inspired by human reading strategies. Smooth Reading processes context in chunks and\niteratively summarizes the contextual information, thereby reducing memory demands and making the\napproach more compatible with Recurrent LLMs. Our experimental results show that this method\nsubstantially narrows the performance gap between Recurrent and Self-Attention LLMs on long-context\n‘tasks, while preserving the efficiency advantages of Recurrent LLMs. Our Smooth Reading boosts SWA-38-\n4k (@ Recurrent LLM) from 5.68% lower to 3.61% higher performance than Self-Attention LLMs on\nLongBench. Besides, our method maintains the high efficiency, training 3x faster and inferring 2x\nfaster at 64k context compared to Self-Attention LLMs. To our knowledge, this is the first work to\nachieve comparable performance using Recurrent LLMs compared with Self-Attention LLMs on long-context\n‘tasks. We hope our method will inspire future research in this area. To facilitate further progress,\nwe will release code and dataset.",
    "authors": "Liu; Kai; Su; Zhan; Dong; Peijie; Mo; Fengran; Gao; Jianfei; ShaoTing; Chen",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19333v1",
    "title": "Injecting External Knowledge into the Reasoning Process Enhances Retrieval-Augmented Generation",
    "abstract": "Retrieval-augmented generation (RAG) has been widely adopted to\n‘augment large language models (LLMs) with external knowledge for knowledge-intensive tasks. However,\nits effectiveness is often undermined by the presence of noisy (i.e., low-quality) retrieved passages.\nEnhancing LLMs’ robustness to such noise is critical for improving the reliability of RAG systems.\nRecent advances have equipped LLMs with strong reasoning and self-reflection capabilities, allowing\n‘them to identify and correct errors in their reasoning process. Inspired by this ability, we propose\nPassage Injection-a simple yet effective method that explicitly incorporates retrieved passages into\nLLMs\" reasoning process, aiming to enhance the model's ability to recognize and resist noisy passages.\nWe validate Passage Injection under general RAG settings using 6/25 as the retriever. Experiments on\nfour reasoning-enhanced LLMs across four factual QA datasets demonstrate that Passage Injection\nsignificantly improves overall RAG performance. Further analysis on two noisy retrieval settings-\nrandom noise, where the model is provided irrelevant passages, and counterfactual noise, where it is\ngiven misleading passages-shows that Passage Injection consistently improves robustness. Controlled\nexperiments confirm that Passage Injection can also effectively leverage helpful passages. These\nfindings suggest that incorporating passages in LLMs’ reasoning process is a promising direction for\nbuilding more robust RAG systems. The code can be found \\href{here}{this https URL}.",
    "authors": "Tang; Minghao; Ni; Shiyu; Guo; Jiafeng; Bi; Keping",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19315v1",
    "title": "AutoPCR: Automated Phenotype Concept Recognition by Prompting",
    "abstract": "Phenotype concept recognition (CR) is a fundamental task in\nbiomedical text mining, enabling applications such as clinical diagnostics and knowledge graph\nconstruction. However, existing methods often require ontology-specific training and struggle to\ngeneralize across diverse text types and evolving biomedical terminology. We present AutoPCR, a\nPronpt-based phenotype CR method that does not require ontology-specific training. AutoPCR performs CR\nin three stages: entity extraction using a hybrid of rule-based and neural tagging strategies,\ncandidate retrieval via SapBERT, and entity linking through prompting a large language model.\nExperiments on four benchmark datasets show that AUtoPCR achieves the best average and most robust\nperformance across both mention-level and document-level evaluations, surpassing prior state-of-the-\nart methods. Further ablation and transfer studies demonstrate its inductive capability and\ngeneralizability to new ontologies.",
    "authors": "Tao; Yicheng; Huang; Yuanhao; Liu; Jie",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19308v1",
    "title": "The Eloquence team submission for task 1 of MLC-SLM challenge",
    "abstract": "In this paper, we present our studies and experiments carried out\nfor the task 1 of the Challenge and Workshop on Multilingual Conversational Speech Language Model\n(HLC-SLM), which focuses on advancing multilingual conversational speech recognition through the\ndevelopment of speech language models architectures. Given the increasing relevance of real-world\nconversational data for building robust Spoken Dialogue Systems, we explore three approaches to\nmultilingual ASR. First, we conduct an evaluation of the official baseline to better understand its\nstrengths and limitations, by training two projectors (linear and qformer) with different foundation\nmodels. Second we leverage the SLAM-ASR framework to train a custom multilingual linear projector.\nFinally we investigate the role of contrastive learning and the extended conversational context in\nenhancing the robustness of recognition.\n\nCurrent browse context:\n\n¢s.8D",
    "authors": "Concina; Lorenzo; Luque; Jordi; Brutti; Alessio; Matassoni; Marco; Zhang; Yuchen",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19303v1",
    "title": "Identifying Fine-grained Forms of Populism in Political Discourse: A Case Study on Donald Trump's Presidential Campaigns",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable\ncapabilities across a wide range of instruction-following tasks, yet their grasp of nuanced social\nscience concepts remains underexplored. This paper examines whether LLMs can identify and classify\nfine-grained forms of populism, a complex and contested concept in both academic and media debates. To\nthis end, we curate and release novel datasets specifically designed to capture populist discourse. We\nevaluate a range of pre-trained (large) language models, both open-weight and proprietary, across\nmultiple prompting paradigms. Our analysis reveals notable variation in performance, highlighting the\nLimitations of LLMs in detecting populist discourse. We find that a fine-tuned ROBERTa classifier\nvastly outperforms all new-era instruction-tuned LLMs, unless fine-tuned. Additionally, we apply our\nbest-performing model to analyze campaign speeches by Donald Trump, extracting valuable insights into\nhis strategic use of populist rhetoric. Finally, we assess the generalizability of these models by\nbenchmarking them on campaign speeches by European politicians, offering a lens into cross-context\ntransferability in political discourse analysis. In this setting, we find that instruction-tuned LLMs\nexhibit greater robustness on out-of-domain data.",
    "authors": "Chalkidis; Ilias; Brandl; Stephanie; Aslanidis; Paris",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19247v1",
    "title": "A Markov Categorical Framework for Language Modeling",
    "abstract": "Auto-regressive language models factorize sequence probabilities and are trained by\nminimizing the negative log-likelihood (NLL) objective. While empirically powerful, 2 deep theoretical\nunderstanding of why this simple objective yields such versatile representations remains elusive. This\nwork introduces a unifying analytical framework using Markov Categories (MCs) to deconstruct the AR\ngeneration process and the NLL objective. We model the single-step generation map as 2 composition of\nMarkov kernels in the category Stoch. This compositional view, when enriched with statistical\ndivergences, allows us to dissect information flow and learned geometry. Our framework makes three\nmain contributions. First, we provide a formal, information-theoretic rationale for the success of\nmodern speculative decoding methods like EAGLE, quantifying the information surplus in hidden states\n‘that these methods exploit. Second, we formalize how NLL minimization forces the model to learn not\njust the next token, but the data's intrinsic conditional stochasticity, a process we analyze using\ncategorical entropy. Third, and most centrally, we prove that NLL training acts as an implicit form of\nspectral contrastive learning. By analyzing the information geometry of the model's prediction head,\nwe show that NLL implicitly forces the learned representation space to align with the eigenspectrum of\na predictive similarity operator, thereby learning a geometrically structured space without explicit\ncontrastive pairs. This compositional and information-geometric perspective reveals the deep\nstructural principles underlying the effectiveness of modern LMs. Project Page: this https URL\nCurrent browse context:\n\n¢s.L6",
    "authors": "Zhang; Yifan",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19227v1",
    "title": "Jailbreaking Large Language Diffusion Models: Revealing Hidden Safety Flaws in Diffusion-Based Text Generation",
    "abstract": "Large Language Diffusion Models (LLDMs) exhibit comparable\nperformance to LLMs while offering distinct advantages in inference speed and mathematical reasoning\n‘this http URL precise and rapid generation capabilities of LLDMs amplify concerns of harmful\ngenerations, while existing jailbreak methodologies designed for Large Language Models (LLMs) prove\nLimited effectiveness against LLOMs and fail to expose safety this http URL defense cannot\ndefinitively resolve harmful generation concerns, as it remains unclear whether LLDMs possess safety\nrobustness or existing attacks are incompatible with diffusion-based this http URL address this, we\nfirst reveal the vulnerability of LLDMs to jailbreak and demonstrate that attack failure in LLDMs\nstems from fundamental architectural this http URL present a PArallel Decoding jailbreak (PAD) for\ndiffusion-based language models. PAD introduces Multi-Point Attention Attack, which guides parallel\ngenerative processes toward harmful outputs that inspired by affirmative response patterns in LLMs.\nExperimental evaluations across four LLDMs demonstrate that PAD achieves jailbreak attack success\nrates by 97%, revealing significant safety vulnerabilities. Furthermore, compared to autoregressive\nLLMs of the same size, LLDMs increase the harmful generation speed by 2x, significantly highlighting\nrisks of uncontrolled this http URL comprehensive analysis, we provide an investigation into LLDM\narchitecture, offering critical insights for the secure deployment of diffusion-based language models.",
    "authors": "Zhang; Yuanhe; Xie; Fangzhou; Zhou; Zhenhong; Li; Zherui; Chen; Hao; Wang; Kun; Guo; Yufei",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19219v1",
    "title": "How Much Do Large Language Model Cheat on Evaluation? Benchmarking Overestimation under the One-Time-Pad-Based Framework",
    "abstract": "",
    "authors": "Liang; Zi; Yu; Liantong; Zhang; Shiyu; Ye; Qingqing; Hu; Haibo",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19204v2",
    "title": "Should Top-Down Clustering Affect Boundaries in Unsupervised Word Discovery?",
    "abstract": "We investigate the problem of segmenting unlabeled speech into\nword-like units and clustering these to create a lexicon. Prior work can be categorized into two\nframeworks. Bottom-up methods first determine boundaries and then cluster the fixed segmented words\ninto a lexicon. In contrast, top-down methods incorporate information from the clustered words to\ninform boundary selection. However, it is unclear whether top-down information is necessary to improve\nsegmentation. To explore this, we look at two similar approaches that differ in whether top-down\nclustering informs boundary selection. Our simple bottom-up strategy predicts word boundaries using\n‘the dissimilarity between adjacent self-supervised features, then clusters the resulting segments to\nconstruct @ lexicon. Our top-down system is an updated version of the ES-KMeans dynamic programming\nmethod that iteratively uses K-means to update its boundaries. On the five-language ZeroSpeech\nbenchmarks, both approaches achieve comparable state-of-the-art results, with the bottom-up system\nbeing nearly five times faster. Through detailed analyses, we show that the top-down influence of ES-\nkMeans can be beneficial (depending on factors like the candidate boundaries), but in many cases the\nsimple bottom-up method performs just as well. For both methods, we show that the clustering step is a\nLimiting factor. Therefore, we recommend that future work focus on improved clustering techniques and\nlearning more discriminative word-like representations. Project code repository: this https URL.\nSubmission history\n\nFrom: Simon Malan [view email][v1] Fri, 25 Jul 2025 12:19:16 UTC (552 KB)\n\n[v2] Mon, 28 Jul 2625 14:43:23 UTC (552 KB)\n\nCurrent browse context:\n\neess.AS.",
    "authors": "Malan; Simon; Van Niekerk; Benjamin; Kamper; Herman",
    "date": ""
  },
  {
    "url": "http://arxiv.org/abs/2507.19196v1",
    "title": "Towards Multimodal Social Conversations with Robots: Using Vision-Language Models",
    "abstract": "Large language models have given social robots the ability to\nautonomously engage in open-domain conversations. However, they are still missing fundamental social\nskill: making use of the multiple modalities that carry social interactions. While previous work has\nfocused on task-oriented interactions that require referencing the environment or specific phenomena\nin social interactions such as dialogue breakdowns, we outline the overall needs of a multimodal\nsystem for social conversations with robots. We then argue that vision-language models are able to\nprocess this wide range of visual information in a sufficiently general manner for autonomous social\nrobots. We describe how to adapt them to this setting, which technical challenges remain, and briefly\ndiscuss evaluation practices.\n\nCurrent browse context:\n\n€s.RO",
    "authors": "Janssens; Ruben; Belpaeme; Tony",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19195v1",
    "title": "Can Small-Scale Data Poisoning Exacerbate Dialect-Linked Biases in Large Language Models?",
    "abstract": "Despite the ongoing improvements in the design of large language\nmodels (LLMs) to foster inclusion and balanced responses, these systems remain susceptible to encoding\nand amplifying social biases. This study examines how dialectal variation, specifically African\nAmerican Vernacular English (AAVE) versus Standard American English (SAE), interacts with data\npoisoning to influence toxicity in outputs. Using both small- and medium-scale LLaMA models, we show\n‘that even minimal exposure to poisoned data significantly increases toxicity for AAVE inputs, while it\nremains comparatively unaffected for SAE. Larger models exhibit a more significant amplification\neffect which suggests heightened susceptibility with scale. To further assess these disparities, we\nemployed GPT-40 as a fairness auditor, which identified harmful stereotypical patterns\ndisproportionately tied to AAVE inputs, including portrayals of aggression, criminality, and\nintellectual inferiority. These findings underscore the compounding impact of data poisoning and\ndialectal bias and emphasize the need for dialect-aware evaluation, targeted debiasing interventions,\nand socially responsible training protocols during development.\n\nCurrent browse context:\n\ncs.cL",
    "authors": "Abbas; Chaymaa; Awad; Mariette; Tajeddine; Razane",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19156v1",
    "title": "An Empirical Investigation of Gender Stereotype Representation in Large Language Models: The Italian Case",
    "abstract": "The increasing use of Large Language Models (LLMs) in a large\nvariety of domains has sparked worries about how easily they can perpetuate stereotypes and contribute\n‘to the generation of biased content. With a focus on gender and professional bias, this work examines\nin which manner LLMs shape responses to ungendered prompts, contributing to biased outputs. This\nanalysis uses a structured experimental method, giving different prompts involving three different\nprofessional job combinations, which are also characterized by a hierarchical relationship. This study\nuses Italian, a language with extensive grammatical gender differences, to highlight potential\nLimitations in current LLMs’ ability to generate objective text in non-English languages. Two popular\nLLM-based chatbots are examined, namely OpenAI ChatGPT (gpt-40-mini) and Google Gemini (gemini-1.5-\nflash). Through APIs, we collected a range of 3600 responses. The results highlight how content\ngenerated by LLMs can perpetuate stereotypes. For example, Gemini associated 100% (ChatGPT 97%) of\n“she’ pronouns to the ‘assistant’ rather than the ‘manager’. The presence of bias in Al-generated text\ncan have significant implications in many fields, such as in the workplaces or in job selections,\nraising ethical concerns about its use. Understanding these risks is pivotal to developing mitigation\nstrategies and assuring that Al-based systems do not increase social inequalities, but rather\ncontribute to more equitable outcomes. Future research directions include expanding the study to\nadditional chatbots or languages, refining prompt engineering methods or further exploiting a larger\n‘experimental base.\n\nCurrent browse context:\n\ncs.cL",
    "authors": "Giachino; Gioele; Rondina; Marco; Vetrò; Antonio; Coppola; Riccardo; De Martin; Juan Carlos",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19132v1",
    "title": "OS-MAP: How Far Can Computer-Using Agents Go in Breadth and Depth?",
    "abstract": "Computer-using agents have shown strong potential to boost human\nproductivity and enable new application forms across platforms. While recent advances have led to\nusable applications, existing benchmarks fail to account for the internal task heterogeneity and the\ncorresponding agent capabilities, as well as their alignment with actual user demands-hindering both\ntargeted capability development and the reliable transition of research progress into practical\ndeployment. To bridge the gap, we present OS-MAP, a benchmark for daily computer-using automation that\norganizes its 416 realistic tasks across 15 applications along two key dimensions: a five-level\n‘taxonomy of automation and a generalization scope derived from a real-world user demand hierarchy. To\nenable fine-grained analysis of required capabilities and alignment with real-world scenarios, OS-HAP\nevaluates agents along tuo dimensions: automation level across a five-level taxonomy, and\ngeneralization scope across a demand hierarchy. This design captures varying levels of required agent\n‘autonomy and generalization, forming @ performance-generalization evaluation matrix for structured and\ncomprehensive assessment. Experiments show that even State-of-the-Art agents with VLM backbones\nstruggle with higher-level tasks involving perception, reasoning, and coordination-highlighting the\nneed for a deeper understanding of current strengths and limitations to drive the future progress in\ncomputer-using agents research and deployment. All code, environments, baselines, and data are\npublicly available at this https URL.\n\nCurrent browse context:\n\ncs.AL",
    "authors": "Chen; Xuetian; Yinghao; Yuan; Xinfeng; Peng; Zhuo; Lu; Li; Yuekeng; Zhang; Zhoujia; Huang; Yingqian; Leyan; Jiaqing; Xie; Tianbao; Wu; Zhiyong; Sun; Qiushi; Qi; Biqing; Zhou; Bowen",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19117v1",
    "title": "Objectifying the Subjective: Cognitive Biases in Topic Interpretations",
    "abstract": "Interpretation of topics is crucial for their downstream\napplications. State-of-the-art evaluation measures of topic quality such as coherence and word\nintrusion do not measure how much a topic facilitates the exploration of a corpus. To design\nevaluation measures grounded on a task, and a population of users, we do user studies to understand\nhow users interpret topics. We propose constructs of topic quality and ask users to assess them in the\ncontext of a topic and provide rationale behind evaluations. We use reflexive thematic analysis to\nidentify themes of topic interpretations from rationales. Users interpret topics based on availability\nand representativeness heuristics rather than probability. We propose a theory of topic interpretation\nbased on the anchoring-and-adjustment heuristic: users anchor on salient words and make semantic\nadjustments to arrive at an interpretation. Topic interpretation can be viewed as making a judgment\nunder uncertainty by an ecologically rational user, and hence cognitive biases aware user models and\nevaluation frameworks are needed.",
    "authors": "Hingmire; Swapnil; Li; Ze Shi; Shiyu; Zeng; Awon; Ahmed Musa; Guerra; Luiz Franciscatto; Ernst; Neil",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19102v1",
    "title": "Distilling a Small Utility-Based Passage Selector to Enhance Retrieval-Augmented Generation",
    "abstract": "Retrieval-augnented generation (RAG) enhances large language\nmodels (LLMs) by incorporating retrieved information. Standard retrieval process prioritized\nrelevance, focusing on topical alignnent betueen queries and passages. In contrast, in RAG, the\nemphasis has shifted to utility, which considers the usefulness of passages for generating accurate\nanswers. Despite empirical evidence showing the benefits of utility-based retrieval in RAG, the high\ncomputational cost of using LLMs for utility judgments limits the number of passages evaluated. This\nrestriction is problematic for complex queries requiring extensive information. To address this, we\npropose a method to distill the utility judgment capabilities of LLMs into smaller, more efficient\nmodels. Our approach focuses on utility-based selection rather than ranking, enabling dynamic passage\nselection tailored to specific queries without the need for fixed thresholds. We train student models\nto learn pseudo-answer generation and utility judgments from teacher LLMs, using a sliding window\nmethod that dynamically selects useful passages. Our experiments demonstrate that utility-based\nselection provides a flexible and cost-effective solution for RAG, significantly reducing\nComputational costs while improving answer quality. We present the distillation results using Quen3-\n328 as the teacher model for both relevance ranking and utility-based selection, distilled into\nRankQuen1.78 and UtilityQuen1.78. Our findings indicate that for complex questions, utility-based\nselection is more effective than relevance ranking in enhancing answer generation performance. We will\nrelease the relevance ranking and utility-based selection annotations for the HS MARCO dataset,\nsupporting further research in this area.\n\nCurrent browse context:\n\ncs.IR",
    "authors": "Zhang; Hengran; Bi; Keping; Guo; Jiafeng; Jiaming; Wang; Shuaiqiang; Yin; Dawei; Cheng; Xueqi",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19090v1",
    "title": "Debating Truth: Debate-driven Claim Verification with Multiple Large Language Model Agents",
    "abstract": "Claim verification is critical for enhancing digital literacy. However, the state-of-\n‘the-art single-LLM methods struggle with complex claim verification that involves multi-faceted\nevidences. Inspired by real-world fact-checking practices, we propose DebateCV, the first claim\nverification framework that adopts  debate-driven methodology using multiple LLM agents. In our\nframework, two Debaters take opposing stances on a claim and engage in multi-round argumentation,\nwhile a Moderator evaluates the arguments and renders a verdict with justifications. To further\nimprove the performance of the Moderator, we introduce a novel post-training strategy that leverages\nsynthetic debate data generated by the zero-shot DebateCV, effectively addressing the scarcity of\nreal-world debate-driven claim verification data. Experimental results show that our method\noutperforms existing claim verification methods under varying levels of evidence quality. Our code and\ndataset are publicly available at this https URL.",
    "authors": "He; Haorui; Li; Yupeng; Wen; Dacheng; Cheng; Reynold; Lau; Francis C M",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19081v1",
    "title": "Arg-LLaDA: Argument Summarization via Large Language Diffusion Models and Sufficiency-Aware Refinement",
    "abstract": "Argument summarization aims to generate concise, structured\nrepresentations of complex, multi-perspective debates. While recent work has advanced the\nidentification and clustering of argumentative components, the generation stage remains underexplored.\nExisting approaches typically rely on single-pass generation, offering limited support for factual\ncorrection or structural refinement. To address this gap, we introduce Arg-LLaDA, a novel large\nLanguage diffusion framework that iteratively improves summaries via sufficiency-guided remasking and\nregeneration. Our method combines a flexible masking controller with a sufficiency-checking module to\nidentify and revise unsupported, redundant, or incomplete spans, yielding more faithful, concise, and\ncoherent outputs. Empirical results on two benchmark datasets demonstrate that Arg-LLaDA surpasses\nstate-of-the-art baselines in 7 out of 10 automatic evaluation metrics. In addition, human evaluations\nreveal substantial improvements across core dimensions, coverage, faithfulness, and conciseness,\nvalidating the effectiveness of our iterative, sufficiency-aware generation strategy.",
    "authors": "Li; Hao; Sun; Yizheng; Schlegel; Viktor; Yang; Kailai; Batista-Navarro; Riza; Nenadic; Goran",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19060v1",
    "title": "PurpCode: Reasoning for Safer Code Generation",
    "abstract": "We introduce PurpCode, the first post-training recipe for\n‘training safe code reasoning models towards generating secure code and defending against malicious\ncyberactivities. PurpCode trains a reasoning model in two stages: (i) Rule Learning, which explicitly\nteaches the model to reference cybersafety rules to generate vulnerability-free code and to avoid\nfacilitating malicious cyberactivities; and (ii) Reinforcement Learning, which optimizes model safety\nand preserves model utility through diverse, multi-objective reward mechanisms. To empower the\n‘training pipelines with comprehensive cybersafety data, we conduct internal red-teaming to synthesize\ncomprehensive and high-coverage prompts based on real-world tasks for inducing unsafe cyberactivities\nin the model. Based on PurpCode, we develop a reasoning-based coding model, namely PurpCode-328, which\ndemonstrates state-of-the-art cybersafety, outperforming various frontier models. Meanwhile, our\nalignment method decreases the model overrefusal rates in both general and cybersafety-specific\nscenarios, while preserving model utility in both code generation and common security knowledge.\nCurrent browse context:\n\ncs.CR",
    "authors": "Liu; Jiawei; Diwan; Nirav; Wang; Zhe; Zhai; Haoyu; Zhou; Xiaona; Nguyen; Kiet A; Yu; Tianjiao; Wahed; Muntasir; Deng; Yinlin; Benkraouda; Hadjer; Wei; Zhang; Lingming; Lourentzou; Ismini; Gang",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19054v1",
    "title": "Closing the Modality Gap for Mixed Modality Search",
    "abstract": "tixed modality search -- retrieving information across a\nheterogeneous corpus composed of images, texts, and multimodal documents -- is an important yet\nunderexplored real-world application. In this work, we investigate how contrastive vision-lenguage\nmodels, such as CLIP, perform on the mixed modality search task. Our analysis reveals a critical\nLimitation: these models exhibit a pronounced modality gap in the embedding space, where image and\n‘text embeddings form distinct clusters, leading to intra-modal ranking bias and inter-modal fusion\nfailure. To address this issue, we propose GR-CLIP, a lightweight post-hoc calibration method that\nremoves the modality gap in CLIP's embedding space. Evaluated on MixBench -- the first benchmark\nspecifically designed for mixed modality search -- GR-CLIP improves NDCG@10 by up to 26 percentage\npoints over CLIP, surpasses recent vision-language generative embedding models by 4 percentage points,\nwhile using 75x less compute.\n\nCurrent browse context:\n\ncs.V",
    "authors": "Li; Binxu; Zhang; Yuhui; Wang; Xiaohan; Weixin; Schmidt; Ludwig; Yeung-Levy; Serena",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19040v1",
    "title": "FD-Bench: A Full-Duplex Benchmarking Pipeline Designed for Full Duplex Spoken Dialogue Systems",
    "abstract": "Full-duplex spoken dialogue systems (FDSDS) enable more natural\nhuman-machine interactions by allowing real-time user interruptions and backchanneling, compared to\n‘traditional SDs that rely on turn-taking. However, existing benchmarks lack metrics for FD scenes,\n2.g., evaluating model performance during user interruptions. In this paper, we present a\ncomprehensive FD benchmarking pipeline utilizing LLMs, TTS, and ASR to address this gap. It assesses\nFDSDS's ability to handle user interruptions, manage delays, and maintain robustness in challenging\nscenarios with diverse novel metrics. We applied our benchmark to three open-source FDSDS (Moshi,\nFreeze-omni, and VITA-1.5) using over 40 hours of generated speech, with 293 simulated conversations\nand 1,200 interruptions. The results show that all models continue to face challenges, such as failing\n‘to respond to user interruptions, under frequent disruptions and noisy conditions. Demonstrations,\ndata, and code will be released.\n\nCurrent browse context:\n\neess.AS.",
    "authors": "Peng; Yizhou; Chao; Yi-Wen; Ng; Dianwen; Ma; Yukun; Ni; Chongjia; Bin; Chng; Eng Siong",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.19037v1",
    "title": "MLLM-based Speech Recognition: When and How is Multimodality Beneficial?",
    "abstract": "Recent advances in multi-modal large language models (MLLMs) have\n‘opened new possibilities for unified modeling of speech, text, images, and other modalities. Building\n‘on our prior work, this paper examines the conditions and model architectures under which multiple\ninput modalities Can improve automatic speech recognition (ASR) accuracy in noisy environment:\nThrough experiments on synthetic and real-world data, we find that (1) harnessing more modalities\nusually improves ASR accuracy, as each modality provides complementary information, but the\nimprovement depends on the amount of auditory noise. (2) Synchronized modalities (¢.g., lip movements)\n‘are more useful at high noise levels whereas unsynchronized modalities (e.g., image context) are most\nhelpful at moderate noise levels. (3) Higher-quality visual representations consistently improve ASR\naccuracy, highlighting the importance of developing more powerful visual encoders. (4) Mamba exhibits\nsimilar trends regarding the benefits of multimodality as do Transformers. (5) The input order of\nmodalities as well as their weights in the loss function can significantly impact accuracy. These\nfindings both offer practical insights and help to deepen our understanding of multi-modal speech\nrecognition under challenging conditions.\n\nCurrent browse context:\n\n¢s.8D",
    "authors": "Guan; Yiwen; Trinh; Viet Anh; Voleti; Vivek; Whitehill; Jacob",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18973v1",
    "title": "A Toolbox, Not a Hammer -- Multi-TAG: Scaling Math Reasoning with Multi-Tool Aggregation",
    "abstract": "",
    "authors": "Yao; Bohan; Yadav; Vikas",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18956v1",
    "title": "A Similarity Measure for Comparing Conversational Dynamics",
    "abstract": "The quality of a conversation goes beyond the individual quality\nof each reply, and instead emerges from how these combine into interactional patterns that give the\nconversation its distinctive overall \"shape\". However, there is no robust automated method for\ncomparing conversations in terms of their overall interactional dynamics. Such methods could enhance\n‘the analysis of conversational data and help evaluate conversational agents more holistically.\n\nIn this work, we introduce a similarity measure for comparing conversations with respect to their\ndynamics. We design a validation framework for testing the robustness of the metric in capturing\ndifferences in conversation dynamics and for assessing its sensitivity to the topic of the\nconversations. Finally, to illustrate the measure’s utility, we use it to analyze conversational\ndynamics in a large online community, bringing new insights into the role of situational power in\nconversations.",
    "authors": "Jung; Sang Min; Zhang; Kaixiang; Danescu-Niculescu-Mizil; Cristian",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18952v1",
    "title": "Legal Document Summarization: Enhancing Judicial Efficiency through Automation Detection",
    "abstract": "Legal document summarization represents a significant advancement\n‘towards improving judicial efficiency through the automation of key information detection. Our\napproach leverages state-of-the-art natural language processing techniques to meticulously identify\nand extract essential data from extensive legal texts, which facilitates a more efficient review\nprocess. By employing advanced machine learning algorithms, the framework recognizes underlying\npatterns within judicial documents to create precise summaries that encapsulate the crucial elements.\n‘This automation alleviates the burden on legal professionals, concurrently reducing the likelihood of\noverlooking vital information that could lead to errors. Through comprehensive experiments conducted\nwith actual legal datasets, we demonstrate the capability of our method to generate high-quality\nsummaries while preserving the integrity of the original content and enhancing processing times\nconsiderably. The results reveal marked improvements in operational efficiency, allowing legal\npractitioners to direct their efforts toward critical analytical and decision-making activities\ninstead of manual reviews. This research highlights promising technology-driven strategies that can\nsignificantly alter workflow dynamics within the legal sector, emphasizing the role of automation in\nrefining judicial processes.",
    "authors": "Li; Yongjie; Nong; Ruilin; Jianan; Evans; Lucas",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18949v1",
    "title": "Adaptive Learning Systems: Personalized Curriculum Design Using LLM-Powered Analytics",
    "abstract": "Large language models (LLMs) are revolutionizing the field of\n‘education by enabling personalized learning experiences tailored to individual student needs. In this\npaper, we introduce a framework for Adaptive Learning Systems that leverages LLM-powered analytics for\npersonalized curriculum design. This innovative approach uses advanced machine learning to analyze\nreal-time data, allowing the system to adapt learning pathways and recommend resources that align with\neach learner's progress. By continuously assessing students, our framework enhances instructional\nstrategies, ensuring that the materials presented are relevant and engaging. Experimental results\nindicate marked improvement in both learner engagement and knowledge retention when using @\ncustomized curriculum. Evaluations conducted across varied educational environments demonstrate the\nframework's flexibility and positive influence on learning outcomes, potentially reshaping\nconventional educational practices into a more adaptive and student-centered model.",
    "authors": "Li; Yongjie; Nong; Ruilin; Jianan; Evans; Lucas",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18945v1",
    "title": "TreeReader: A Hierarchical Academic Paper Reader Powered by Language Models",
    "abstract": "Efficiently navigating and understanding academic papers is crucial for scientific\nprogress. Traditional linear formats like PDF and HTML can cause cognitive overload and obscure a\npaper's hierarchical structure, making it difficult to locate key information. while LLM-based\nchatbots offer summarization, they often lack nuanced understanding of specific sections, may produce\nunreliable information, and typically discard the document's navigational structure. Drawing insights\nfrom a formative study on academic reading practices, we introduce TreeReader, a novel language model-\naugmented paper reader. TreeReader decomposes papers into an interactive tree structure where each\nsection is initially represented by an LLi-generated concise summary, with underlying details\naccessible on demand. This design allows users to quickly grasp core ideas, selectively explore\nsections of interest, and verify summaries against the source text. A user study was conducted to\nevaluate TreeReader's impact on reading efficiency and comprehension. TreeReader provides a more\nfocused and efficient way to navigate and understand complex academic literature by bridging\nhierarchical summarization with interactive exploration.\n\nCurrent browse context:\n\ncs-HE",
    "authors": "Zhang; Zijian; Chen; Pan; Du; Fangshi; Ye; Runlong; Huang; Oliver; Liut; Michael; Aspuru-Guzik; Alán",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18940v1",
    "title": "LLaVA-NeuMT: Selective Layer-Neuron Modulation for Efficient Multilingual Multimodal Translation",
    "abstract": "Multimodal Machine Translation (MIT) enhances translation quality\nby incorporating visual context, helping to resolve textual ambiguities. While existing !IT methods\nperform well in bilingual settings, extending them to multilingual translation remains challenging due\n‘to cross-lingual interference and ineffective parameter-sharing strategies. To address this, we\nPropose LLaVA-NeulfT, a novel multimodal multilingual translation framework that explicitly models\nLanguage-specific and language-agnostic representations to mitigate multilingual interference. Our\napproach consists of a layer selection mechanism that identifies the most informative layers for\ndifferent language pairs and a neuron-level adaptation strategy that dynamically selects language-\nspecific and agnostic neurons to improve translation quality while reducing redundancy. We conduct\nextensive experiments on the M3-Multi30K and M3-AmbigCaps datasets, demonstrating that LLaVA-NeulT,\nwhile fine-tuning only 40\\% of the model parameters, surpasses full fine-tuning approaches and\nultimately achieves SOTA results on both datasets. Our analysis further provides insights into the\nimportance of selected layers and neurons in multimodal multilingual adaptation, offering an efficient\nand scalable solution to cross-lingual adaptation in multimodal translation.",
    "authors": "Wei; Jingxuan; Jia; Caijun; Chen; Qi; Cai; Yujun; Sun; Linzhuang; Zhang; Xiangxiang; Wu; Gaowei; Yu; Bihui",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18932v1",
    "title": "Benchmarking Multimodal Understanding and Complex Reasoning for ESG Tasks",
    "abstract": "Environmental, Social, and Governance (ESG) reports are essential\nfor evaluating sustainability practices, ensuring regulatory compliance, and promoting financial\n‘transparency. However, these documents are often lengthy, structurally diverse, and multimodal,\ncomprising dense text, structured tables, complex figures, and layout-dependent semantics. Existing AT\nsystems often struggle to perform reliable document-level reasoning in such settings, and no dedicated\nbenchmark currently exists in ESG domain. To fill the gap, we introduce \\textbf{MMESGBench}, a first\nof-its-kind benchmark dataset targeted to evaluate multimodal understanding and complex reasoning\nacross structurally diverse and multi-source ESG documents. This dataset is constructed via @ human-AT\ncollaborative, multi-stage pipeline. First, 2 multimodal LLM generates candidate question-answer (QA)\npairs by jointly interpreting rich textual, tabular, and visual information from layout-aware document\npages. Second, an LLM verifies the semantic accuracy, completeness, and reasoning complexity of each\nQA pair. This automated process is followed by an expert-in-the-loop validation, where domain\nspecialists validate and calibrate QA pairs to ensure quality, relevance, and diversity. MMESGBench\ncomprises 933 validated QA pairs derived from 45 ESG documents, spanning across seven distinct\ndocument types and three major ESG source categories. Questions are categorized as single-page, cross-\npage, or unanswerable, with each accompanied by fine-grained multimodal evidence. Initial experiments\nvalidate that multimodal and retrieval-augmented models substantially outperform text-only baselines,\nparticularly on visually grounded and cross-page tasks. MMESGBench is publicly available as an open-\nsource dataset at this https URL.",
    "authors": "Zhang; Lei; Zhou; Xin; He; Chaoyue; Wang; Di; Wu; Yi; Xu; Hong; Liu; Wei; Miao; Chunyan",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18918v1",
    "title": "Uncovering Cross-Linguistic Disparities in LLMs using Sparse Autoencoders",
    "abstract": "Multilingual large language models (LLMs) exhibit strong cross-\nLinguistic generalization, yet medium to low resource languages underperform on common benchmarks such\nas ARC-Challenge, MILU, and HellaSwag. We analyze activation patterns in Genma-2-28 across all 26\nresidual layers and 10 languages: Chinese (zh), Russian (ru), Spanish (es), Italian (it), medium to\nlow resource languages including Indonesian (id), Catalan (ca), Marathi (mr), Malayalam (ml), and\nHindi (hi), with English (en) as the reference. Using Sparse Autoencoders (SAEs), we reveal systematic\ndisparities in activation patterns. Medium to low resource languages receive up to 26.27 percent lower\nactivations in early layers, with a persistent gap of 19.89 percent in deeper layers. To address this,\nwe apply activation-aware fine-tuning via Low-Rank Adaptation (LoRA), leading to substantial\nactivation gains, such as 87.69 percent for Malayalam and 86.32 percent for Hindi, while maintaining\nEnglish retention at approximately 91 percent. After fine-tuning, benchmark results show modest but\nconsistent improvements, highlighting activation alignment as a key factor in enhancing multilingual\nLLM performance.",
    "authors": "Xuan; Richmond Sin Jing; Huseynov; Jalil; Zhang; Yang",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18915v1",
    "title": "Mining Contextualized Visual Associations from Images for Creativity Understanding",
    "abstract": "Understanding another person's creative output requires a shared\nLanguage of association. However, when training vision-language models such as CLIP, we rely on web-\nscraped datasets containing short, predominantly literal, alt-text. In this work, we introduce a\nmethod for mining contextualized associations for salient visual elements in an image that can scale\nto any unlabeled dataset. Given an image, we can use these mined associations to generate high quality\ncreative captions at increasing degrees of abstraction. With our method, we produce a new dataset of\nvisual associations and 1.7m creative captions for the images in MSCOCO. Human evaluation confirms\n‘that these captions remain visually grounded while exhibiting recognizably increasing abstraction.\nMoreover, fine-tuning 2 visual encoder on this dataset yields meaningful improvements in zero-shot\nimage-text retrieval in two creative domains: poetry and metaphor visualization. We release our\ndataset, our generation code and our models for use by the broader community.",
    "authors": "Sahu; Ananya; Ananthram; Amith; McKeown; Kathleen",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18910v1",
    "title": "A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems: Progress, Gaps, and Future Directions",
    "abstract": "Retrieval-Augmented Generation (RAG) represents @ major\nadvancement in natural language processing (NLP), combining large language models (LLMs) with\ninformation retrieval systems to enhance factual grounding, accuracy, and contextual relevance. This\npaper presents a comprehensive systematic review of RAG, tracing its evolution from early developments\nin open domain question answering to recent state-of-the-art implementations across diverse\napplications. The review begins by outlining the motivations behind RAG, particularly its ability to\nmitigate hallucinations and outdated knowledge in parametric models. Core technical components-\nretrieval mechanisms, sequence-to-sequence generation models, and fusion strategies are examined in\ndetail. A year-by-year analysis highlights key milestones and research trends, providing insight into\nRAG's rapid growth. The paper further explores the deployment of RAG in enterprise systems, addressing\npractical challenges related to retrieval of proprietary data, security, and scalability. A\ncomparative evaluation of RAG implementations is conducted, benchmarking performance on retrieval\naccuracy, generation fluency, latency, and computational efficiency. Persistent challenges such as\nretrieval quality, privacy concerns, and integration overhead are critically assessed. Finally, the\nreview highlights emerging solutions, including hybrid retrieval approaches, privacy-preserving\ntechniques, optimized fusion strategies, and agentic RAG architectures. These innovations point toward\na future of more reliable, efficient, and context-aware knowledge-intensive NLP systems.",
    "authors": "Oche; Agada Joseph; Folashade; Ademola Glory; Ghosal; Tirthankar; Biswas; Arpan",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18905v1",
    "title": "Large language models provide unsafe answers to patient-posed medical questions",
    "abstract": "Millions of patients are already using large language model (LLM) chatbots for\nmedical advice on a regular basis, raising patient safety concerns. This physician-led red-teaming\nstudy compares the safety of four publicly available chatbots--Claude by Anthropic, Gemini by Google,\nGPT-40 by OpenAI, and Llama3-708 by Meta--on a new dataset, HealthAdvice, using an evaluation\nframework that enables quantitative and qualitative analysis. In total, 888 chatbot responses are\nevaluated for 222 patient-posed advice-seeking medical questions on primary care topics spanning\ninternal medicine, women's health, and pediatrics. We find statistically significant differences\nbetween chatbots. The rate of problematic responses varies from 21.6 percent (Claude) to 43.2 percent\n(Llama), with unsafe responses varying from 5 percent (Claude) to 13 percent (GPT-40, Llama).\nQualitative results reveal chatbot responses with the potential to lead to serious patient harm. This\nstudy suggests that millions of patients could be receiving unsafe medical advice from publicly\navailable chatbots, and further work is needed to improve the clinical safety of these powerful tools.",
    "authors": "Draelos; Rachel L; Afreen; Samina; Blasko; Barbara; Brazile; Tiffany; Chase; Natasha; Desai; Dimple; Evert; Jessica; Gardner; Heather L; Herrmann; Lauren; House; Aswathy Vaikom; Kass; Stephanie; Kavan; Marianne; Khemani; Kirshma; Koire; Amanda; McDonald; Rabeeah; Zahraa; Shah; Amy",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18902v1",
    "title": "SLoW: Select Low-frequency Words! Automatic Dictionary Selection for Translation on Large Language Models",
    "abstract": "There are more than 7,000 languages around the world, and current\nLarge Language Models (LLMs) only support hundreds of languages. Dictionary-based prompting methods\ncan enhance translation on them, but most methods use all the available dictionaries, which could be\nexpensive. Instead, it will be flexible to have a trade-off between token consumption and translation\nperformance. This paper proposes a novel task called \\textbf{A}utomatic \\textbf{D}ictionary\n\\textbf{S}election (\\textbf{ADS}). The goal of the task is to automatically select which dictionary to\nuse to enhance translation. We propose a novel and effective method which we call \\textbf{S}elect\n\\textbf{Lo}u-frequency \\textbf{i}ords! (\\textbf{SLoll}) which selects those dictionaries that have a\nlower frequency. Our methods have unique advantages. First, there is no need for access to the\n‘training data for frequency estimation (which is usually unavailable). Second, it inherits the\nadvantage of dictionary-based methods, where no additional tuning is required on LLMs. Experimental\nresults on 10@ languages from FLORES indicate that SLoW surpasses strong baselines, and it can\nobviously save token usage, with many languages even surpassing the translation performance of the\nfull dictionary baseline. \\footnote{A shocking fact is that there is no need to use the actual training\ndata (often unobtainable) for frequency estimation, and an estimation frequency obtained using public\nresources is still apparently effective in improving translation with ChatGPT and Llama, and\nDeepSeek.}\\footnote{Code and data available upon publication. }",
    "authors": "Lu; Hongyuan; Li; Zixuan; Zhang; Zefan; Lam; Wai",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18901v1",
    "title": "REPRO-Bench: Can Agentic AI Systems Assess the Reproducibility of Social Science Research?",
    "abstract": "Assessing the reproducibility of social science papers is\nessential for promoting rigor in research processes, but manual assessment is costly. With recent\nadvances in agentic AI systems (i.e., AI agents), we seek to evaluate their capability to automate\n‘this process. However, existing benchmarks for reproducing research papers (1) focus solely on\nreproducing results using provided code and data without assessing their consistency with the paper,\n(2) oversimplify real-world scenarios, and (3) lack necessary diversity in data formats and\nprogramming languages. To address these issues, we introduce REPRO-Bench, a collection of 112 task\ninstances, each representing 2 social science paper with a publicly available reproduction report. The\nagents are tasked with assessing the reproducibility of the paper based on the original paper PDF and\n‘the corresponding reproduction package. REPRO-Bench features end-to-end evaluation tasks on the\nreproducibility of social science papers with complexity comparable to real-world assessments. We\nevaluate three representative AI agents on REPRO-Bench, with the best-performing agent achieving an\naccuracy of only 21.4%. Building on our empirical analysis, we develop REPRO-Agent, which improves the\nhighest accuracy achieved by existing agents by 71%. We conclude that more advanced AI agents should\nbe developed to automate real-world reproducibility assessment. REPRO-Bench is publicly available at\n‘this https URL.",
    "authors": "Hu; Chuxuan; Zhang; Liyun; Lim; Yeji; Wadhwani; Aum; Peters; Austin; Kang; Daniel",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18890v1",
    "title": "NUTMEG: Separating Signal From Noise in Annotator Disagreement",
    "abstract": "NLP models often rely on human-labeled data for training and\nevaluation. Many approaches crowdsource this data from a large number of annotators with varying\nskills, backgrounds, and motivations, resulting in conflicting annotations. These conflicts have\n‘traditionally been resolved by aggregation methods that assume disagreements are errors. Recent work\nhas argued that for many tasks annotators may have genuine disagreements and that variation should be\n‘treated as signal rather than noise. However, few models separate signal and noise in annotator\ndisagreement. In this work, we introduce NUTMEG, a new Bayesian model that incorporates information\nabout annotator backgrounds to remove noisy annotations from human-labeled training data while\npreserving systematic disagreements. Using synthetic data, we show that NUTHEG is more effective at\nrecovering ground-truth from annotations with systematic disagreement than traditional aggregation\nmethods. We provide further analysis characterizing how differences in subpopulation sizes, rates of\ndisagreement, and rates of spam affect the performance of our model. Finally, we demonstrate that\ndownstream models trained on NUTMEG-aggregated data significantly outperform models trained on data\nfrom traditionally aggregation methods. Our results highlight the importance of accounting for both\nannotator competence and systematic disagreements when training on human-labeled data.",
    "authors": "Ivey; Jonathan; Gauch; Susan; Jurgens; David",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18884v1",
    "title": "MindFlow+: A Self-Evolving Agent for E-Commerce Customer Service",
    "abstract": "High-quality dialogue is crucial for e-commerce customer service,\nyet traditional intent-based systems struggle with dynamic, multi-turn interactions. We present\nMindFlowt, a self-evolving dialogue agent that learns domain-specific behavior by combining large\nLanguage models (LLMs) with imitation learning and offline reinforcement learning (RL). MindFlow+\nintroduces two data-centric mechanisms to guide learning: tool-augmented demonstration construction,\nwhich exposes the model to knowledge-enhanced and agentic (ReAct-style) interactions for effective\n‘tool use; and reward-conditioned data modeling, which aligns responses with task-specific goals using\nreward signals. To evaluate the model's role in response generation, we introduce the AI Contribution\nRatio, a novel metric quantifying AI involvement in dialogue. Experiments on real-world e-commerce\nconversations show that MindFlow+ outperforms strong baselines in contextual relevance, flexibility,\nand task accuracy. These results demonstrate the potential of combining LLMs tool reasoning, and\nreward-guided learning to build domain-specialized, context-aware dialogue systems.",
    "authors": "Gong; Ming; Huang; Xucheng; Xu; Ziheng; Asari; Vijayan K",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18863v1",
    "title": "Phoneme-Level Visual Speech Recognition via Point-Visual Fusion and Language Model Reconstruction",
    "abstract": "Visual Automatic Speech Recognition (V-ASR) is @ challenging task\n‘that involves interpreting spoken language solely from visual information, such as lip movements and\nfacial expressions. This task is notably challenging due to the absence of auditory cues and the\nvisual ambiguity of phonemes that exhibit similar visemes-distinct sounds that appear identical in lip\nmotions. Existing methods often aim to predict words or characters directly from visual cues, but they\nconmonly suffer from high error rates due to viseme ambiguity and require large amounts of pre-\n‘training data. We propose a novel phoneme-based two-stage framework that fuses visual and landmark\nmotion features, followed by an LLM model for word reconstruction to address these challenges. Stage 1\nconsists of V-ASR, which outputs the predicted phonemes, thereby reducing training complexity.\nMeanwhile, the facial landmark features address speaker-specific facial characteristics. Stage 2\ncomprises an encoder-decoder LLM model, NLLB, that reconstructs the output phonemes back to words.\nBesides using a large visual dataset for deep learning fine-tuning, our PV-ASR method demonstrates\nsuperior performance by achieving 17.4% WER on the LRS2 and 21.0% WER on the LRS3 dataset.\n\nSubmission history\n\nFrom: Matthew Kit Khinn Teng Mr [view email][vi] Fri, 25 Jul 2025 00:38:39 UTC (Bel KB)",
    "authors": "Teng; Matthew Kit Khinn; Zhang; Haibo; Saitoh; Takeshi",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18857v1",
    "title": "PrismRAG: Boosting RAG Factuality with Distractor Resilience and Strategized Reasoning",
    "abstract": "",
    "authors": "Kachuee; Mohammad; Gollapudi; Teja; Kim; Minseok; Huang; Yin; Sun; Kai; Yang; Xiao; Wang; Jiaqi; Shah; Nirav; Liu; Yue; Colak; Aaron; Kumar; Anuj; Yih; Wen-tau; Dong; Xin Luna",
    "date": "25 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18827v1",
    "title": "CueBuddy: helping non-native English speakers navigate English-centric STEM education",
    "abstract": "Students across the world in STEM classes, especially in the\nGlobal South, fall behind their peers who are more fluent in English, despite being at par with them\nin terms of Scientific prerequisites. While many of them are able to follow everyday English at ease,\nkey terms in English stay challenging. In most cases, such students have had most of their course\nprerequisites in a lower resource language. Live speech translation to lower resource languages is a\npromising area of research, however, models for speech translation can be too expensive on a large\nscale and often struggle with technical content. In this paper, we describe CueBuddy, which aims to\nremediate these issues by providing real-time “lexical cues” through technical keyword spotting along\nreal-time multilingual glossary lookup to help students stay up to speed with complex English jargon\nwithout disrupting their concentration on the lecture. We also describe the limitations and future\n‘extensions of our approach.",
    "authors": "Gupta; Pranav",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18791v1",
    "title": "Evaluating Code-Mixing in LLMs Across 18 Languages",
    "abstract": "Code-mixing, the practice of switching between languages within a\nconversation, presents unique challenges for traditional natural language processing. Existing\nbenchmarks, such as LinCE and GLUECOS, are limited by narrow language pairings and tasks, failing to\nadequately evaluate the code-mixing capabilities of large language models (LLMs). Despite the\nsignificance of code-mixing for multilingual users, research on LLMs in this context remains limited.\nAdditionally, current methods for generating code-mixed data are underdeveloped. In this paper, we\nconduct @ comprehensive evaluation of LLMs’ performance on code-mixed data across 18 languages from\nseven language families. We also propose a novel approach for generating synthetic code-mixed texts by\ncombining word substitution with GPT-4 prompting. Our analysis reveals consistent underperformance of\nLLMs on code-mixed datasets involving multiple language families. We suggest that improvements in\n‘training data size, model scale, and few-shot learning could enhance their performance.",
    "authors": "Yang; Yilun; Chai; Yekun",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18769v1",
    "title": "ylmmcl at Multilingual Text Detoxification 2025: Lexicon-Guided Detoxification and Classifier-Gated Rewriting",
    "abstract": "In this work, we introduce our solution for the Multilingual Text\nDetoxification Task in the PAN-2025 competition for the ylmmcl team: @ robust multilingual text\ndetoxification pipeline that integrates lexicon-guided tagging, a fine-tuned sequence-to-sequence\nmodel (s-nlp/mt0-x1-detox-orpo) and an iterative classifier-based gatekeeping mechanism. Our approach\ndeparts from prior unsupervised or monolingual pipelines by leveraging explicit toxic word annotation\nvia the multilingual_toxic_lexicon to guide detoxification with greater precision and cross-lingual\ngeneralization. Our final model achieves the highest STA (0.922) from our previous attempts, and an\naverage official score of 0.612 for toxic inputs in both the development and test sets. It also\n\nachieved xCOMET scores of @.793 (dev) and 0.787 (test). This\nbacktranslation methods across multiple languages, and shows\nsettings (English, Russian, French). Despite some trade-offs\nimprovements in detoxification strength. In the competition,\nscore of 0.612.",
    "authors": "Lai-Lopez; Nicole; Wang; Lusha; Yuan; Su; Zhang; Liza",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18762v1",
    "title": "The Role of Orthographic Consistency in Multilingual Embedding Models for Text Classification in Arabic-Script Languages",
    "abstract": "In natural language processing, multilingual models like mBERT and XLM-ROBERTa\npromise broad coverage but often struggle with languages that share a script yet differ in\n‘orthographic norms and cultural context. This issue is especially notable in Arabic-script languages\nsuch as Kurdish Sorani, Arabic, Persian, and Urdu. ble introduce the Arabic Script RoBERTa (AS-ROBERTa)\nfamily: four RoBERTa-based models, each pre-trained on a large corpus tailored to its specific\nlanguage. By focusing pre-training on language-specific script features and statistics, our models\ncapture patterns overlooked by general-purpose models. When fine-tuned on classification tasks, AS-\nRoBERTa variants outperform mBERT and XLM-RoBERTa by 2 to 5 percentage points. An ablation study\nconfirms that script-focused pre-training is central to these gains. Error analysis using confusion\nmatrices shows how shared script traits and domain-specific content affect performance. Our results\nhighlight the value of script-aware specialization for languages using the Arabic script and support\nfurther work on pre-training strategies rooted in script and language specificity.\n\nSubmission history\n\nFrom: Abdulhady Abdullah [view email][v1] Thu, 24 Jul 2025 19:28:33 UTC (1,066 KB)",
    "authors": "Abdullah; Abdulhady Abas; Gandomi; Amir H; Rashid; Tarik A; Mirjalili; Seyedali; Abualigah; Laith; Živković; Milena; Veisi; Hadi",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18742v1",
    "title": "Specification Self-Correction: Mitigating In-Context Reward Hacking Through Test-Time Refinement",
    "abstract": "Language models (IMs) are susceptible to in-context reward\nhacking, where they exploit flaws in tainted or faulty written specifications or rubrics to achieve\nhigh scores without fulfilling the user's true intent. We introduce Specification Self-Correction\n(SSC), a novel, test-time framework that enables an LM to identify and correct flaws within its own\nguiding specification. SSC employs a multi-step inference process where the model first generates a\nFesponse based on a potentially tainted specification, critiques its output, and then revises the\nspecification itself to remove the exploitable loophole. A final, more robust response is then\ngenerated using this self-corrected specification. Across experiments spanning creative writing and\nagentic coding tasks with several Lis, we demonstrate that while models initially game tainted\nspecifications in 50-70\\X of cases, the SSC process reduces this vulnerability by over 90\\%. This\ndynamic repair occurs at inference time, requires no weight modification, and leads to more robustly\naligned model behavior. Code at this https URL .",
    "authors": "Gallego; Víctor",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18624v1",
    "title": "Checklists Are Better Than Reward Models For Aligning Language Models",
    "abstract": "Language models must be adapted to understand and follow user\ninstructions. Reinforcement learning is widely used to facilitate this -- typically using fixed\ncriteria such as \"helpfulness\" and \"harmfulness”. In our work, we instead propose using flexible,\ninstruction-specific criteria as a means of broadening the impact that reinforcement learning can have\nin eliciting instruction following. We propose \"Reinforcement Learning from Checklist Feedback”\n(RLCF). From instructions, we extract checklists and evaluate how well responses satisfy each item -\nusing both AI judges and specialized verifier programs - then combine these scores to compute rewards\nfor RL. We compare RLCF with other alignment methods applied to a strong instruction following model\n(Quen2.5-78-Instruct) on five widely-studied benchmarks -- RLCF is the only method to improve\nperformance on every benchmark, including @ 4-point boost in hard satisfaction rate on FollowBench, a\n6-point increase on InFoBench, and a 3-point rise in win rate on Arena-Hard. These results establish\nchecklist feedback as a key tool for improving language models’ support of queries that express a\nmultitude of needs.",
    "authors": "Viswanathan; Vijay; Sun; Yanchao; Ma; Shuang; Kong; Xiang; Cao; Meng; Neubig; Graham; Wu; Tongshuang",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18618v1",
    "title": "TRPrompt: Bootstrapping Query-Aware Prompt Optimization from Textual Rewards",
    "abstract": "Prompt optimization improves the reasoning abilities of large\nLanguage models (LLMs) without requiring parameter updates to the target model. Following heuristic-\nbased \"Think step by step” approaches, the field has evolved in tuo main directions: while one group\nof methods uses textual feedback to elicit improved prompts from general-purpose LLMs in a training-\nfree way, @ concurrent line of research relies on numerical rewards to train a special prompt model,\ntailored for providing optimal prompts to the target model. In this paper, we introduce the Textual\nReward Prompt framework (TRPrompt), which unifies these approaches by directly incorporating textual\nfeedback into training of the prompt model. Our framework does not require prior dataset collection\nand is being iteratively improved with the feedback on the generated prompts. lihen coupled with the\ncapacity of an LLM to internalize the notion of what a “good” prompt is, the high-resolution signal\nprovided by the textual rewards allows us to train a prompt model yielding state-of-the-art query-\nspecific prompts for the problems from the challenging math datasets GsllHard and MATH.",
    "authors": "Nica; Andreea; Zakazov; Ivan; Baldwin; Nicolas Mario; Geng; Saibo; West; Robert",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18616v1",
    "title": "SynC: Synthetic Image Caption Dataset Refinement with One-to-many Mapping for Zero-shot Image Captioning",
    "abstract": "Zero-shot Image Captioning (ZIC) increasingly utilizes synthetic\ndatasets generated by text-to-image (T2I) models to mitigate the need for costly manual annotation.\nHowever, these T2I models often produce images that exhibit semantic misalignments with their\ncorresponding input captions (e.g., missing objects, incorrect attributes), resulting in noisy\nsynthetic image-caption pairs that can hinder model training. Existing dataset pruning techniques are\nlargely designed for removing noisy text in web-crawled data. However, these methods are ill-suited\nfor the distinct challenges of synthetic data, where captions are typically well-formed, but images\nmay be inaccurate representations. To address this gap, we introduce SynC, a novel framework\nspecifically designed to refine synthetic image-caption datasets for ZIC. Instead of conventional\nfiltering or regeneration, SynC focuses on reassigning captions to the most semantically aligned\nimages already present within the synthetic image pool. Our approach employs @ one-to-many mapping\nstrategy by initially retrieving multiple relevant candidate images for each caption. We then apply a\ncycle-consistency-inspired alignment scorer that selects the best image by verifying its ability to\nretrieve the original caption via image-to-text retrieval. Extensive evaluations demonstrate that SynC\nconsistently and significantly improves performance across various ZIC models on standard benchmarks\n(WS-COCO, Flickr30k, NoCaps), achieving state-of-the-art results in several scenarios. SynC offers an\neffective strategy for curating refined synthetic data to enhance ZIC.\n\nCurrent browse context:\n\ncs.V",
    "authors": "Kim; Si-Woo; Jeon; MinJu; Ye-Chan; Lee; Soeun; Taewhan; Dong-Jin",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18584v1",
    "title": "AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance Data Synthesis for Specialist LLMs",
    "abstract": "Despite the impressive performance of large language models\n(LuMs) in general domains, they often underperform in specialized domains. Existing approaches\ntypically rely on data synthesis methods and yield promising results by using unlabeled data to\ncapture domain-specific features. However, these methods either incur high computational costs or\nsuffer from performance limitations, while also demonstrating insufficient generalization across\ndifferent tasks. To address these challenges, we propose AQuilt, a framework for constructing\ninstruction-tuning data for any specialized domains from corresponding unlabeled data, including\nAnswer, Question, Unlabeled data, Inspection, Logic, and Task type. By incorporating logic and\ninspection, we encourage reasoning processes and self-inspection to enhance model performance.\nMoreover, customizable task instructions enable high-quality data generation for any task. As a\nresult, we construct a dataset of 703k examples to train a powerful data synthesis model. Experiments\nshow that AQuilt is comparable to DeepSeek-V3 while utilizing just 17% of the production cost. Further\nanalysis demonstrates that our generated data exhibits higher relevance to downstream tasks. Source\ncode, models, and scripts are available at this https URL.",
    "authors": "Ke; Xiaopeng; Deng; Hexuan; Liu; Xuebo; Rao; Jun; Song; Zhenxi; Yu; Zhang; Min",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18583v1",
    "title": "DR.EHR: Dense Retrieval for Electronic Health Record with Knowledge Injection and Synthetic Data",
    "abstract": "Electronic Health Records (EHRs) are pivotal in clinical\npractices, yet their retrieval remains a challenge mainly due to semantic gap issues. Recent\nadvancements in dense retrieval offer promising solutions but existing models, both general-domain and\nbiomedical-domain, fall short due to insufficient medical knowledge or mismatched training corpora.\nThis paper introduces \\texttt{this http URL}, a series of dense retrieval models specifically tailored\nfor EHR retrieval. We propose a two-stage training pipeline utilizing MIMIC-IV discharge summaries to\naddress the need for extensive medical knowledge and large-scale training data. The first stage\ninvolves medical entity extraction and knowledge injection from a biomedical knowledge graph, while\n‘the second stage employs large language models to generate diverse training data. We train tuo\nvariants of \\texttt{this http URL}, with 110M and 78 parameters, respectively. Evaluated on the CliniQ\nbenchmark, our models significantly outperforms all existing dense retrievers, achieving state-of-the-\nart results. Detailed analyses confirm our models’ superiority across various match and query types,\nparticularly in challenging semantic matches like implication and abbreviation. Ablation studies\nvalidate the effectiveness of each pipeline component, and supplementary experiments on EHR QA\ndatasets demonstrate the models’ generalizability on natural language questions, including complex\nones with multiple entities. This work significantly advances EHR retrieval, offering a robust\nsolution for clinical applications.\n\nCurrent browse context:\n\ncs.IR",
    "authors": "Zhao; Zhengyun; Ying; Huaiyuan; Zhong; Yue; Yu; Sheng",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18580v1",
    "title": "System Report for CCL25-Eval Task 10: SRAG-MAV for Fine-Grained Chinese Hate Speech Recognition",
    "abstract": "This paper presents our system for CCL25-Eval Task 10, addressing\nFine-Grained Chinese Hate Speech Recognition (FGCHSR). We propose a novel SRAG-MAV framework that\nsynergistically integrates task reformulation(TR), Self-Retrieval-Augmented Generation (SRAG), and\nMulti-Round Accumulative Voting (MAV). Our method reformulates the quadruplet extraction task into\ntriplet extraction, uses dynamic retrieval from the training set to create contextual prompts, and\napplies multi-round inference with voting to improve output stability and performance. Our system,\nbased on the Quen2.5-78 model, achieves a Hard Score of 26.66, a Soft Score of 48.35, and an Average\nScore of 37.505 on the STATE ToxiCN dataset, significantly outperforming baselines such as GPT-40\n(Average Score 15.63) and fine-tuned Quen2.5-78 (Average Score 35.365). The code is available at this\nhttps URL.",
    "authors": "Wang; Jiahao; Liu; Ramen; Zhang; Longhui; Li; Jing",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18578v1",
    "title": "Wide-In, Narrow-Out: Revokable Decoding for Efficient and Effective DLLMs",
    "abstract": "Diffusion Large Language Models (DLLMs) have emerged as a compelling alternative to\nAutoregressive models, designed for fast parallel generation. However, existing DLLMs are plagued by a\nsevere quality-speed trade-off, where faster parallel decoding leads to significant performance\ndegradation. We attribute this to the irreversibility of standard decoding in DLLMs, which is easily\npolarized into the wrong decoding direction along with early error context accumulation. To resolve\n‘this, we introduce Wide-In, Narrow-Out (WINO), a training-free decoding algorithm that enables\nrevokable decoding in DLLMS. WINO employs a parallel draft-and-verify mechanism, aggressively drafting\nmultiple tokens while simultaneously using the model's bidirectional context to verify and re-mask\nsuspicious ones for refinement. Verified in open-source DLLMs like LLaDA and MlaDA, WINO is shown to\ndecisively improve the quality-speed trade-off. For instance, on the GSM8K math benchmark, it\naccelerates inference by 6§\\times$ while improving accuracy by 2.58%; on Flickr30K captioning, it\nachieves 2 10$\\times$ speedup with higher performance. More comprehensive experiments are conducted to\ndemonstrate the superiority and provide an in-depth understanding of WINO.",
    "authors": "Hong; Feng; Yu; Geng; Ye; Huang; Haicheng; Zheng; Zhang; Ya; Wang; Jiangchao",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18576v2",
    "title": "SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\\circ}$ Law",
    "abstract": "",
    "authors": "Lab; Shanghai AI; Bao; Yicheng; Chen; Guanxu; Mingkang; Yunhao; Chiyu; Lingjie; Sirui; Xinquan; Jie; Yu; Deng; Ding; Yizhuo; Dan; Xiaoshan; Yi; Dong; Zhichen; Du; Lingxiao; Fan; Feng; Xinshun; Fu; Yanwei; Gao; Ge; Ruijun; Gu; Tianle; Lujun; Jiaxuan; He; Qianxi; Hou; Hu; Xuhao; Hong; Kaichen; Shiyang; Jiang; Lei; Shanzhe; Li; Hao; Juncheng; Xiangtian; Yafu; Xueyan; Qihua; Zhixuan; Bangwei; Zongkai; Lu; Chaochao; Xiaoya; Zhenghao; Lv; Qitan; Ma; Caoyuan; Jiachen; Zhongtian; Meng; Miao; Ziqi; Niu; Yazhe; Peng; Pu; Qi; Han; Xingge; Qu; Jingjing; Jiashu; Wanying; Wenwen; Xiaoye; Ren; Shao; Jing; Wenqi; Shuai; Shi; Song; Xin; Teng; Yan; Tong; Xuan; Wang; Xuhong; Shujie; Ruofan; Wenjie; Yajie; Wei; Muhao; Wen; Xiaoyu; Wu; Xiong; Xu; Chao; Yao; Ye; Zhenyun; Zhang; Bo; Jinxuan; Zheng; Zhou; Zhanhui; Zhu",
    "date": ""
  },
  {
    "url": "http://arxiv.org/abs/2507.18572v1",
    "title": "PosterMate: Audience-driven Collaborative Persona Agents for Poster Design",
    "abstract": "Poster designing can benefit from synchronous feedback from\ntarget audiences. However, gathering audiences with diverse perspectives and reconciling them on\ndesign edits can be challenging. Recent generative AI models present opportunities to simulate human-\nlike interactions, but it is unclear how they may be used for feedback processes in design. We\nintroduce Postertlate, a poster design assistant that facilitates collaboration by creating audience-\ndriven persona agents constructed from marketing documents. Posterate gathers feedback from each\npersona agent regarding poster components, and stimulates discussion with the help of a moderator to\nreach a conclusion. These agreed-upon edits can then be directly integrated into the poster design.\nThrough our user study (N-12), we identified the potential of Postertate to capture overlooked\nviewpoints, while serving as an effective prototyping tool. Additionally, our controlled online\nevaluation (N=100) revealed that the feedback from an individual persona agent is appropriate given\nits persona identity, and the discussion effectively synthesizes the different persona agents’\nperspectives.\n\nCurrent browse context:\n\ncs-HE",
    "authors": "Shin; Donghoon; Lee; Daniel; Hsieh; Gary; Chan; Gromit Yeuk-Yin",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18570v1",
    "title": "Hybrid Tokenization Strategy for DNA Language Model using Byte Pair Encoding and K-MER Methods",
    "abstract": "This paper presents a novel hybrid tokenization strategy that\nenhances the performance of DNA Language Models (DLIMs) by combining 6-mer tokenization with Byte Pair\nEncoding (BPE-600). Traditional k-mer tokenization is effective at capturing local DNA sequence\nstructures but often faces challenges, including uneven token distribution and a limited understanding\nof global sequence context. To address these limitations, we propose merging unique 6mer tokens with\noptimally selected BPE tokens generated through 600 BPE cycles. This hybrid approach ensures a\nbalanced and context-aware vocabulary, enabling the model to capture both short and long patterns\nwithin DNA sequences simultaneously. A foundational DLM trained on this hybrid vocabulary was\nevaluated using next-k-mer prediction as a fine-tuning task, demonstrating significantly improved\nperformance. The model achieved prediction accuracies of 10.78% for 3-mers, 10.1% for 4-mers, and\n4.12% for S-mers, outperforming state-of-the-art models such as NT, DNABERT2, and GROVER. These\nresults highlight the ability of the hybrid tokenization strategy to preserve both the local sequence\nstructure and global contextual information in DNA modeling. This work underscores the importance of\nadvanced tokenization methods in genomic language modeling and lays a robust foundation for future\napplications in downstream DNA sequence analysis and biological research.\n\nSubmission history\n\nFrom: Md Hasibur Rahman [view email][vi] Thu, 24 Jul 2025 16:45:23 UTC (4,324 KB)",
    "authors": "Sapkota; Ganesh; Rahman; Md Hasibur",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18562v1",
    "title": "GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation",
    "abstract": "Multimodal Machine Translation (IIT) has demonstrated the significant help of visual\ninformation in machine translation. However, existing MPIT methods face challenges in leveraging the\nmodality gap by enforcing rigid visual-linguistic alignment whilst being confined to inference within\n‘their trained multimodal domains. In this work, we construct novel multimodal scene graphs to preserve\nand integrate modality-specific information and introduce GIIFT, a two-stage Graph-guided Inductive\nImage-Free MNT framework that uses a cross-modal Graph Attention Network adapter to learn multimodal\nknowledge in a unified fused space and inductively generalize it to broader image-free translation\ndomains. Experimental results on the Multi3ek dataset of English-to-French and English-to-German tasks\ndemonstrate that our GIIFT surpasses existing approaches and achieves the state-of-the-art, even\nwithout images during inference. Results on the WHT benchmark show significant improvements over the\nimage-free translation baselines, demonstrating the strength of GIIFT towards inductive image-free\ninference.",
    "authors": "Xiong; Jiafeng; Zhao; Yuting",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18546v1",
    "title": "GLiNER2: An Efficient Multi-Task Information Extraction System with Schema-Driven Interface",
    "abstract": "Information extraction (IE) is fundamental to numerous NLP\napplications, yet existing solutions often require specialized models for different tasks or rely on\ncomputationally expensive large language models. We present GLNER2, a unified framework that enhances\n‘the original GLINER architecture to support named entity recognition, text classification, and\nhierarchical structured data extraction within a single efficient model. Built pretrained transformer\nencoder architecture, GLiNER2 maintains CPU efficiency and compact size while introducing multi-task\ncomposition through an intuitive schema-based interface. Our experiments demonstrate competitive\nperformance across extraction and classification tasks with substantial improvements in deployment\naccessibility compared to LLM-based alternatives. We release GLINER2 as an open-source pip-installable\nlibrary with pre-trained models and documentation at this https URL.",
    "authors": "Zaratiana; Urchade; Pasternak; Gil; Boyd; Oliver; Hurn-Maloney; George; Lewis; Ash",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18542v1",
    "title": "Effective Multi-Task Learning for Biomedical Named Entity Recognition",
    "abstract": "Biomedical Named Entity Recognition presents significant\nchallenges due to the complexity of biomedical terminology and inconsistencies in annotation across\ndatasets. This paper introduces SRU-NER (Slot-based Recurrent Unit NER), a novel approach designed to\nhandle nested named entities while integrating multiple datasets through an effective multi-task\nlearning strategy. SRU-NER mitigates annotation gaps by dynamically adjusting loss computation to\navoid penalizing predictions of entity types absent in a given dataset. Through extensive experiments,\nincluding @ cross-corpus evaluation and human assessment of the model's predictions, SRU-NER achieves\ncompetitive performance in biomedical and general-domain NER tasks, while improving cross-domain\ngeneralization.",
    "authors": "Ruano; João; Correia; Gonçalo M; Barreiros; Leonor; Mendes; Afonso",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18523v1",
    "title": "The Moral Gap of Large Language Models",
    "abstract": "Moral foundation detection is crucial for analyzing social discourse and developing\nethically-aligned AI systems. while large language models excel across diverse tasks, their\nperformance on specialized moral reasoning remains unclear.\n\nThis study provides the first comprehensive comparison betueen state-of-the-art LLMs and fine-tuned\ntransformers across Twitter and Reddit datasets using ROC, PR, and DET curve analysis.\n\nResults reveal substantial performance gaps, with LLMs exhibiting high false negative rates and\nsystematic under-detection of moral content despite prompt engineering efforts. These findings\ndemonstrate that task-specific fine-tuning remains superior to prompting for moral reasoning\napplications.\n\nCurrent browse context:\n\ncs.cL",
    "authors": "Skorski; Maciej; Landowska; Alina",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18504v1",
    "title": "Not All Features Deserve Attention: Graph-Guided Dependency Learning for Tabular Data Generation with Language Models",
    "abstract": "Large Language Models (LLMs) have shoun strong potential for tabular data generation\nby modeling textualized feature-value pairs. However, tabular data inherently exhibits sparse feature-\nLevel dependencies, where many feature interactions are structurally insignificant. This creates a\nFundamental mismatch as Lis\" self-attention mechanism inevitably distributes focus across all pairs,\ndiluting attention on critical relationships, particularly in datasets with complex dependencies or\nsemantically ambiguous features. To address this limitation, we propose GraDe (Graph-Guided Dependency\nLearning), @ novel method that explicitly integrates sparse dependency graphs into LLMs\" attention\nmechanism. Grae employs @ lightweight dynamic graph learning module guided by externally extracted\nfunctional dependencies, prioritizing key feature interactions while suppressing irrelevant ones. Our\nexperiments across diverse real-world datasets demonstrate that Grabe outperforms existing LLN-based\napproaches by up to 12% on complex datasets uhile achieving competitive results with state-of-the-art\napproaches in synthetic data quality. Our method is minimally intrusive yet effective, offering a\npractical solution for structure-aware tabular data modeling with LLMs.",
    "authors": "Zhang; Zheyu; Yang; Shuo; Prenkaj; Bardh; Kasneci; Gjergji",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18455v1",
    "title": "LLM-based Embedders for Prior Case Retrieval",
    "abstract": "In common law systems, legal professionals such as lawyers and\njudges rely on precedents to build their arguments. As the volume of cases has grown massively over\n‘time, effectively retrieving prior cases has become essential. Prior case retrieval (PCR) is an\ninformation retrieval (IR) task that aims to automatically identify the most relevant court cases for\na specific query from 2 large pool of potential candidates. while IR methods have seen several\nparadigm shifts over the last few years, the vast majority of PCR methods continue to rely on\n‘traditional IR methods, such as 6125. The state-of-the-art deep learning IR methods have not been\nsuccessful in PCR due to two key challenges: i. Lengthy legal text limitation; when using the powerful\nBERT-based transformer models, there is a limit of input text lengths, which inevitably requires to\nshorten the input via truncation or division with a loss of legal context information. ii. Lack of\nlegal training data; due to data privacy concerns, available PCR datasets are often limited in size,\nmaking it difficult to train deep learning-based models effectively. In this research, we address\n‘these challenges by leveraging LLM-based text embedders in PCR. LLM-based embedders support longer\ninput lengths, and since we use them in an unsupervised manner, they do not require training data,\naddressing both challenges simultaneously. In this paper, we evaluate state-of-the-art LU\"-based text\nembedders in four PCR benchmark datasets and show that they outperform BM25 and supervised\n‘transformer-based models.\n\nSubmission history\n\nFrom: Damith Premasiri Dola Mullage [view email][v1] Thu, 24 Jul 2025 14:36:10 UTC (5,602 KB)",
    "authors": "Premasiri; Damith; Ranasinghe; Tharindu; Mitkov; Ruslan",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18451v1",
    "title": "Generation of Synthetic Clinical Text: A Systematic Review",
    "abstract": "Generating clinical synthetic text represents an effective solution for common\nclinical NLP issues like sparsity and privacy. This paper aims to conduct a systematic review on\ngenerating synthetic medical free-text by formulating quantitative analysis to three research\nquestions concerning (i) the purpose of generation, (ii) the techniques, and (iii) the evaluation\nmethods. We searched Publled, ScienceDirect, lieb of Science, Scopus, IEEE, Google Scholar, and arXiv\ndatabases for publications associated with generating synthetic medical unstructured free-text. We\nhave identified 94 relevant articles out of 1,398 collected ones. A great deal of attention has been\ngiven to the generation of synthetic medical text from 2018 onwards, where the main purpose of such a\ngeneration is towards text augmentation, assistive writing, corpus building, privacy-preserving,\nannotation, and usefulness. Transformer architectures were the main predominant technique used to\ngenerate the text, especially the GPTs. On the other hand, there were four main aspects of evaluation,\nincluding similarity, privacy, structure, and utility, where utility was the most frequent method used\nto assess the generated synthetic medical text. Although the generated synthetic medical text\ndemonstrated a moderate possibility to act as real medical documents in different downstream NLP\n‘tasks, it has proven to be a great asset as augmented, complementary to the real documents, towards\nimproving the accuracy and overcoming sparsity/undersampling issues. Yet, privacy is still a major\nissue behind generating synthetic medical text, where more human assessments are needed to check for\n‘the existence of any sensitive information. Despite that, advances in generating synthetic medical\ntext will considerably accelerate the adoption of workflows and pipeline development, discarding the\n‘time-consuming legalities of data transfer.",
    "authors": "Alshaikhdeeb; Basel; Hemedan; Ahmed Abdelmonem; Ghosh; Soumyabrata; Balaur; Irina; Satagopam; Venkata",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18448v1",
    "title": "Restoring Rhythm: Punctuation Restoration Using Transformer Models for Bangla, a Low-Resource Language",
    "abstract": "Punctuation restoration enhances the readability of text and is\ncritical for post-processing tasks in Automatic Speech Recognition (ASR), especially for low-resource\nlanguages like Bangla. In this study, we explore the application of transformer-based models,\nspecifically XLM-RoBERTa-large, to automatically restore punctuation in unpunctuated Bangla text. We\nfocus on predicting four punctuation marks: period, comma, question mark, and exclamation mark across\ndiverse text domains. To address the scarcity of annotated resources, we constructed a large, varied\n‘training corpus and applied data augmentation techniques. Our best-performing model, trained with an\naugmentation factor of alpha = 0.20%, achieves an accuracy of 97.1% on the News test set, 91.2% on the\nReference set, and 90.2% on the ASR set.\n\nResults show Strong generalization to reference and ASR transcripts, demonstrating the model's\neffectiveness in real-world, noisy scenarios. This work establishes a strong baseline for Bangla\npunctuation restoration and contributes publicly available datasets and code to support future\nresearch in low-resource NLP.\n\nSubmission history\n\nFrom: Md Adyelullahil Mamun [view email][vi] Thu, 24 Jul 2025 14:33:13 UTC (405 KB)\n\nCurrent browse context:\n\ncs.cL",
    "authors": "Mamun; Md Obyedullahil; Md Adyelullahil; Ahmad; Arif; Emu; Md Imran Hossain",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18442v1",
    "title": "AraTable: Benchmarking LLMs' Reasoning and Understanding of Arabic Tabular Data",
    "abstract": "The cognitive and reasoning abilities of large language models\n(LLMs) have enabled remarkable progress in natural language processing. However, their performance in\ninterpreting structured data, especially in tabular formats, remains limited. Although benchmarks for\nEnglish tabular data are widely available, Arabic is still underrepresented because of the limited\navailability of public resources and its unique language features. To address this gap, we present\nAraTable, a novel and comprehensive benchmark designed to evaluate the reasoning and understanding\ncapabilities of LLMs when applied to Arabic tabular data. AraTable consists of various evaluation\n‘tasks, such as direct question answering, fact verification, and complex reasoning, involving a wide\nrange of Arabic tabular sources. Our methodology follows a hybrid pipeline, where initial content is\ngenerated by LLMs and subsequently filtered and verified by human experts to ensure high dataset\nquality. Initial analyses using AraTable show that, while LLMs perform adequately on simpler tabular\ntasks such as direct question answering, they continue to face significant cognitive challenges when\ntasks require deeper reasoning and fact verification. This indicates that there are substantial\n‘opportunities for future work to improve performance on complex tabular reasoning tasks. We also\npropose a fully automated evaluation framework that uses a self-deliberation mechanism and achieves\nperformance nearly identical to that of human judges. This research provides @ valuable, publicly\navailable resource and evaluation framework that can help accelerate the development of foundational\nmodels for processing and analysing Arabic structured data.",
    "authors": "Alshaikh; Rana; Alghanmi; Israa; Jeawak; Shelan",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18417v1",
    "title": "FinDPO: Financial Sentiment Analysis for Algorithmic Trading through Preference Optimization of LLMs",
    "abstract": "Opinions expressed in online finance-related textual data are\nhaving an increasingly profound impact on trading decisions and market movements. This trend\nhighlights the vital role of sentiment analysis as a tool for quantifying the nature and strength of\nsuch opinions. With the rapid development of Generative AI (GenAI), supervised fine-tuned (SFT) large\nLanguage models (LLMs) have become the de facto standard for financial sentiment analysis. However,\n‘the SFT paradigm can lead to memorization of the training data and often fails to generalize to unseen\nsamples. This is a critical limitation in financial domains, where models must adapt to previously\nunobserved events and the nuanced, domain-specific language of finance. To this end, we introduce\nFinDPO, the first finance-specific LLM framework based on post-training human preference alignment via\nDirect Preference Optimization (DPO). The proposed FinDPO achieves state-of-the-art performance on\nstandard sentiment classification benchmarks, outperforming existing supervised fine-tuned models by\n11% on the average. Uniquely, the FinDPO framework enables the integration of @ fine-tuned causal LLM\ninto realistic portfolio strategies through a novel ‘logit-to-score’ conversion, which transforms\ndiscrete sentiment predictions into continuous, rankable sentiment scores (probabilities). In this\nway, simulations demonstrate that FinDPO is the first sentiment-based approach to maintain substantial\npositive returns of 67% annually and strong risk-adjusted performance, as indicated by a Sharpe ratio\nof 2.0, even under realistic transaction costs of 5 basis points (bps).\n\nCurrent browse context:\n\ncs.cL",
    "authors": "Iacovides; Giorgos; Zhou; Wuyang; Mandic; Danilo",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18406v1",
    "title": "Factual Inconsistencies in Multilingual Wikipedia Tables",
    "abstract": "Wikipedia serves as a globally accessible knowledge source with\ncontent in over 300 languages. Despite covering the same topics, the different versions of Wikipedia\nare written and updated independently. This leads to factual inconsistencies that can impact the\nneutrality and reliability of the encyclopedia and AI systems, which often rely on Wikipedia as a main\n‘training source. This study investigates cross-lingual inconsistencies in Wikipedia's structured\ncontent, with a focus on tabular data. We developed a methodology to collect, align, and analyze\ntables from Wikipedia multilingual articles, defining categories of inconsistency. We apply various\nquantitative and qualitative metrics to assess multilingual alignment using a sample dataset. These\ninsights have implications for factual verification, multilingual knowledge interaction, and design\nfor reliable AI systems leveraging Wikipedia content.\n\nCurrent browse context:\n\ncs.cL",
    "authors": "Cappa; Silvia; Kong; Lingxiao; Peet; Pille-Riin; Wei; Fanfu; Zhou; Yuchen; Kalo; Jan-Christoph",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18392v1",
    "title": "CLEAR: Error Analysis via LLM-as-a-Judge Made Easy",
    "abstract": "The evaluation of Large Language Models (LLMs) increasingly\nrelies on other LLMs acting as judges. However, current evaluation paradigms typically yield a single\nscore or ranking, answering which model is better but not why. While essential for benchmarking, these\n‘top-level scores obscure the specific, actionable reasons behind a model's performance. To bridge this\ngap, we introduce CLEAR, an interactive, open-source package for LiM-based error analysis. CLEAR first\ngenerates per-instance textual feedback, then it creates a set of system-level error issues, and\nquantifies the prevalence of each identified issue. Our package also provides users with an\ninteractive dashboard that allows for a comprehensive error analysis through aggregate visualizations,\napplies interactive filters to isolate specific issues or score ranges, and drills down to the\nindividual instances that exemplify a particular behavioral pattern. We demonstrate CLEAR analysis for\nRAG and Math benchmarks, and showcase its utility through a user case study.\n\nCurrent browse context:\n\ncs.cL",
    "authors": "Yehudai; Asaf; Eden; Lilach; Perlitz; Yotam; Bar-Haim; Roy; Shmueli-Scheuer; Michal",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18343v1",
    "title": "Hybrid Annotation for Propaganda Detection: Integrating LLM Pre-Annotations with Human Intelligence",
    "abstract": "Propaganda detection on social media remains challenging due to\n‘task complexity and limited high-quality labeled data. This paper introduces a novel framework that\ncombines human expertise with Large Language Model (LLM) assistance to improve both annotation\nconsistency and scalability. We propose a hierarchical taxonomy that organizes 14 fine-grained\npropaganda techniques into three broader categories, conduct a human annotation study on the HOP\ndataset that reveals low inter-annotator agreement for fine-grained labels, and implement an LLM-\nassisted pre-annotation pipeline that extracts propagandistic spans, generates concise explanations,\nand assigns local labels as well as @ global label. A secondary human verification study shows\nsignificant improvements in both agreement and time-efficiency. Building on this, we fine-tune smaller\nLanguage models (SLMs) to perform structured annotation. Instead of fine-tuning on human annotations,\nwe train on high-quality LLM-generated data, allowing a large model to produce these annotations and a\nsmaller model to learn to generate them via knowledge distillation. Our work contributes towards the\ndevelopment of scalable and robust propaganda detection systems, supporting the idea of transparent\nand accountable media ecosystems in line with SDG 16. The code is publicly available at our GitHub\nrepository.",
    "authors": "Sahitaj; Ariana; Premtim; Solopova; Veronika; Li; Jiaao; Möller; Sebastian; Schmitt; Vera",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18340v1",
    "title": "TDR: Task-Decoupled Retrieval with Fine-Grained LLM Feedback for In-Context Learning",
    "abstract": "In-context learning (ICL) has become a classic approach for enabling LLMs to handle\nvarious tasks based on 2 few input-output examples. The effectiveness of ICL heavily relies on the\nquality of these examples, and previous works which focused on enhancing example retrieval\ncapabilities have achieved impressive performances. However, tuo challenges remain in retrieving high-\nquality examples: (1) Difficulty in distinguishing cross-task data distributions, (2) Difficulty in\nmaking the fine-grained connection between retriever output and feedback from LLMs. In this paper, we\npropose a novel framework called TDR. TDR decouples the ICL examples from different tasks, which\nenables the retrieval module to retrieve examples specific to the target task within a multi-task\ndataset. Furthermore, TDR models fine-grained feedback from LLMs to supervise and guide the training\nof the retrieval module, which helps to retrieve high-quality examples. We conducted extensive\nexperiments on a suite of 30 NLP tasks, the results demonstrate that TDR consistently improved results\nacross all datasets and achieves state-of-the-art performance. Meanwhile, our approach is a plug-and-\nplay method, which can be easily combined with various LLMs to improve example retrieval abilities for\nICL. The code is available at this https URL.",
    "authors": "Chen; Yifu; Huang; Bingchen; Wang; Zhiling; Du; Yuanchao; Luo; Junfeng; Shen; Lei; Zhineng",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18338v1",
    "title": "Uncertainty Quantification for Evaluating Machine Translation Bias",
    "abstract": "In machine translation (MT), when the source sentence includes a\nLexeme whose gender is not overtly marked, but whose target-language equivalent requires gender\nspecification, the model must infer the appropriate gender from the context and/or external knowledge.\nStudies have shown that MT models exhibit biased behaviour, relying on stereotypes even when they\nClash with contextual information. We posit that apart from confidently translating using the correct\ngender when it is evident from the input, models should also maintain uncertainty about the gender\nwhen it is ambiguous. Using recently proposed metrics of semantic uncertainty, we find that models\nwith high translation and gender accuracy on unambiguous instances do not necessarily exhibit the\nexpected level of uncertainty in ambiguous ones. Similarly, debiasing has independent effects on\nambiguous and unambiguous translation instances.\n\n‘Submission history\n\nFrom: Ieva Raminta Staliunaite [view email][v1] Thu, 24 Jul 2025 1:",
    "authors": "Staliūnaitė; Ieva Raminta; Cheng; Julius; Vlachos; Andreas",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18305v1",
    "title": "BadReasoner: Planting Tunable Overthinking Backdoors into Large Reasoning Models for Fun or Profit",
    "abstract": "Large reasoning models (LRMs) have emerged as a significant\nadvancement in artificial intelligence, representing a specialized class of large language models\n(LLMs) designed to tackle complex reasoning tasks. The defining characteristic of LRMs lies in their\nextensive chain-of-thought (CoT) reasoning capabilities. In this paper, we identify a previously\nunexplored attack vector against LRMs, which we term “overthinking backdoors\". We advance this concept\nby proposing a novel tunable backdoor, which moves beyond simple on/off attacks to one where an\nattacker can precisely control the extent of the model's reasoning verbosity. Our attack is\nimplemented through a novel data poisoning methodology. It pairs a tunable trigger-where the number of\nrepetitions signals the desired intensity-with a correspondingly verbose CoT response. These responses\nare programmatically generated by instructing a teacher LLM to inject a controlled number of redundant\nrefinement steps into a correct reasoning process. The approach preserves output correctness, which\nensures stealth and establishes the attack as a pure resource-consumption vector. Extensive empirical\nresults on various LRMs demonstrate that our method can reliably trigger a controllable, multi-fold\nincrease in the length of the reasoning process, without degrading the final answer\"s correctness. Our\nsource code is available at this https URL.",
    "authors": "Yi; Biao; Fei; Zekun; Geng; Jianing; Li; Tong; Nie; Zheli",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18302v1",
    "title": "LoRA-Leak: Membership Inference Attacks Against LoRA Fine-tuned Language Models",
    "abstract": "Language Models (Lis) typically adhere to a “pre-training and fine-tuning\" paradign,\nhere @ universal pre-trained model can be fine-tuned to cater to various specialized domains. Low-\nRank Adaptation (LORA) has gained the most widespread use in LM fine-tuning due to its lightweight\ncomputational cost and remarkable performance. Because the proportion of parameters tuned by LORA is\nrelatively small, there might be a misleading inpression that the LoRA fine-tuning data is\ninvulnerable to Nenbership Inference attacks (NIAs). However, we identity that utilizing the pre-\ntrained model can induce more information leakage, which is neglected by existing MIAs. therefore, we\nintroduce LoRA-Leak, a holistic evaluation framework for MIAs against the fine-tuning datasets of Lis.\nLoRs-Leak incorporates fifteen menbership inference attacks, including ten existing MIAs, and five\nimproved MIAs that leverage the pre-trained model as a reference. In experiments, we apply LoRA-Leak\nto three advanced Lis across three popular natural language processing tasks, demonstrating that LoRA-\nbased fine-tuned LNs are still vulnerable to MIAs (e.g-, 9.775 AUC under conservative fine-tuning\nsettings). lie also applied LoRA-Leak to different fine-tuning settings to understand the resulting\nprivacy risks. he further explore four defenses and find that only dropout and excluding specific LM\nlayers during fine-tuning effectively mitigate MIA risks while maintaining utility. We highlight that\nunder the “pre-training and fine-tuning\" paradign, the existence of the pre-trained model makes MIA a\nmore severe risk for LoRA-based Li's. lie hope that our findings can provide guidance on data privacy\nprotection for specialized LM providers.\n\nCurrent browse context:\n\ncs.cR",
    "authors": "Ran; Delong; He; Xinlei; Cong; Tianshuo; Wang; Anyu; Li; Qi; Xiaoyun",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18294v1",
    "title": "StyleAdaptedLM: Enhancing Instruction Following Models with Efficient Stylistic Transfer",
    "abstract": "",
    "authors": "Ramu; Pritika; Saxena; Apoorv; Y; Meghanath M; Sankar; Varsha; Basu; Debraj",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18264v1",
    "title": "Zero-shot OCR Accuracy of Low-Resourced Languages: A Comparative Analysis on Sinhala and Tamil",
    "abstract": "Solving the problem of Optical Character Recognition (OCR) on\nprinted text for Latin and its derivative scripts can now be considered settled due to the volumes of\nresearch done on English and other High-Resourced Languages (HL). However, for Low-Resourced\nLanguages (LRL) that use unique scripts, it remains an open problem. This study presents a comparative\nanalysis of the zero-shot performance of six distinct OCR engines on two LRLs: Sinhala and Tamil. The\nselected engines include both commercial and open-source systems, aiming to evaluate the strengths of\neach category. The Cloud Vision API, Surya, Document AI, and Tesseract were evaluated for both Sinhala\nand Tamil, while Subasa OCR and EasyOCR were examined for only one language due to their limitations.\n‘The performance of these systems was rigorously analysed using five measurement techniques to assess\naccuracy at both the character and word levels. According to the findings, Surya delivered the best\nperformance for Sinhala across all metrics, with a WER of 2.61%. Conversely, Document AI excelled\nacross all metrics for Tamil, highlighted by a very low CER of 0.78%. In addition to the above\nanalysis, we also introduce a novel synthetic Tamil OCR benchmarking dataset.\n\nSubmission history\n\nFrom: Nevidu Jayatilleke Hr. [view email][v1] Thu, 24 Jul 2025 10:08:43 UTC (9,528 KB)",
    "authors": "Jayatilleke; Nevidu; De Silva; Nisansa",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18263v1",
    "title": "Locate-and-Focus: Enhancing Terminology Translation in Speech Language Models",
    "abstract": "Direct speech translation (ST) has garnered increasing attention\nnowadays, yet the accurate translation of terminology within utterances remains a great challenge. In\n‘this regard, current studies mainly concentrate on leveraging various translation knowledge into ST\nmodels. However, these methods often struggle with interference from irrelevant noise and can not\nfully utilize the translation knowledge. To address these issues, in this paper, we propose a novel\nLocate-and-Focus method for terminology translation. It first effectively locates the speech clips\ncontaining terminologies within the utterance to construct translation knowledge, minimizing\nirrelevant information for the ST model. Subsequently, it associates the translation knowledge with\n‘the utterance and hypothesis from both audio and textual modalities, allowing the ST model to better\nfocus on translation knowledge during translation. Experimental results across various datasets\ndemonstrate that our method effectively locates terminologies within utterances and enhances the\nsuccess rate of terminology translation, while maintaining robust general translation performance.",
    "authors": "Wu; Suhang; Tang; Jialong; Yang; Chengyi; Zhang; Pei; Baosong; Li; Junhui; Yao; Junfeng; Min; Su; Jinsong",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18252v1",
    "title": "Multimodal Behavioral Patterns Analysis with Eye-Tracking and LLM-Based Reasoning",
    "abstract": "Eye-tracking data reveals valuable insights into users’ cognitive\nstates but is difficult to analyze due to its structured, non-linguistic nature. While large language\nmodels (LLMs) excel at reasoning over text, they struggle with temporal and numerical data. This paper\npresents a multimodal human-AI collaborative framework designed to enhance cognitive pattern\n‘extraction from eye-tracking signals. The framework includes: (1) a multi-stage pipeline using\nhorizontal and vertical segmentation alongside LLM reasoning to uncover latent gaze patterns; (2) an\nExpert-Model Co-Scoring Module that integrates expert judgment with LLM output to generate trust\nscores for behavioral interpretations; and (3) a hybrid anomaly detection module combining LSTM-based\n‘temporal modeling with LLM-driven semantic analysis. Our results across several LLMs and prompt\nstrategies show improvements in consistency, interpretability, and performance, with up to 50%\naccuracy in difficulty prediction tasks. This approach offers a scalable, interpretable solution for\ncognitive modeling and has broad potential in adaptive learning, human-computer interaction, and\n‘educational analytics.",
    "authors": "Guo; Dongyang; Abdrabou; Yasmeen; Thaqi; Enkeleda; Kasneci; Enkelejda",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18224v2",
    "title": "Assemble Your Crew: Automatic Multi-agent Communication Topology Design via Autoregressive Graph Generation",
    "abstract": "Multi-agent systems (MAS) based on large language models (LLMs)\nhave emerged as a powerful solution for dealing with complex problems across diverse domains. The\neffectiveness of MAS is critically dependent on its collaboration topology, which has become a focal\npoint for automated design research. However, existing approaches are fundamentally constrained by\n‘their reliance on a template graph modification paradigm with a predefined set of agents and hard-\ncoded interaction structures, significantly limiting their adaptability to task-specific requirements.\nTo address these limitations, we reframe MAS design as a conditional autoregressive graph generation\n‘task, where both the system composition and structure are designed jointly. We propose ARG-Designer, a\nnovel autoregressive model that operationalizes this paradigm by constructing the collaboration graph\nfrom scratch. Conditioned on a natural language task query, ARG-Designer sequentially and dynamically\ndetermines the required number of agents, selects their appropriate roles from an extensible pool, and\nestablishes the optimal communication links between them. This generative approach creates a\ncustomized topology in a flexible and extensible manner, precisely tailored to the unique demands of\ndifferent tasks. Extensive experiments across six diverse benchmarks demonstrate that ARG-Designer not\nonly achieves state-of-the-art performance but also enjoys significantly greater token efficiency and\nenhanced extensibility. The source code of ARG-Designer is available at this https URL.\n\nSubmission history\n\nFrom: Shiyuan Li [view email][v1] Thu, 24 Jul 2025 09:17:41 UTC (813 KB)\n\n[v2] Sat, 26 Jul 2025 01:49:31 UTC (813 KB)",
    "authors": "Li; Shiyuan; Yixin; Wen; Qingsong; Zhang; Chengqi; Pan; Shirui",
    "date": ""
  },
  {
    "url": "http://arxiv.org/abs/2507.18212v1",
    "title": "Prune&Comp: Free Lunch for Layer-Pruned LLMs via Iterative Pruning with Magnitude Compensation",
    "abstract": "Layer pruning has emerged as a promising technique for\ncompressing large language models (LLMs) while achieving acceleration proportional to the pruning\nratio. In this work, we identify that removing any layer induces a significant magnitude gap in hidden\nstates, resulting in substantial performance degradation. To address this issue, we propose\nPrune&Comp, a novel plug-and-play layer pruning scheme that leverages magnitude compensation to\nmitigate such gaps in a training-free manner. Specifically, we first estimate the magnitude gap caused\nby layer removal and then eliminate this gap by rescaling the remaining weights offline, with zero\nruntime overhead incurred. We further demonstrate the advantages of Prune&Comp through an iterative\npruning strategy. When integrated with an iterative prune-and-compensate loop, Prune&Comp consistently\nenhances existing layer pruning metrics. For instance, when 5 layers of LLaMA-3-8B are pruned using\n‘the prevalent block influence metric, Prune&Comp nearly halves the perplexity and retains 93.19\\% of\n‘the original model's question-answering performance, outperforming the baseline by 4.01%.",
    "authors": "Chen; Xinrui; Zhang; Hongxing; Zeng; Fanyi; Wei; Yongxian; Wang; Yizhi; Ling; Xitong; Li; Guanghao; Yuan; Chun",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18203v1",
    "title": "Exploring the Impact of Instruction-Tuning on LLM's Susceptibility to Misinformation",
    "abstract": "Instruction-tuning enhances the ability of large language models\n(LLMs) to follow user instructions more accurately, improving usability while reducing harmful\noutputs. However, this process may increase the model's dependence on user input, potentially leading\nto the unfiltered acceptance of misinformation and the generation of hallucinations. Existing studies\nprimarily highlight that LLMs are receptive to external information that contradict their parametric\nknowledge, but little research has been conducted on the direct impact of instruction-tuning on this\nphenomenon. In our study, we investigate the impact of instruction-tuning on LLM's susceptibility to\nmisinformation. Our analysis reveals that instruction-tuned LLMs are significantly more likely to\naccept misinformation when it is presented by the user. A comparison with base models shows that\ninstruction-tuning increases reliance on user-provided information, shifting susceptibility from the\nassistant role to the user role. Furthermore, we explore additional factors influencing misinformation\nsusceptibility, such as the role of the user in prompt structure, misinformation length, and the\npresence of warnings in the system prompt. Our findings underscore the need for systematic approaches\n‘to mitigate unintended consequences of instruction-tuning and enhance the reliability of LLMs in real-\nworld applications.",
    "authors": "Han; Kyubeen; Jang; Junseo; Kim; Hongjin; Jeong; Geunyeong; Harksoo",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18202v1",
    "title": "Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token Probability Method for Poisoned Document Detection",
    "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language\nModels (LLMs) by providing external knowledge for accurate and up-to-date responses. However, this\nreliance on external sources exposes a security risk, attackers can inject poisoned documents into the\nknowledge base to steer the generation process toward harmful or misleading outputs. In this paper, we\npropose Gradient-based Masked Token Probability (GHTP), a novel defense method to detect and filter\nout adversarially crafted documents. Specifically, GHTP identifies high-impact tokens by examining\ngradients of the retrievers similarity function. These key tokens are then masked, and their\nprobabilities are checked via a Masked Language Model (MLM). Since injected tokens typically exhibit\nmarkedly low masked-token probabilities, this enables GTP to easily detect malicious documents and\nachieve high-precision filtering. Experiments demonstrate that GITP is able to eliminate over 90% of\npoisoned content while retaining relevant documents, thus maintaining robust retrieval and generation\nperformance across diverse datasets and adversarial settings.",
    "authors": "Kim; San; Jonghwi; Jeon; Yejin; Lee; Gary Geunbae",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18197v1",
    "title": "Integrating an ISO30401-compliant Knowledge management system with existing business processes of an organization",
    "abstract": "Business process modeling is used by most organizations as an essential framework for\n‘ensuring efficiency and effectiveness of the work and workflow performed by its employees and for\nensuring the alignment of such work with its strategic goals. For organizations that are compliant or\nnear-compliant with ISO 9001, this approach involves the detailed mapping of processes, sub-processes,\nactivities, and tasks. 15030401 is a Management System Standard, introduced in 2018, establishing\nuniversal requirements for the set up of a Knowledge Management System in an organization. As\n*\"15030401 implementers'' we regularly face the challenge of explaining our clients how the knowledge\ndevelopment, transformation and conveyances activities depicted in IS0304@1 do integrate with existing\noperational processes. This article recaps process modelling principles in the context of 1509001 and\nexplores, based on our experience, how an 15030401-compliant Knowledge Management System (KMS)\nentuines with all other processes of an Integrated Management System and in particular how it can be\nimplemented by deploying the mechanisms of the SECI model through the steps of PDCA cycles.\n‘Submission history\n\nFrom: Patrick PRIEUR [view email] [via CCSD proxy][v1] Thu, 24 Jul 2025 08:54:19 UTC (1,720 KB)",
    "authors": "Belloni; Aline; Prieur; Patrick",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18190v2",
    "title": "TN-AutoRCA: Benchmark Construction and Agentic Framework for Self-Improving Alarm-Based Root Cause Analysis in Telecommunication Networks",
    "abstract": "Root Cause Analysis (RCA) in teleconmunication networks is a critical task, yet it\npresents a formidable challenge for Artificial Intelligence (AI) due to its complex, graph-based\nreasoning requirements and the scarcity of realistic benchmarks.\n\nSubmission history\n\nFrom: Qianjin Yu [view email][v1] Thu, 24 Jul 2025 08:40:08 UTC (1,532 KB)\n\n[v2] Mon, 28 Jul 2025 12:33:37 UTC (1,532 KB)",
    "authors": "Wu; Keyu; Yu; Qianjin; Mei; Manlin; Liu; Ruiting; Wang; Jun; Zhang; Kailai; Bao; Yelun",
    "date": ""
  },
  {
    "url": "http://arxiv.org/abs/2507.18182v1",
    "title": "SCOPE: Stochastic and Counterbiased Option Placement for Evaluating Large Language Models",
    "abstract": "Large Language Models (LLMs) can achieve inflated scores on\nmultiple-choice tasks by exploiting inherent biases in option positions or labels, rather than\ndemonstrating genuine understanding. This study introduces SCOPE, an evaluation framework designed to\nmeasure and mitigate such selection bias in a dataset-independent manner. By repeatedly invoking a\nnull prompt that lacks semantic content, SCOPE estimates each model's unique position-bias\ndistribution. It then redistributes the answer slot according to the inverse-bias distribution,\n‘thereby equalizing the lucky-rate, the probability of selecting the correct answer by chance.\nFurthermore, it prevents semantically similar distractors from being placed adjacent to the answer,\n‘thereby blocking near-miss guesses based on superficial proximity cues. Across multiple benchmark\nexperiments, SCOPE consistently outperformed existing debiasing methods in terms of stable performance\nimprovements and showed clearer confidence distributions over correct options. This framework thus\noffers a new standard for enhancing the fairness and reliability of LLM evaluations.",
    "authors": "Jeong; Wonjun; Kim; Dongseok; Whangbo; Taegkeun",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18171v1",
    "title": "Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models",
    "abstract": "Despite the widespread use of Transformer-based text embedding models in NLP tasks,\nsurprising ‘sticky tokens’ can undermine the reliability of embeddings. These tokens, when repeatedly\ninserted into sentences, pull sentence similarity toward a certain value, disrupting the normal\ndistribution of embedding distances and degrading downstream performance. In this paper, we\nsystematically investigate such anomalous tokens, formally defining them and introducing an efficient\ndetection method, Sticky Token Detector (STD), based on sentence and token filtering. Applying STD to\n40 checkpoints across 14 model families, we discover a total of 868 sticky tokens. Our analysis\nreveals that these tokens often originate from special or unused entries in the vocabulary, as well as\nfragmented subwords from multilingual corpora. Notably, their presence does not strictly correlate\nwith model size or vocabulary size. We further evaluate how sticky tokens affect downstream tasks like\nclustering and retrieval, observing significant performance drops of up to 50%. Through attention-\nlayer analysis, we show that sticky tokens disproportionately dominate the model's internal\nrepresentations, raising concerns about tokenization robustness. Qur findings show the need for better\ntokenization strategies and model design to mitigate the impact of sticky tokens in future text\n‘embedding applications.",
    "authors": "Chen; Kexin; Wang; Dongxia; Liu; Yi; Zhang; Haonan; Wenhai",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18161v1",
    "title": "Recent Trends in Distant Conversational Speech Recognition: A Review of CHiME-7 and 8 DASR Challenges",
    "abstract": "",
    "authors": "Cornell; Samuele; Boeddeker; Christoph; Park; Taejin; Huang; He; Raj; Desh; Wiesner; Matthew; Masuyama; Yoshiki; Chang; Xuankai; Wang; Zhong-Qiu; Squartini; Stefano; Garcia; Paola; Watanabe; Shinji",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18143v2",
    "title": "HIVMedQA: Benchmarking large language models for HIV medical decision support",
    "abstract": "",
    "authors": "Cardenal-Antolin; Gonzalo; Fellay; Jacques; Jaha; Bashkim; Kouyos; Roger; Beerenwinkel; Niko; Duroux; Diane",
    "date": ""
  },
  {
    "url": "http://arxiv.org/abs/2507.18140v1",
    "title": "MathOPEval: A Fine-grained Evaluation Benchmark for Visual Operations of MLLMs in Mathematical Reasoning",
    "abstract": "Recent progress in Multi-modal Large Language Models (MLLMs) has\nenabled step-by-step multi-modal mathematical reasoning by performing visual operations based on the\n‘textual instructions. A promising approach uses code as an intermediate representation to precisely\n‘express and manipulate the images in the reasoning steps. However, existing evaluations focus mainly\n‘on text-only reasoning outputs, leaving the MLLM's ability to perform accurate visual operations via\ncode largely unexplored. This work takes a first step toward addressing that gap by evaluating MLLM's\ncode-based capabilities in multi-modal mathematical this http URL, our framework focuses on two key\nevaluation aspects: (1) Multi-modal Code Generation (MCG) evaluates the model's ability to accurately\nunderstand and construct visualizations from scratch. (2) Multi-modal Code Editing (NCE) assesses the\nmodel's capacity for fine-grained operations, which include three types: Deletion, Modification and\nAnnotation. To evaluate the above tasks, we incorporate a dataset that covers the five most popular\n‘types of mathematical figures, including geometric diagrams, function plots, and three types of\nstatistical charts, to provide 2 comprehensive and effective measurement of existing MLLMs. Our\nexperimental evaluation involves nine mainstream MLLMs, and the results reveal that existing models\nstill lag significantly behind human performance in performing fine-grained visual operations.",
    "authors": "Li; Xiaoyuan; Moxin; Wang; Wenjie; Men; Rui; Zhang; Yichang; Feng; Fuli; Dayiheng; Junyang",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18123v1",
    "title": "Actively evaluating and learning the distinctions that matter: Vaccine safety signal detection from emergency triage notes",
    "abstract": "The rapid development of COVID-19 vaccines has showcased the\nglobal communitys ability to combat infectious diseases. However, the need for post-licensure\nSurveillance systems has grown due to the limited window for safety data collection in clinical trials\nand early widespread implementation. This study aims to employ Natural Language Processing techniques\nand Active Learning to rapidly develop a classifier that detects potential vaccine safety issues from\n‘emergency department notes. ED triage notes, containing expert, succinct vital patient information at\n‘the point of entry to health systems, can significantly contribute to timely vaccine safety signal\nsurveillance. While keyword-based classification can be effective, it may yield false positives and\ndemand extensive keyword modifications. This is exacerbated by the infrequency of vaccination-related\nED presentations and their similarity to other reasons for ED visits. NLP offers a more accurate and\nefficient alternative, albeit requiring annotated data, which is often scarce in the medical field.\nActive learning optimizes the annotation process and the quality of annotated data, which can result\nin faster model implementation and improved model performance. This work combines active learning,\ndata augmentation, and active learning and evaluation techniques to create a classifier that is used\nto enhance vaccine safety surveillance from ED triage notes.",
    "authors": "Khademi; Sedigh; Palmer; Christopher; Javed; Muhammad; Clothier; Hazel; Buttery; Jim; Dimaguila; Gerardo Luis; Black",
    "date": "24 Jul 2025"
  },
  {
    "url": "http://arxiv.org/abs/2507.18119v2",
    "title": "GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic Awareness",
    "abstract": "Recent advances in end-to-end spoken language models (SLMs) have\nsignificantly improved the ability of AI systems to engage in natural spoken interactions. However,\nmost existing models treat speech merely as a vehicle for linguistic content, often overlooking the\nrich paralinguistic and speaker characteristic cues embedded in human speech, such as dialect, age,\nemotion, and non-speech vocalizations. In this work, we introduce GOAT-SLM, a novel spoken language\nmodel with paralinguistic and speaker characteristic awareness, designed to extend spoken language\nmodeling beyond text semantics. GOAT-SLM adopts a dual-modality head architecture that decouples\nLinguistic modeling from acoustic realization, enabling robust language understanding while supporting\n‘expressive and adaptive speech generation. To enhance model efficiency and versatility, we propose @\nmodular, staged training strategy that progressively aligns linguistic, paralinguistic, and speaker\ncharacteristic information using large-scale speech-text corpora. Experimental results on TELEVAL, a\nmulti-dimensional evaluation benchmark, demonstrate that GOAT-SLM achieves well-balanced performance\nacross both semantic and non-semantic tasks, and outperforms existing open-source models in handling\nemotion, dialectal variation, and age-sensitive interactions. This work highlights the importance of\nmodeling beyond linguistic content and advances the development of more natural, adaptive, and\nsocially aware spoken language systems.\n\nSubmission history\n\nFrom: Zehan Li [view email][vi] Thu, 24 Jul 2025 06:10:29 UTC (1,836 KB)\n\n[v2] Fri, 25 Jul 2025 08:25:27 UTC (1,836 KB)\n\nCurrent browse context:\n\ncs.cL",
    "authors": "Chen; Hongjie; Li; Zehan; Song; Yaodong; Deng; Wenming; Yao; Yitong; Zhang; Yuxin; Lv; Hang; Zhu; Xuechao; Kang; Jian; Jie; Wang; Chao; Shuangyong; Yongxiang; He; Zhongjiang; Xuelong",
    "date": ""
  },
  {
    "url": "http://arxiv.org/abs/2507.18115v1",
    "title": "Agentic AI framework for End-to-End Medical Data Inference",
    "abstract": "",
    "authors": "Shimgekar; Soorya Ram; Vassef; Shayan; Goyal; Abhay; Kumar; Navin; Saha; Koustuv",
    "date": "24 Jul 2025"
  }
]