This is a paragraph with HTML tags.
The quick brown fox jumps over the lazy dog.
Contact: [EMAIL], Phone: [PHONE], Card: [CARD]
This is important, important, important, important.
Nearly all human work is collaborative; thus, the evaluation of
real-world NLP applications often requires multiple dimensions that align with diverse human
perspectives. As real human evaluator resources are often scarce and costly, the emerging "LLM-as-a-
Judge” paradigm sheds light on a promising approach to leverage LLM agents to believably simulate
human evaluators. Yet, to date, existing LLM-as-a-judge approaches face two limitations: persona
descriptions of agents are often arbitrarily designed, and the frameworks are not generalizable to
other tasks. To address these challenges, we propose MAJ-EVAL, a Multi-Agent-as-Judge evaluation
framework that can automatically construct multiple evaluator personas with distinct dimensions from
relevant text documents (e.g., research papers), instantiate LLM agents with the personas, and engage
in-group debates with multi-agents to Generate multi-dimensional feedback. Our evaluation experiments
in both the educational and medical domains demonstrate that MAJ-EVAL can generate evaluation results
‘that better align with human experts’ ratings compared with conventional automated evaluation metrics
and existing LLM-as-a~judge methods.
This study investigates the mechanisms and factors influencing
memorization in fine-tuned large language models (LLMs), with a focus on the medical domain due to its
privacy-sensitive nature. We examine how different aspects of the fine-tuning process affect a model's
propensity to memorize training data, using the PHEE dataset of pharmacovigilance events.
Our research employs two main approaches: a membership inference attack to detect memorized data, and
‘a generation task with prompted prefixes to assess verbatim reproduction. We analyze the impact of
adapting different weight matrices in the transformer architecture, the relationship between
perplexity and memorization, and the effect of increasing the rank in low-rank adaptation (LORA) fine-
Key findings include: (1) Value and Output matrices contribute more significantly to memorization
compared to Query and Key matrices; (2) Lower perplexity in the fine-tuned model correlates with
increased memorization; (3) Higher LoRA ranks lead to increased memorization, but with diminishing
returns at higher ranks.
These results provide insights into the trade-offs between model performance and privacy risks in
fine-tuned LLMs. Our findings have implications for developing more effective and responsible
strategies for adapting large language models while managing data privacy concerns.
benefit substantially from chain-of-thought (CoT) reasoning, yet pushing their performance typically
‘tuning (PEFT) helps reduce cost, most existing approaches primarily address domain adaptation or
layer-wise allocation rather than explicitly tailoring data and parameters to different response
demands. Inspired by "Thinking, Fast and Slow,” which characterizes two distinct modes of thought-
system 1 (fast, intuitive, often automatic) and System 2 (slower, more deliberative and analytic)-we
draw an analogy that different “subregions” of an LLM's parameters might similarly specialize for
‘tasks that demand quick, intuitive responses versus those requiring multi-step logical reasoning.
‘Therefore, we propose LORA-PAR, a dual-system LoRA framework that partitions both data and parameters
by System 1 or System 2 demands, using fewer yet more focused parameters for each task. Specifically,
we classify task data via multi-model role-playing and voting, and partition parameters based on
importance scoring, then adopt a two-stage fine-tuning strategy of training System 1 tasks with
supervised fine-tuning (SFT) to enhance knowledge and intuition and refine System 2 tasks with
reinforcement learning (RL) to reinforce deeper logical deliberation next. Extensive experiments show
‘that the tuo-stage fine-tuning strategy, SFT and RL, lowers active parameter usage while matching or
In finance, Large Language Models (LLMs) face frequent knowledge
conflicts due to discrepancies between pre-trained parametric knowledge and real-time market data.
‘These conflicts become particularly problematic when LLMs are deployed in real-world investment
services, where misalignment between a model's embedded preferences and those of the financial
institution can lead to unreliable recommendations. Yet little research has examined what investment
views LLMs actually hold. We propose an experimental framework to investigate such conflicts, offering
‘the first quantitative analysis of confirmation bias in LLM-based investment analysis. Using
hypothetical scenarios with balanced and imbalanced arguments, we extract models’ latent preferences
and measure their persistence. Focusing on sector, size, and momentum, our analysis reveals distinct,
model-specific tendencies. In particular, we observe a consistent preference for large-cap stocks and
contrarian strategies across most models. These preferences often harden into confirmation bias, with
models clinging to initial judgments despite counter-evidence.
Current browse context:
Instruction-tuning large language models (LLMs) reduces the
diversity of their outputs, which has implications for many tasks, particularly for creative tasks.
This paper investigates the ~“diversity gap'' for a writing prompt narrative generation task. This gap
emerges as measured by current diversity metrics for various open-weight and open-source LLMs. The
results show significant decreases in diversity due to instruction-tuning. We explore the diversity
loss at each fine-tuning stage for the OLMo and OLMo 2 models to further understand how output
diversity is affected. The results indicate that DPO has the most substantial impact on diversity.
Motivated by these findings, we present a new decoding strategy, conformative decoding, which guides
an instruct model using its more diverse base model to reintroduce output diversity. We show that
conformative decoding typically increases diversity and even maintains or improves quality.
adopting diverse personas. In this study, we examine how assigning a persona influences a model's
reasoning on an objective task. Using activation patching, we take a first step toward understanding
how key components of the model encode persona-specific information. Our findings reveal that the
early Multi-Layer Perceptron (MLP) layers attend not only to the syntactic structure of the input but
also process its semantic content. These layers transform persona tokens into richer representations,
which are then used by the middle Multi-Head Attention (MHA) layers to shape the model's output.
Additionally, we identify specific attention heads that disproportionately attend to racial and color-
based identities.
Hallucinations in large language models pose a critical challenge for applications
requiring factual reliability, particularly in high-stakes domains such as finance. This work presents
an effective approach for detecting and editing factually incorrect content in model-generated
responses based on the provided context. Given a user-defined domain-specific error taxonomy, we
construct a synthetic dataset by inserting tagged errors into financial question-answering corpora and
‘then fine-tune four language models, Phi-4, Phi-4-mini, Quen3-48, and Quen3-148, to detect and edit
‘these factual inaccuracies. Our best-performing model, fine-tuned Phi-4, achieves an 8% improvement in
binary F1 score and a 30% gain in overall detection performance compared to OpenAl-o3. Notably, our
fine-tuned Phi-4-mini model, despite having only 4 billion parameters, maintains competitive
performance with just a 2% drop in binary detection and a 0.1% decline in overall detection compared
‘to OpenAI-o3. Our work provides a practical solution for detecting and editing factual inconsistencies
in financial text generation while introducing a generalizable framework that can enhance the
‘trustworthiness and alignment of large language models across diverse applications beyond finance. Our
code and data are available at this https URL.
This work introduces MediQAl, a French medical question answering
dataset designed to evaluate the capabilities of language models in factual medical recall and
reasoning over real-world clinical scenarios. MediQAl contains 32,603 questions sourced from French
medical examinations across 41 medical subjects. The dataset includes three tasks: (i) Multiple-Choice
Question with Unique answer, (ii) Multiple-Choice Question with Multiple answer, and (iii) Open-Ended
Question with Short-Answer. Each question is labeled as Understanding or Reasoning, enabling a
detailed analysis of models’ cognitive capabilities. We validate the MediQAl dataset through extensive
evaluation with 14 large language models, including recent reasoning-augmented models, and observe a
significant performance gap between factual recall and reasoning tasks. Our evaluation provides a
comprehensive benchmark for assessing language models’ performance on French medical question
answering, addressing @ crucial gap in multilingual resources for the medical domain.
In-Context Learning (ICL) enables Large Language Models (LLMs) to
perform tasks by conditioning on input-output examples in the prompt, without requiring any update in
model parameters. While widely adopted, it remains unclear whether prompting with multiple examples is
‘the most effective and efficient way to convey task information. In this work, we propose Soft
Injection of task embeddings. The task embeddings are constructed only once using few-shot ICL prompts
and repeatedly used during inference. Soft injection is performed by softly mixing task embeddings
with attention head activations using pre-optimized mixing parameters, referred to as soft head-
selection parameters. This method not only allows a desired task to be performed without in-prompt
demonstrations but also significantly outperforms existing ICL approaches while reducing memory usage
‘and compute cost at inference time. An extensive evaluation is performed across 57 tasks and 12 LLMs,
spanning four model families of sizes from 48 to 708. Averaged across 57 tasks, our method outperforms
1-shot ICL by 10.1%-13.9% across 12 LLMs. Additional analyses show that our method also serves as an
insightful tool for analyzing task-relevant roles of attention heads, revealing that task-relevant
head positions selected by our method transfer across similar tasks but not across dissimilar ones --
underscoring the task-specific nature of head functionality. Our soft injection method opens a new
paradigm for reducing prompt length and improving task performance by shifting task conditioning from
‘the prompt space to the activation space.
Ing2LaTex is a practically significant task that involves
converting mathematical expressions or tabular data from images into LaTeX code. In recent years,
vision-language models (VLMs) have demonstrated strong performance across a variety of visual
understanding tasks, owing to their generalization capabilities. lihile some studies have explored the
use of Vis for the ImgLaTeX task, their performance often falls short of expectations. Empirically,
VUis sometimes struggle with fine-grained visual elements, leading to inaccurate LaTeX predictions. To
address this challenge, we propose $A°2R"2$: Advancing Img2LaTeX Conversion via Visual Reasoning with
Attention-Guided Refinement, a framework that effectively integrates attention localization and
iterative refinement within 2 visual reasoning framework, enabling VLMs to perform self-correction and
progressively improve prediction quality. For effective evaluation, we introduce a new dataset,
Img2LaTex-Hard-1K, consisting of 1,100 carefully curated and challenging examples designed to
rigorously evaluate the capabilities of VMs within this task domain. Extensive experimental results
demonstrate that: (1) $A°2R"2$ significantly improves model performance across six evaluation metrics
spanning both textual and visual levels, consistently outperforming other baseline methods; (2)
Increasing the number of inference rounds yields notable performance gains, underscoring the potential
of $A*2R°2$ in test-time scaling scenarios; (3) Ablation studies and human evaluations validate the
practical effectiveness of our approach, as well as the strong synergy among its core components
during inference.
Project-specific code completion is a critical task that
leverages context from a project to generate accurate code. State-of-the-art methods use retrieval-
augmented generation (RAG) with large language models (LLMs) and project information for code
completion. However, they often struggle to incorporate internal API information, which is crucial for
accuracy, especially when APIs are not explicitly imported in the file.
To address this, we propose a method to infer internal API information without relying on imports. Our
method extends the representation of APIs by constructing usage examples and semantic descriptions,
building a knowledge base for LLMs to generate relevant completions. We also introduce Projgench, &
benchmark that avoids leaked imports and consists of large-scale real-world projects.
Experiments on ProjBench and CrossCodetval show that our approach significantly outperforms existing
methods, improving code exact match by 22.72% and identifier exact match by 18.31%. Additionally,
integrating our method with existing baselines boosts code match by 47.80% and identifier match by
Non-manual facial features play @ crucial role in sign language
communication, yet their importance in automatic sign language recognition (ASLR) remains
underexplored. While prior studies have shown that incorporating facial features can improve
recognition, related work often relies on hand-crafted feature extraction and fails to go beyond the
comparison of manual features versus the combination of manual and facial features. In this work, we
systematically investigate the contribution of distinct facial regionseyes, mouth, and full faceusing
two different deep learning models (a CNN-based model and a transformer-based model) trained on an SLR
dataset of isolated signs with randomly selected classes. Through quantitative performance and
qualitative saliency map evaluation, we reveal that the mouth is the most important non-manual facial
feature, significantly improving accuracy. Our findings highlight the necessity of incorporating
facial features in ASLR.
Medical reports contain rich clinical information but are often
unstructured and written in domain-specific language, posing challenges for information extraction.
While proprietary large language models (LLMs) have shown promise in clinical natural language
processing, their lack of transparency and data privacy concerns limit their utility in healthcare.
This study therefore evaluates nine open-source generative LLMs on the DRAGON benchmark, which
includes 28 clinical information extraction tasks in Dutch. We developed \texttt{1lm\_extractinator},
a publicly available framework for information extraction using open-source generative LLMs, and used
it to assess model performance in a zero-shot setting. Several 14 billion parameter models, Phi-4-148,
Quen-2.5-148, and DeepSeek-R1-148, achieved competitive results, while the bigger Llama-3.3-708 model
achieved slightly higher performance at greater computational cost. Translation to English prior to
inference consistently degraded performance, highlighting the need of native-language processing.
‘These findings demonstrate that open-source LLMs, when used with our framework, offer effective,
scalable, and privacy-conscious solutions for clinical information extraction in low-resource
The concept of diversity has received increased consideration in
Natural Language Processing (NLP) in recent years. This is due to various motivations like promoting
and inclusion, approximating human linguistic behavior, and increasing systems’ performance. Diversity
has however often been addressed in an ad hoc manner in NLP, and with few explicit links to other
domains where this notion is better theorized. We survey articles in the ACL Anthology from the past 6
years, with “diversity” or “diverse” in their title. We find a wide range of settings in which
diversity is quantified, often highly specialized and using inconsistent terminology. We put forward a
unified taxonomy of why, what on, where, and how diversity is measured in NLP. Diversity measures are
cast upon a unified framework from ecology and economy (Stirling, 2007) with 3 dimensions of
diversity: variety, balance and disparity. We discuss the trends which emerge due to this systematized
approach. We believe that this study paves the way towards a better formalization of diversity in NLP,
which should bring a better understanding of this notion and a better comparability between various
approaches.
Large language models (LLMs) are increasingly integrated into
users’ daily lives, leading to @ growing demand for personalized outputs. Previous work focuses on
leveraging a user’s own history, overlooking inter-user differences that are crucial for effective
personalization. While recent work has attempted to model such differences, the reliance on language-
based prompts often hampers the effective extraction of meaningful distinctions. To address these
issues, we propose Difference-auare Embedding-based Personalization (DEP), 2 framework that models
inter-user differences in the latent space instead of relying on language prompts. DEP constructs soft
prompts by contrasting a user's embedding with those of peers who engaged with similar content,
highlighting relative behavioral signals. A sparse autoencoder then filters and compresses both user-
specific and difference-aware embeddings, preserving only task-relevant features before injecting them
into a frozen LLM. Experiments on personalized review generation show that DEP consistently
‘outperforms baseline methods across multiple metrics. Our code is available at this https URL.
Prevention of Future Deaths (PFD) reports, issued by coroners in England and Wales,
flag systemic hazards that may lead to further loss of life. Analysis of these reports has previously
been constrained by the manual effort required to identify and code relevant cases. In 2025, the
Office for National Statistics (ONS) published a national thematic review of child-suicide PFD reports
($\leq$ 18 years), identifying 37 cases from January 2015 to November 2023 - a process based entirely
‘on manual curation and coding. We evaluated whether a fully automated, open source "text-to-table”
Language-model pipeline (PFD Toolkit) could reproduce the ONS's identification and thematic analysis
of child-suicide PFD reports, and assessed gains in efficiency and reliability. All 4,249 PFD reports
published from July 2013 to November 2023 were processed via PFD Toolkit's large language model
pipelines. Automated screening identified cases where the coroner attributed death to suicide in
individuals aged 18 or younger, and eligible reports were coded for recipient category and 23 concern
sub-themes, replicating the ONS coding frame. PFD Toolkit identified 72 child-suicide PFD reports -
almost twice the ONS count. Three blinded clinicians adjudicated a stratified sample of 144 reports to
validate the child-suicide screening. Against the post-consensus clinical annotations, the LLM-based
workflow showed substantial to almost-perfect agreement (Cohen's $\kappa$ = 0.82, 95% CI: 0.66-0.98,
raw agreement = 91%). The end-to-end script runtime was 8m 16s, transforming @ process that previously
‘took months into one that can be completed in minutes. This demonstrates that automated LLM analysis
can reliably and efficiently replicate manual thematic reviews of coronial data, enabling scalable,
reproducible, and timely insights for public health and safety. The PFD Toolkit is openly available
for future research.
Text embeddings have attracted growing interest due to their effectiveness across a
wide range of natural language processing (NLP) tasks, such as retrieval, classification, clustering,
bitext mining, and summarization. With the emergence of pretrained language models (PLMs), general-
purpose text embeddings (GPTE) have gained significant traction for their ability to produce rich,
‘transferable representations. The general architecture of GPTE typically leverages PLIls to derive
dense text representations, which are then optimized through contrastive learning on large-scale
pairwise datasets. In this survey, we provide a comprehensive overview of GPTE in the era of PLMs,
focusing on the roles PLis play in driving its development. We first examine the fundamental
architecture and describe the basic roles of PLMs in GPTE, i.e., embedding extraction, expressivity
enhancement, training strategies, learning objectives, and data construction. Then, we describe
advanced roles enabled by PLMs, such as multilingual support, multimodal integration, code
understanding, and scenario-specific adaptation. Finally, we highlight potential future research
directions that move beyond traditional improvement goals, including ranking integration, safety
considerations, bias mitigation, structural information incorporation, and the cognitive extension of
‘embeddings. This survey aims to serve as a valuable reference for both newcomers and established
researchers seeking to understand the current state and future potential of GPTE.
The growing use of large language models (LLMs) has increased the need for automatic
evaluation systems, particularly to address the challenge of information hallucination. Although
existing faithfulness evaluation approaches have shown promise, they are predominantly English-focused
and often require expensive human-labeled training data for fine-tuning specialized models. As LLMs
see increased adoption in multilingual contexts, there is a need for accurate faithfulness evaluators
‘that can operate across languages without extensive labeled data. This paper presents Self-Taught
Evaluators for Multilingual Faithfulness, @ framework that learns exclusively from synthetic
multilingual summarization data while leveraging cross-lingual transfer learning. Through experiments
comparing language-specific and mixed-language fine-tuning approaches, we demonstrate a consistent
relationship between an LLM's general language capabilities and its performance in language-specific
evaluation tasks. Our framework shows improvements over existing baselines, including state-of-the-art
English evaluators and machine translation-based approaches.
‘their substantial computational and memory requirements pose significant barriers to practical
deployment. Current parameter reduction techniques primarily involve training MLLMs from Small
Language Models (SLMs), but these methods offer limited flexibility and remain computationally
intensive. To address this gap, we propose to directly compress existing MLLMs through structural
pruning combined with efficient recovery training. Specifically, we investigate two structural pruning
paradigms--layerwise and widthwise pruning--applied to the language model backbone of MLLMs, alongside
supervised finetuning and knowledge distillation. Additionally, we assess the feasibility of
conducting recovery training with only a small fraction of the available data. Our results show that
widthwise pruning generally maintains better performance in low-resource scenarios with limited
computational resources or insufficient finetuning data. As for the recovery training, finetuning only
‘the multimodal projector is sufficient at small compression levels (< 20%). Furthermore, a combination
of supervised finetuning and hidden-state distillation yields optimal recovery across various pruning
levels. Notably, effective recovery can be achieved with as little as 5% of the original training
data, while retaining over 95% of the original performance. Through empirical study on tuo
representative LLMs, i.e., LLaVA-v1.5-78 and Bunny-v1.0-38, this study offers actionable insights for
practitioners aiming to compress LLMs effectively without extensive computation resources or
The increasing integration of Visual Language Models (VLMs) into AI systems
necessitates robust model alignment, especially when handling multimodal content that combines text
and images. Existing evaluation datasets heavily lean towards text-only prompts, leaving visual
vulnerabilities under evaluated. To address this gap, we propose \textbf{Text2VLM}, @ novel multi-
stage pipeline that adapts text-only datasets into multimodal formats, specifically designed to
evaluate the resilience of VLMs against typographic prompt injection attacks. The Text2VLM pipeline
identifies harmful content in the original text and converts it into a typographic image, creating a
multimodal prompt for VLMs. Also, our evaluation of open-source VLMs highlights their increased
susceptibility to prompt injection when visual inputs are introduced, revealing critical weaknesses in
‘the current models’ alignment. This is in addition to a significant performance gap compared to
closed-source frontier models. We validate Text2VLM through human evaluations, ensuring the alignment
of extracted salient concepts; text summarization and output classification align with human
‘expectations. Text2VLM provides a scalable tool for comprehensive safety assessment, contributing to
‘the development of more robust safety mechanisms for VLls. By enhancing the evaluation of multimodal
vulnerabilities, Text2ViM plays a role in advancing the safe deployment of VLMs in diverse, real-world
The rapid spread of multilingual misinformation requires robust automated fact
verification systems capable of handling fine-grained veracity assessments across diverse languages.
While large language models have shown remarkable capabilities across many NLP tasks, their
effectiveness for multilingual claim verification with nuanced classification schemes remains
understudied. We conduct a comprehensive evaluation of five state-of-the-art language models on the X-
Fact dataset, which spans 25 languages with seven distinct veracity categories. Our experiments
compare small language models (encoder-based XLM-R and mT5) with recent decoder-only LLMs (Llama 3.1,
Quen 2.5, Mistral Nemo) using both prompting and fine-tuning approaches. Surprisingly, we find that
XLU-R (270M parameters) substantially outperforms all tested LLMs (7-128 parameters), achieving 57.7%
macro-Fl compared to the best LLM performance of 16.9%. This represents @ 15.8% improvement over the
previous state-of-the-art (41.9%), establishing new performance benchmarks for multilingual fact
verification. Our analysis reveals problematic patterns in LLM behavior, including systematic
difficulties in leveraging evidence and pronounced biases toward frequent categories in imbalanced
data settings. These findings suggest that for fine-grained multilingual fact verification, smaller
specialized models may be more effective than general-purpose large models, with important
implications for practical deployment of fact-checking systems.
Submission history
Recent advancements, such as Group Relative Policy Optimization
(GRPO), have enhanced the reasoning capabilities of large language models by optimizing the arithmetic
mean of token-level rewards. However, GRPO suffers from unstable policy updates when processing tokens
with outlier importance-weighted rewards, which manifests as extreme importance sampling ratios during
‘training, i.e., the ratio between the sampling probabilities assigned to a token by the current and
old policies. In this work, we propose Geometric-Mean Policy Optimization (GHPO), a stabilized variant
of GRPO. Instead of optimizing the arithmetic mean, GMPO maximizes the geometric mean of token-level
rewards, which is inherently less sensitive to outliers and maintains a more stable range of
importance sampling ratio. In addition, we provide comprehensive theoretical and experimental analysis
to justify the design and stability benefits of GMPO. Beyond improved stability, GYPO-78 outperforms
GRPO by an average of 4.1% on multiple mathematical benchmarks and 1.4% on multimodal reasoning
benchmark, including AIME24, AMC, MATHS®0, OlympiadBench, Minerva, and Geometry3K. Code is available
at this https URL.
Large Language Models (LLMs) have been extensively adopted in Knowledge Graph
Completion (KGC), showcasing significant research advancements. However, as black-box models driven by
deep neural architectures, current LLi-based KGC methods rely on implicit knowledge representation
with parallel propagation of erroneous knowledge, thereby hindering their ability to produce
conclusive and decisive reasoning outcomes. We aim to integrate neural-perceptual structural
information with ontological knowledge, leveraging the powerful capabilities of LLMs to achieve a
deeper understanding of the intrinsic logic of the knowledge. We propose an ontology enhanced KGC
method using LLMs -- OL-KGC. It first leverages neural perceptual mechanisms to effectively embed
structural information into the textual space, and then uses an automated extraction algorithm to
retrieve ontological knowledge from the knowledge graphs (KGs) that needs to be completed, which is
further transformed into a textual format comprehensible to LLMs for providing logic guidance. We
conducted extensive experiments on three widely-used benchmarks -- FB1SK-237, UMLS and WNIBRR. The
‘experimental results demonstrate that OL-KGC significantly outperforms existing mainstream KGC methods
across multiple evaluation metrics, achieving state-of-the-art performance.
Antisocial behavior (ASB) on social media-including hate speech, harassment, and
trolling-poses growing challenges for platform safety and societal wellbeing. While prior work has
primarily focused on detecting harmful content after it appears, predictive approaches aim to forecast
future harmful behaviors-such as hate speech propagation, conversation derailment, or user recidivism
before they fully unfold. Despite increasing interest, the field remains fragmented, lacking a unified
‘taxonomy or clear synthesis of existing methods. This paper presents @ systematic review of over 49
studies on ASB prediction, offering a structured taxonomy of five core task types: early harm
detection, harm emergence prediction, harm propagation prediction, behavioral risk prediction, and
proactive moderation support. We analyze how these tasks differ by temporal framing, prediction
granularity, and operational goals. In addition, we examine trends in modeling techniques-from
Classical machine learning to pre-trained language models-and assess the influence of dataset
characteristics on task feasibility and generalization. Our review highlights methodological
challenges, such as dataset scarcity, temporal drift, and limited benchmarks, while outlining emerging
research directions including multilingual modeling, cross-platform generalization, and human-in-the-
loop systems. By organizing the field around a coherent framework, this survey aims to guide future
work toward more robust and socially responsible ASB prediction.
From: Anais ollagnier [view email] [via CCSD proxy][v2] Mon, 28 Jul 2025 08:27:58 UTC (1,059 KB)
ble present Z5E-Cap (Zero-Shot Ensemble for Captioning), our 4th place system in
Event-Enriched Image Analysis (EVENTA) shared task on article-grounded image retrieval and captioning.
Our zero-shot approach requires no finetuning on the competition's data. For retrieval, we ensemble
similarity scores from CLIP, SiglIP, and DINOv2. For captioning, we leverage a carefully engineered
prompt to guide the Gemma 3 model, enabling it to link high-level events from the article to the
visual content in the image. Our system achieved a final score of 0.42002, securing a top-4 position
on the private test set, demonstrating the effectiveness of combining foundation models through
‘ensembling and prompting. Our code is available at this https URL.
Large Language Models (LLMs) are widely used to generate plausible text on online
platforms, without revealing the generation process. As users increasingly encounter such black-box
outputs, detecting hallucinations has become a critical challenge. To address this challenge, we focus
‘on developing a hallucination detection framework for black-box generators. Motivated by the
observation that hallucinations, once introduced, tend to persist, we sample future contexts. The
sampled future contexts provide valuable clues for hallucination detection and can be effectively
integrated with various sampling-based methods. We extensively demonstrate performance improvements
across multiple methods using our proposed sampling approach.
We introduce Kimi K2, a Mixture-of-Experts (MoE) large language
model with 32 billion activated parameters and 1 trillion total parameters. We propose the MuonClip
optimizer, which improves upon Muon with a novel Qk-clip technique to address training instability
while enjoying the advanced token efficiency of Muon. Based on MuonClip, K2 was pre-trained on 15.5
‘trillion tokens with zero loss spike. During post-training, K2 undergoes a multi-stage post-training
process, highlighted by a large-scale agentic data synthesis pipeline and a joint reinforcement
learning (RL) stage, where the model improves its capabilities through interactions with real and
synthetic environments.
Kimi K2 achieves state-of-the-art performance among open-source non-thinking models, with strengths in
agentic capabilities. Notably, K2 obtains 66.1 on Tau2-Bench, 76.5 on ACEBench (En), 65.8 on SWE-Bench
Verified, and 47.3 on SWE-Bench Multilingual -- surpassing most open and closed-sourced baselines in
non-thinking settings. It also exhibits strong capabilities in coding, mathematics, and reasoning
‘tasks, with a score of 53.7 on LiveCodeBench v6, 49.5 on AIME 2025, 75.1 on GPQA-Diamond, and 27.1 on
038ench, all without extended thinking. These results position Kimi K2 as one of the most capable
open-source large language models to date, particularly in software engineering and agentic tasks. We
release our base and post-trained model checkpoints to facilitate future research and applications of
Current browse context
We introduce a novel multi-labeled scheme for joint annotation of hate and counter-
hate speech in social media conversations, categorizing hate and counter-hate messages into thematic
and rhetorical dimensions. The thematic categories outline different discursive aspects of each type
of speech, while the rhetorical dimension captures how hate and counter messages are communicated,
drawing on Aristotle's Logos, Ethos and Pathos. We annotate a sample of 92 conversations, consisting
of 720 tweets, and conduct statistical analyses, incorporating public metrics, to explore patterns of
interaction between the thematic and rhetorical dimensions within and between hate and counter-hate
speech. Our findings provide insights into the spread of hate messages on social media, the strategies
used to counter them, and their potential impact on online behavior.
The demand for Large Language Models (LLMs) capable of sophisticated mathematical
reasoning is growing across industries. However, the development of performant mathematical LLMs is
critically bottlenecked by the scarcity of difficult, novel training data. We introduce \textbf{SAND-
Math} (Synthetic Augmented Novel and Difficult Mathematics problems and solutions), a pipeline that
addresses this by first generating high-quality problems from scratch and then systematically
elevating their complexity via a new \textbf{Difficulty Hiking} step. We demonstrate the effectiveness
‘of our approach through two key findings. First, augmenting a strong baseline with SAND-Math data
significantly boosts performance, outperforming the next-best synthetic dataset by \textbf{$\uparrou$
17.85 absolute points} on the AINE25 benchmark. Second, in a dedicated ablation study, we show our
Difficulty Hiking process is highly effective: by increasing average problem difficulty from 5.02 to
5.98, this step lifts AINE25 performance from 46.38\% to 49.23\%. The full generation pipeline, final
dataset, and a fine-tuned model form a practical and scalable toolkit for building more capable and
efficient mathematical reasoning LLMs. SAND-Math dataset is released here: \href{this https URL}{this
https URL}
Recent advances have enabled LLM-powered AI agents to autonomously execute complex
‘tasks by combining language model reasoning with tools, memory, and web access. But can these systems
be trusted to follow deployment policies in realistic environments, especially under attack? To
investigate, we ran the largest public red-teaming competition to date, targeting 22 frontier AI
agents across 44 realistic deployment scenarios. Participants submitted 1.8 million prompt-injection
attacks, with over 60,000 successfully eliciting policy violations such as unauthorized data access,
illicit financial actions, and regulatory noncompliance. We use these results to build the Agent Red
‘Teaming (ART) benchmark - a curated set of high-impact attacks - and evaluate it across 19 state-of-
‘the-art models. Nearly all agents exhibit policy violations for most behaviors within 10-100 queries,
with high attack transferability across models and tasks. Importantly, we find limited correlation
between agent robustness and model size, capability, or inference-time compute, suggesting that
additional defenses are needed against adversarial misuse. Our findings highlight critical and
persistent vulnerabilities in today's AI agents. By releasing the ART benchmark and accompanying
evaluation framework, we aim to support more rigorous security assessment and drive progress toward
Aquaculture plays a vital role in global food security and coastal economies by
providing sustainable protein sources. As the industry expands to meet rising demand, it faces growing
challenges such as disease outbreaks, inefficient feeding practices, rising labor costs, logistical
inefficiencies, and critical hatchery issues, including high mortality rates and poor water quality
control. Although artificial intelligence has made significant progress, existing machine learning
methods fall short of addressing the domain-specific complexities of aquaculture. To bridge this gap,
we introduce AQUA, the first large language model (LLM) tailored for aquaculture, designed to support
farmers, researchers, and industry practitioners. Central to this effort is AQUADAPT (Data
Acquisition, Processing and Tuning), an Agentic Framework for generating and refining high-quality
synthetic data using a combination of expert knowledge, largescale language models, and automated
evaluation techniques. Our work lays the foundation for LLM-driven innovations in aquaculture
research, advisory systems, and decision-making tools.
Fron: Praneeth Narisetty [view email][v1] Mon, 28 Jul 2025 05:06:07 UTC (689 KB)
A multi-modal guardrail must effectively filter image content based on user-defined
policies, identifying material that may be hateful, reinforce harmful stereotypes, contain explicit
material, or spread misinformation. Deploying such guardrails in real-world applications, however,
poses significant challenges. Users often require varied and highly customizable policies and
‘typically cannot provide abundant examples for each custom policy. Consequently, an ideal guardrail
should be scalable to the multiple policies and adaptable to evolving user standards with minimal
retraining. Existing fine-tuning methods typically condition predictions on pre-defined policies,
restricting their generalizability to new policies or necessitating extensive retraining to adapt.
Conversely, training-free methods struggle with limited context lengths, making it difficult to
incorporate all the policies comprehensively. To overcome these limitations, we propose to condition
model's judgment on “precedents”, which are the reasoning processes of prior data points similar to
‘the given input. By leveraging precedents instead of fixed policies, our approach greatly enhances the
flexibility and adaptability of the guardrail. In this paper, we introduce a critique-revise mechanism
for collecting high-quality precedents and two strategies that utilize precedents for robust
prediction. Experimental results demonstrate that our approach outperforms previous methods across
both few-shot and full-dataset scenarios and exhibits superior generalization to novel policies.
Recent studies have explored various approaches for treating
candidate named entity spans as both source and target sequences in named entity recognition (NER) by
leveraging large language models (LLMs). Although previous approaches have successfully generated
candidate named entity spans with suitable labels, they rely solely on input context information when
using LLMs, particularly, ChatGPT. However, NER inherently requires capturing detailed labeling
requirements with input context information. To address this issue, we propose a novel method that
leverages code-based prompting to improve the capabilities of LLMs in understanding and performing
NER. By embedding code within prompts, we provide detailed B10 schema instructions for labeling,
‘thereby exploiting the ability of LLMs to comprehend long-range scopes in programming languages.
Experimental results demonstrate that the proposed code-based prompting method outperforms
conventional text-based prompting on ten benchmarks across English, Arabic, Finnish, Danish, and
German datasets, indicating the effectiveness of explicitly structuring NER instructions. We also
verify that combining the proposed code-based prompting method with the chain-of-thought prompting
further improves performance.
in image captioning, yet they still lag behind their English counterparts due to limited multilingual
‘training data and costly large-scale model parameterization. Retrieval-augmented generation (RAG)
offers a promising alternative by conditioning caption generation on retrieved examples in the target
Language, reducing the need for extensive multilingual training. However, multilingual RAG captioning
models often depend on retrieved captions translated from English, which can introduce mismatches and
Linguistic biases relative to the source language. We introduce CONCAP, a multilingual image
captioning model that integrates retrieved captions with image-specific concepts, enhancing the
contextualization of the input image and grounding the captioning process across different languages.
Experiments on the X"3600 dataset indicate that CONCAP enables strong performance on low- and mid-
resource languages, with highly reduced data requirements. Our findings highlight the effectiveness of
concept-aware retrieval augmentation in bridging multilingual performance gaps.
Chain-of-Thought (CoT) prompting helps models think step by step.
But what happens when they must see, understand, and judge-all at once? In visual tasks grounded in
social context, where bridging perception with norm-grounded judgments is essential, flat CoT often
breaks down. We introduce Cognitive Chain-of-Thought (CoCoT), @ prompting strategy that scaffolds VLM
reasoning through three cognitively inspired stages: perception, situation, and norm. Our experiments
show that, across multiple multimodal benchmarks (including intent disambiguation, commonsense
reasoning, and safety), CoCoT consistently outperforms CoT and direct prompting (+8\% on average). Our
findings demonstrate that cognitively grounded reasoning stages enhance interpretability and social
awareness in VLMs, paving the way for safer and more reliable multimodal systems.
Large language models (LLMs) have shown remarkable capabilities
across various tasks, that are learned from massive amounts of text-based data. Although LLMs can
control output sequence length, particularly in instruction-based settings, the internal mechanisms
behind this control have been unexplored yet. In this study, we provide empirical evidence on how
output sequence length information is encoded within the internal representations in LLMs. In
particular, our findings show that multi-head attention mechanisms are critical in determining output
sequence length, which can be adjusted in a disentangled manner. By scaling specific hidden units
within the model, we can control the output sequence length without losing the informativeness of the
generated text, thereby indicating that length information is partially disentangled from semantic
information. Moreover, some hidden units become increasingly active as prompts become more length-
specific, thus reflecting the model's internal awareness of this attribute. Our findings suggest that
LLMs have learned robust and adaptable internal mechanisms for controlling output length without any
Recent advancements in Large Language Models (LLMs) have shown
outstanding potential for role-playing applications. Evaluating these capabilities is becoming crucial
yet remains challenging. Existing benchmarks mostly adopt a \textbf{character-centric} approach,
simplify user-character interactions to isolated Q&A tasks, and fail to reflect real-world
applications. To address this limitation, we introduce RMTBench, a comprehensive \textbf{user-centric}
bilingual role-playing benchmark featuring 80 diverse characters and over 8,000 dialogue rounds.
RMTBench includes custom characters with detailed backgrounds and abstract characters defined by
simple traits, enabling evaluation across various user scenarios. Our benchmark constructs dialogues
based on explicit user motivations rather than character descriptions, ensuring alignment with
practical user applications. Furthermore, we construct an authentic multi-turn dialogue simulation
mechanism. With carefully selected evaluation dimensions and LLM-based scoring, this mechanism
captures the complex intention of conversations between the user and the character. By shifting focus
from character background to user intention fulfillment, RMTBench bridges the gap between academic
evaluation and practical deployment requirements, offering a more effective framework for assessing
role-playing capabilities in LLMs. All code and datasets will be released soon.
We present DYNARTmo, @ dynamic articulatory model designed to
visualize speech articulation processes in a two-dimensional midsagittal plane. The model builds upon
‘the UK-DYNANO framework and integrates principles of articulatory underspecification, segmental and
gestural control, and coarticulation. DYNARTmo simulates six key articulators based on ten continuous
and six discrete control parameters, allowing for the generation of both vocalic and consonantal
articulatory configurations. The current implementation is embedded in 2 web-based application
(SpeecharticulationTrainer) that includes sagittal, glottal, and palatal views, making it suitable for
Use in phonetics education and speech therapy. While this paper focuses on the static modeling
aspects, future work will address dynamic movement generation and integration with articulatory-
Language processing (NLP), as most everyday communication in the Arab world occurs in dialects that
diverge significantly from Modern Standard Arabic (HSA). This linguistic divide limits access to
digital services and educational resources and impedes progress in Arabic machine translation. This
paper presents two core contributions to advancing DA-MSA translation for the Levantine, Egyptian, and
Gulf dialects, particularly in low-resource and computationally constrained settings: a comprehensive
evaluation of training-free prompting techniques, and the development of a resource-efficient fine-
‘tuning pipeline. Our evaluation of prompting strategies across six large language models (LLMs) found
‘that few-shot prompting consistently outperformed zero-shot, chain-of-thought, and our proposed Ara-
TEaR method. GPT-4o achieved the highest performance across all prompting settings. For fine-tuning, a
quantized Genma2-98 model achieved 3 CHrF++ score of 49.88, outperforming zero-shot GPT-40 (44.58).
Joint multi-dialect trained models outperformed single-dialect counterparts by over 10% CHrF++, and 4-
bit quantization reduced memory usage by 60% with less than 1% performance loss. The results and
insights of our experiments offer a practical blueprint for improving dialectal inclusion in Arabic
NLP, showing that high-quality DA-MSA machine translation is achievable even with limited resources
and paving the way for more inclusive language technologies.
Scientific research increasingly relies on specialized
computational tools, yet effectively utilizing these tools demands substantial domain expertise. while
Large Language Models (LLMs) show promise in tool automation, they struggle to seamlessly integrate
and orchestrate multiple tools for complex scientific workflows. Here, we present SciToolagent, an
LLM-powered agent that automates hundreds of scientific tools across biology, chemistry, and materials
science. At its core, SciToolagent leverages a scientific tool knowledge graph that enables
intelligent tool selection and execution through graph-based retrieval-augmented generation. The agent
also incorporates a comprehensive safety-checking module to ensure responsible and ethical tool usage.
Extensive evaluations on a curated benchmark demonstrate that SciToolagent significantly outperforms
existing approaches. Case studies in protein engineering, chemical reactivity prediction, chemical
synthesis, and metal-organic framework screening further demonstrate SciToolagent's capability to
automate complex scientific workflows, making advanced research tools accessible to both experts and
‘their internal language processing remains poorly understood. We analyze how Aya-23-88, a decoder-only
LLM trained on balanced multilingual data, handles code-mixed, cloze, and translation tasks compared
‘to predominantly monolingual models like Llama 3 and Chinese-LLaMA-2. Using logit lens and neuron
specialization analyses, we find: (1) Aya-23 activates typologically related language representations
during translation, unlike English-centric models that rely on a single pivot language; (2) code-mixed
neuron activation patterns vary with mixing rates and are shaped more by the base language than the
mixed-in one; and (3) Aya-23"s languagespecific neurons for code-mixed inputs concentrate in final
layers, diverging from prior findings on decoder-only models. Neuron overlap analysis further shows
‘that script similarity and typological relations impact processing across model types. These findings
reveal how multilingual training shapes LLM internals and inform future cross-lingual transfer
research.
‘Submission history
From: Katharina Trinley [view email][v1] Sun, 27 Jul 2025 13:53:45 UTC (12,000 KB)
effectively leveraging sequential environmental feedback (EF) signals, such as natural language
evaluations, for feedback-independent chain-of-thought (CoT) reasoning. Existing approaches either
convert &F into scalar rewards, losing rich contextual information, or employ refinement datasets,
failing to exploit the multi-step and discrete nature of EF interactions. To address these
Limitations, we propose MoL-RL, a novel training paradigm that integrates multi-step EF signals into
LLMs through a dual-objective optimization framework. Our method combines MoL (Mixture-of-Losses)
continual training, which decouples domain-specific EF signals (optimized via cross-entropy loss) and
general language capabilities (preserved via Kullback-Leibler divergence), with GRPO-based post-
‘training to distill sequential EF interactions into single-step inferences. This synergy enables
robust feedback-independent reasoning without relying on external feedback loops. Experimental results
on mathematical reasoning (MATH-500, AIME24/AIME25) and code generation (CodeAgent-Test) benchmarks
demonstrate that MoL-RL achieves state-of-the-art performance with the Quen3-88 model, while
maintaining strong generalization across model scales (Quen3-48). This work provides a promising
approach for leveraging multi-step textual feedback to enhance LLMs’ reasoning capabilities in diverse
Shaping inclusive representations that embrace diversity and
‘ensure fair participation and reflections of values is at the core of many conversation-based models.
However, many existing methods rely on surface inclusion using mention of user demographics or
behavioral attributes of social groups. Such methods overlook the nuanced, implicit expression of
opinion embedded in conversations. Furthermore, the over-reliance on overt cues can exacerbate
misalignment and reinforce harmful or stereotypical representations in model outputs. Thus, we took a
step back and recognized that equitable inclusion needs to account for the implicit expression of
‘opinion and use the stance of responses to validate the normative alignment. This study aims to
evaluate how opinions are represented in NLP or computational models by introducing an alignment
evaluation framework that foregrounds implicit, often overlooked conversations and evaluates the
normative social views and discourse. Our approach models the stance of responses as a proxy for the
underlying opinion, enabling @ considerate and reflective representation of diverse social viewpoints.
We evaluate the framework using both (i) positive-unlabeled (PU) online learning with base
classifiers, and (ii) instruction-tuned language models to assess post-training alignment. Through
‘this, we provide a lens on how implicit opinions are (mis)represented and offer a pathway toward more
inclusive model behavior.
Current language model training paradigms typically terminate
learning upon reaching the end-of-sequence (}) token, overlooking the potential learning opportunities
in the post-completion space. We propose Post-Completion Learning (PCL), a novel training framework
‘that systematically utilizes the sequence space after model output completion, to enhance both the
reasoning and self-evaluation abilities. PCL enables models to continue generating self-assessment
and reward predictions during training, while maintaining efficient inference by stopping at the
To fully utilize this post-completion space, we design a white-box reinforcement learning method: let
‘the model evaluate the output content according to the reward rules, then calculate and align the
score with the reward functions for supervision. We implement dual-track SFT to optimize both
reasoning and evaluation capabilities, and mixed it with RL training to achieve multi-objective hybrid
optimization.
‘traditional SFT and RL methods. Our method provides a new technical path for language model training
‘that enhances output quality while preserving deployment efficiency.
Professionalism is a crucial yet underexplored dimension of
expert communication, particularly in high-stakes domains like finance. This paper investigates how
Linguistic features can be leveraged to model and evaluate professionalism in expert questioning. We
introduce @ novel annotation framework to quantify structural and pragmatic elements in financial
analyst questions, such as discourse regulators, prefaces, and request types. Using both human-
authored and large language model (LLM)-generated questions, we construct two datasets: one annotated
for perceived professionalism and one labeled by question origin. We show that the same linguistic
features correlate strongly with both human judgments and authorship origin, suggesting a shared
stylistic foundation. Furthermore, a classifier trained solely on these interpretable features
outperforms gemini-2.0 and SVM baselines in distinguishing expert-authored questions. Our findings
demonstrate that professionalism is @ learnable, domain-general construct that can be captured through
linguistically grounded modeling.
Recent progress in large language models (LLMs) has opened new
possibilities for mental health support, yet current approaches lack realism in simulating specialized
psychotherapy and fail to capture therapeutic progression over time. Narrative therapy, which helps
individuals transform problematic life stories into empowering alternatives, remains underutilized due
to limited access and social stigma. We address these limitations through a comprehensive framework
with two core components. First, INT (Interactive Narrative Therapist) simulates expert narrative
‘therapists by planning therapeutic stages, guiding reflection levels, and generating contextually
appropriate expert-like responses. Second, IMA (Innovative Moment Assessment) provides a therapy-
centric evaluation method that quantifies effectiveness by tracking "Innovative Moments” (IMs),
critical narrative shifts in client speech signaling therapy progress. Experimental results on 260
simulated clients and 230 human participants reveal that INT consistently outperforms standard LLMs in
‘therapeutic quality and depth. We further demonstrate the effectiveness of INT in synthesizing high-
News recommendation systems play a vital role in mitigating
information overload by delivering personalized news content. A central challenge is to effectively
model both multi-view news representations and the dynamic nature of user interests, which often span
both short- and long-term preferences. Existing methods typically rely on single-view features of news
articles (e.g., titles or categories) or fail to comprehensively capture user preferences across time
scales. In this work, we propose Co-NAML-LSTUR, a hybrid news recommendation framework that integrates
NAML for attentive multi-view news modeling and LSTUR for capturing both long- and short-term user
representations. Our model also incorporates BERT-based word embeddings to enhance semantic feature
extraction. We evaluate Co-NAML-LSTUR on two widely used benchmarks, MIND-small and MIND-large.
Experimental results show that Co-NAML-LSTUR achieves substantial improvements over most state-of-the-
art baselines on MIND-small and MIND-large, respectively. These results demonstrate the effectiveness
of combining multi-view news representations with dual-scale user modeling. The implementation of our
model is publicly available at this https URL.
Current evaluations of large language models (LLMs) rely on
benchmark scores, but it is difficult to interpret what these individual scores reveal about a model's
overall skills. Specifically, as a comunity we lack understanding of how tasks relate to one another,
what they measure in common, how they differ, or which ones are redundant. As a result, models are
often assessed via a single score averaged across benchmarks, an approach that fails to capture the
models’ wholistic strengths and limitations. Here, we propose a new evaluation paradigm that uses
factor analysis to identify latent skills driving performance across benchmarks. We apply this method
‘to a comprehensive new leaderboard showcasing the performance of 69 LLMs on 44 tasks, and identify a
small set of latent skills that largely explain performance. Finally, we turn these insights into
practical tools that identify redundant tasks, aid in model selection, and profile models along each
Large reasoning models (LRM) with long chain-of-thought (CoT)
capabilities have shown strong performance on objective tasks, such as math reasoning and coding.
However, their effectiveness on subjective questions that may have different responses from different
perspectives is still limited by a tendency towards homogeneous reasoning, introduced by the reliance
‘on a single ground truth in supervised fine-tuning and verifiable reward in reinforcement learning.
Motivated by the finding that increasing role perspectives consistently improves performance, we
propose MultiRole-R1, a diversity-enhanced framework with multiple role perspectives, to improve the
accuracy and diversity in subjective reasoning tasks. MultiRole-R1 features an unsupervised data
construction pipeline that generates reasoning chains that incorporate diverse role perspectives. We
further employ reinforcement learning via Group Relative Policy Optimization (GRPO) with reward
shaping, by taking diversity as a reward signal in addition to the verifiable reward. With specially
designed reward functions, we successfully promote perspective diversity and lexical diversity,
uncovering a positive relation between reasoning diversity and accuracy. Our experiment on six
benchmarks demonstrates MultiRole-R1's effectiveness and generalizability in enhancing both subjective
and objective reasoning, showcasing the potential of diversity-enhanced training in LRMS.
Session history is @ common way of recording user interacting
behaviors throughout a browsing activity with multiple products. For example, if an user clicks a
product webpage and then leaves, it might because there are certain features that don't satisfy the
user, which serve as an important indicator of on-the-spot user preferences. However, all prior works
fail to capture and model customer intention effectively because insufficient information exploitation
and only apparent information like descriptions and titles are used. There is also a lack of data and
corresponding benchmark for explicitly modeling intention in E-commerce product purchase sessions. To
address these issues, we introduce the concept of an intention tree and propose a dataset curation
pipeline. Together, we construct a sibling multimodal benchmark, SessionIntentBench, that evaluates
L(V)LMs* capability on understanding inter-session intention shift with four subtasks. With 1,952,177
intention entries, 1,132,145 session intention trajectories, and 13,003,664 available tasks mined
using 10,905 sessions, we provide a scalable way to exploit the existing session data for customer
intention understanding. We conduct human annotations to collect ground-truth label for a subset of
collected data to form an evaluation gold set. Extensive experiments on the annotated data further
confirm that current L(V)LMs fail to capture and utilize the intention across the complex session
setting. Further analysis show injecting intention enhances LLMs’ performances.
Large language models (LLMs), despite their extensive pretraining
on diverse datasets, require effective alignment to human preferences for practical and reliable
deployment. Conventional alignment methods typically employ off-policy learning and depend on human-
annotated datasets, which limits their broad applicability and introduces distribution shift issues
during training. To address these challenges, we propose Self-Generated Preference Optimization based
‘on Self-Improver (SGPO), an innovative alignment framework that leverages an on-policy self-improving
mechanism. Specifically, the improver refines responses from a policy model to self-generate
preference data for direct preference optimization (DPO) of the policy model. Here, the improver and
policy are unified into a single model, and in order to generate higher-quality preference data, this
self-improver learns to make incremental yet discernible improvements to the current responses by
referencing supervised fine-tuning outputs. Experimental results on Alpacafval 2.@ and Arena-Hard show
‘that the proposed SGPO significantly improves performance over DPO and baseline self-improving methods
without using external preference data.
development and evaluation through simulated interactions. While current Large Language Models (LLMs)
have advanced user simulation capabilities, we reveal that they struggle to consistently demonstrate
goal-oriented behavior across multi-turn conversations--a critical limitation that compromises their
reliability in downstream applications. We introduce User Goal State Tracking (UGST), @ novel
framework that tracks user goal progression throughout conversations. Leveraging UGST, we present @
‘three-stage methodology for developing user simulators that can autonomously track goal progression
and reason to generate goal-aligned responses. Moreover, we establish comprehensive evaluation metrics
for measuring goal alignment in user simulators, and demonstrate that our approach yields substantial
improvements across two benchmarks (MultiNOZ 2.4 and {\tau}-Bench). Our contributions address a
critical gap in conversational AI and establish UGST as an essential framework for developing goal-
aligned user simulators.
Reinforcement learning (RL) plays @ crucial role in shaping the
behavior of large language and reasoning models (LLMs/LRMs). However, it often produces brittle and
unstable policies, leading to critical failures such as spurious reasoning, deceptive alignment, and
instruction disobedience that undermine the trustworthiness and safety of LLMs/LRMs. Currently, these
issues lack a unified theoretical explanation and are typically addressed using ad-hoc heuristics.
‘This paper presents a rigorous mathematical framework for analyzing the stability of the mapping from
a reward function to the optimal policy. We show that policy brittleness often stems from non-unique
optimal actions, a common occurrence when multiple valid traces exist in a reasoning task. This
‘theoretical lens provides a unified explanation for a range of seemingly disparate failures, reframing
‘them as rational outcomes of optimizing rewards that may be incomplete or noisy, especially in the
presence of action degeneracy. We extend this analysis from the fundamental single-reward setting to
‘the more realistic multi-reward RL across diverse domains, showing how stability is governed by an
“effective reward” aggregation mechanism. We also prove that entropy regularization restores policy
stability at the cost of increased stochasticity. Our framework provides a unified explanation for
recent empirical findings on deceptive reasoning, instruction-following trade-offs, and RLHF-induced
sophistry, and is further validated through perturbation experiments in multi-reward RL. This work
advances policy-stability analysis from empirical heuristics towards @ principled theory, offering
essential insights for designing safer and more trustworthy AI systems.
Document Understanding (DU) in long-contextual scenarios with
complex layouts remains a significant challenge in vision-language research. Although Large Vision-
Language Models (LVLMs) excel at short-context DU tasks, their performance declines in long-context
settings. A key limitation is the scarcity of fine-grained training data, particularly for low-
resource languages such as Arabic. Existing state-of-the-art techniques rely heavily on human
annotation, which is costly and inefficient. We propose a fully automated, multi-agent interactive
framework to generate long-context questions efficiently. Our approach efficiently generates high-
quality single- and multi-page questions for extensive English and Arabic documents, covering hundreds
of pages across diverse domains. This facilitates the development of LVLMs with enhanced long-context
understanding ability. Experimental results in this work have shown that our generated English and
Arabic questions (\textbf{AaratngLongBench}) are quite challenging to major open- and close-source
LVLMs. The code and data proposed in this work can be found in this https URL. Sample Question and
Answer (QA) pairs and structured system prompts can be found in the Appendix.
This paper presents the technical solution developed by team CRUISE for the KDD Cup
2025 Meta Comprehensive RAG Benchmark for Multi-modal, Multi-turn (CRAG-MH) challenge. The challenge
aims to address a critical limitation of modern Vision Language Models (VLMs): their propensity to
hallucinate, especially when faced with egocentric imagery, long-tail entities, and complex, multi-hop
questions. This issue is particularly problematic in real-world applications where users pose fact-
seeking queries that demand high factual accuracy across diverse modalities. To tackle this, we
propose a robust, multi-stage framework that prioritizes factual accuracy and truthfulness over
completeness. Our solution integrates a lightweight query router for efficiency, a query-aware
retrieval and summarization pipeline, a dual-pathways generation and a post-hoc verification. This
conservative strategy is designed to minimize hallucinations, which incur a severe penalty in the
competition's scoring metric. Our approach achieved 3rd place in Task 1, demonstrating the
effectiveness of prioritizing answer reliability in complex multi-modal RAG systems. Our
implementation is available at this https URL .
Generative AI can now synthesize strikingly realistic images from
‘text, yet output quality remains highly sensitive to how prompts are phrased. Direct Preference
Optimization (DPO) offers a lightweight, off-policy alternative to RL for automatic prompt
engineering, but its token-level regularization leaves semantic inconsistency unchecked as prompts
‘that win higher preference scores can still drift away from the user's intended meaning.
We introduce Sem-DPO, 2 variant of DPO that preserves semantic consistency yet retains its simplicity
and efficiency. Sem-DPO scales the DPO loss by an exponential weight proportional to the cosine
distance between the original prompt and winning candidate in embedding space, softly down-weighting
‘training signals that would otherwise reward semantically mismatched prompts. We provide the first
analytical bound on semantic drift for preference-tuned prompt generators, showing that Sem-DPO keeps
learned prompts within a provably bounded neighborhood of the original text. On three standard text-
‘to-image prompt-optimization benchmarks and two language models, Sem-DPO achieves 8-12% higher CLIP
similarity and 5-9% higher human-preference scores (HPSv2.1, PickScore) than DPO, while also
outperforming state-of-the-art baselines. These findings suggest that strong flat baselines augmented
with semantic weighting should become the new standard for prompt-optimization studies and lay the
groundwork for broader, semantics-aware preference optimization in language models.
From: Azal Ahmad Khan [view email][v1] Sun, 27 Jul 2025 05:20:13 UTC (10,692 KB)
Preserving ancient languages is essential for understanding
humanity's cultural and linguistic heritage, yet Old English remains critically under-resourced,
Limiting its accessibility to modern natural language processing (NLP) techniques. We present a
scalable framework that uses advanced large language models (LLMs) to generate high-quality Old
English texts, addressing this gap. Our approach combines parameter-efficient fine-tuning (Low-Rank
Adaptation, LORA), data augmentation via backtranslation, and a dual-agent pipeline that separates the
‘tasks of content generation (in English) and translation (into Old English). Evaluation with automated
metrics (BLEU, METEOR, and CHRF) shows significant improvements over baseline models, with BLEU scores
increasing from 26 to over 65 for English-to-Old English translation. Expert human assessment also
confirms high grammatical accuracy and stylistic fidelity in the generated texts. Beyond expanding the
Old English corpus, our method offers @ practical blueprint for revitalizing other endangered
languages, effectively uniting AI innovation with the goals of cultural preservation.
The Transformer, with its scaled dot-product attention mechanism,
has become a foundational architecture in modern AI. However, this mechanism is computationally
intensive and incurs substantial energy costs. We propose a new Transformer architecture
EcoTransformer, in which the output context vector is constructed as the convolution of the values
using a Laplacian kernel, where the distances are measured by the Li metric between the queries and
keys. Compared to dot-product based attention, the new attention score calculation is free of matrix
multiplication. It performs on par with, or even surpasses, scaled dot-product attention in NLP,
bioinformatics, and vision tasks, while consuming significantly less energy.
Speech language models refer to language models with speech
processing and understanding capabilities. One key desirable capability for speech language models is
‘the ability to capture the intricate interdependency between content and prosody. The existing
mainstream paradigm of training speech language models, which converts speech into discrete tokens
before feeding them into LLMs, is sub-optimal in learning prosody information -- we find that the
resulting LLMs do not exhibit obvious emerging prosody processing capabilities via pre-training alone.
To overcome this, we propose ProsodyLM, which introduces a simple tokenization scheme amenable to
learning prosody. Each speech utterance is first transcribed into text, followed by a sequence of
word-level prosody tokens. Compared with conventional speech tokenization schemes, the proposed
‘tokenization scheme retains more complete prosody information, and is more understandable to text-
based LLMs. We find that ProsodylM can learn surprisingly diverse emerging prosody processing
capabilities through pre-training alone, ranging from harnessing the prosody nuances in generated
speech, such as contrastive focus, understanding emotion and stress in an utterance, to maintaining
prosody consistency in long contexts.
image captioning often suffers from a lack of detail, with base models producing short, generic
captions. This limitation persists even though VLMs are equipped with strong vision and language
backbones. While supervised data and complex reward functions have been proposed to improve detailed
image captioning, we identify @ simpler underlying issue: a bias towards the end-of-sequence (EOS)
‘token, which is introduced during cross-entropy training. We propose an unsupervised method to debias
‘the model's tendency to predict the EOS token prematurely. By reducing this bias, we encourage the
generation of longer, more detailed captions without the need for intricate reward functions or
Supervision. Our approach is straightforward, effective, and easily applicable to any pretrained
model. We demonstrate its effectiveness through experiments with three VLMs and on three detailed
captioning benchmarks. Our results show a substantial increase in caption length and relevant details,
albeit with an expected increase in the rate of hallucinations.
From: Abdelrahman Mohamed [view email][v1] Sat, 26 Jul 2025 23:00:43 UTC (1,048 KB)
generate outputs aligned with end-user preferences without further training. Recent post-training
methods achieve this by using small guidance models to modify token generation during inference. These
methods typically optimize a reward function KL-regularized by the original LLM taken as the reference
policy. A critical limitation, however, is their dependence on a pre-trained reward model, which
requires fitting to human preference feedback--a potentially unstable process. In contrast, we
introduce PITA, @ novel framework that integrates preference feedback directly into the LLI's token
generation, eliminating the need for a reward model. PITA learns a small preference-based guidance
policy to modify token probabilities at inference time without LLM fine-tuning, reducing computational
cost and bypassing the pre-trained reward model dependency. The problem is framed as identifying an
underlying preference distribution, solved through stochastic search and iterative refinement of the
preference-based guidance model. We evaluate PITA across diverse tasks, including mathematical
reasoning and sentiment classification, demonstrating its effectiveness in aligning LLM outputs with
user preferences.
From: Sarat Chandra Bobbili [view email][vi] Sat, 26 Jul 2025 21:
models (LLMs) by integrating external knowledge retrieved at inference time. hile RAG demonstrates
strong performance on benchmarks largely derived from general-domain corpora like Wikipedia, its
effectiveness under realistic, diverse retrieval scenarios remains underexplored. We evaluated RAG
systems using MassiveDS, a large-scale datastore with mixture of knowledge, and identified critical
Limitations: retrieval mainly benefits smaller models, rerankers add minimal value, and no single
retrieval source consistently excels. Moreover, current LLMs struggle to route queries across
heterogeneous knowledge sources. These findings highlight the need for adaptive retrieval strategies
before deploying RAG in real-world settings. Our code and data can be found at this https URL.
The uninterpretability of DNNs has led to the adoption of abstract interpretation-
based certification as a practical means to establish trust in real-world systems that rely on DNNs.
However, the current landscape supports only a limited set of certifiers, and developing new ones or
modifying existing ones for different applications remains difficult. This is because the mathematical
design of certifiers is expressed at the neuron level, while their implementations are optimized and
‘executed at the tensor level. This mismatch creates a semantic gap between design and implementation,
making manual bridging both complex and expertise-intensive -- requiring deep knowledge in formal
methods, high-performance computing, etc.
We propose @ compiler framework that automatically translates neuron-level specifications of DNN
certifiers into tensor-based, layer-level implementations. This is enabled by tuo key innovations: 3
novel stack-based intermediate representation (IR) and a shape analysis that infers the implicit
‘tensor operations needed to simulate the neuron-level semantics. During lifting, the shape analysis
creates tensors in the minimal shape required to perform the corresponding operations. The IR also
enables domain-specific optimizations as rewrites. At runtime, the resulting tensor computations
exhibit sparsity tied to the DNN architecture. This sparsity does not align well with existing
formats. To address this, we introduce g-BCSR, a double-compression format that represents tensors as
collections of blocks of varying sizes, each possibly internally sparse.
Using our compiler and g-BCSR, we make it easy to develop new certifiers and analyze their utility
across diverse DNNs. Despite its flexibility, the compiler achieves performance comparable to hand-
‘optimized implementations.
Existing Log Anomaly Detection (LogAD) methods are often slow,
dependent on error-prone parsing, and use unrealistic evaluation protocols. We introduce $K"4$, an
unsupervised and parser-independent framework for high-performance online detection. $k°4$ transforms
arbitrary log embeddings into compact four-dimensional descriptors (Precision, Recall, Density,
Coverage) using efficient k-nearest neighbor (k-NN) statistics. These descriptors enable lightweight
detectors to accurately score anomalies without retraining. Using a more realistic online evaluation
protocol, $k*4$ sets a new state-of-the-art (AUROC: 0.995-0.999), outperforming baselines by large
margins while being orders of magnitude faster, with training under 4 seconds and inference as low as
Statistical infographics are powerful tools that simplify complex
data into visually engaging and easy-to-understand formats. Despite advancements in AI, particularly
with LLMs, existing efforts have been limited to generating simple charts, with no prior work
addressing the creation of complex infographics from text-heavy documents that demand a deep
understanding of the content. We address this gap by introducing the task of generating statistical
infographics composed of multiple sub-charts (e.g-, line, bar, pie) that are contextually accurate,
insightful, and visually aligned. To achieve this, we define infographic metadata that includes its
title and textual insights, along with sub-chart-specific details such as their corresponding data and
alignment. We also present Infodat, the first benchmark dataset for text-to-infographic metadata
generation, where each sample links a document to its metadata. We propose Infogen, a two-stage
framework where fine-tuned LLMs first generate metadata, which is then converted into infographic
code. Extensive evaluations on Infodat demonstrate that Infogen achieves state-of-the-art performance,
‘outperforming both closed and open-source LLMs in text-to-statistical infographic generation.
The efficacy of Large Language Models (LLMs) in long-context
tasks is often hampered by the substantial memory footprint and computational demands of the Key-Value
(Kv) cache. Current compression strategies, including token eviction and learned projections,
frequently lead to biased representations -- either by overemphasizing recent/high-attention tokens or
by repeatedly degrading information from earlier context -- and may require costly model retraining.
We present FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), @ novel, training-free KV cache
compression framework that ensures unbiased information retention. FAEDKV operates by transforming the
Kv cache into the frequency domain using a proposed Infinite-Window Fourier Transform (INDFT). This
approach allows for the equalized contribution of all tokens to the compressed representation,
effectively preserving both early and recent contextual information. A preliminary frequency ablation
study identifies critical spectral components for layer-wise, targeted compression. Experiments on
LongBench benchmark demonstrate FAEDKV's superiority over existing methods by up to 22\%. In addition,
‘our method shows superior, position-agnostic retrieval accuracy on the Needle-In-A-Haystack task
compared to compression based approaches.
We propose a meta learning framework for detecting anomalies in human language across
diverse domains with limited labeled data. Anomalies in language ranging from spam and fake news to
hate speech pose a major challenge due to their sparsity and variability. We treat anomaly detection
as a few shot binary classification problem and leverage meta-learning to train models that generalize
across tasks. Using datasets from domains such as SMS spam, COVID-19 fake news, and hate speech, we
evaluate model generalization on unseen tasks with minimal labeled anomalies. Our method combines
episodic training with prototypical networks and domain resampling to adapt quickly to new anomaly
detection tasks. Empirical results show that our method outperforms strong baselines in Fl and AUC
scores. We also release the code and benchmarks to facilitate further research in few-shot text
anomaly detection.
Large language models (LLMs) like GPT-3 and BERT have revolutionized natural language
processing (NLP), yet their environmental costs remain dangerously overlooked. This article critiques
the sustainability of LLMs, quantifying their carbon footprint, water usage, and contribution to e-
waste through case studies of models such as GPT-4 and energy-efficient alternatives like Mistral 78.
Training @ Single LLM can emit carbon dioxide equivalent to hundreds of cars driven annually, while
data centre cooling exacerbates water scarcity in vulnerable regions. Systemic challenges corporate
greenwashing, redundant model development, and regulatory voids perpetuate harm, disproportionately
burdening marginalized communities in the Global South. However, pathways exist for sustainable NL
technical innovations (.g., model pruning, quantum computing), policy reforms (carbon taxes,
mandatory emissions reporting), and cultural shifts prioritizing necessity over novelty. By analysing
industry leaders (Google, Microsoft) and laggards (Amazon), this work underscores the urgency of
ethical accountability and global cooperation. Without immediate action, AIs ecological toll risks
outpacing its societal benefits. The article concludes with a call to align technological progress
with planetary boundaries, advocating for equitable, transparent, and regenerative AI systems that
prioritize both human and environmental well-being.
Fron: Sayed Mahbub Hasan Amiri [view enail][v1] Sat, 26 Jul 2025 17:
The advent of large language models (LLMs) has led to significant
achievements in various domains, including legal text processing. Leveraging LLMs for legal tasks is a
natural evolution and an increasingly compelling choice. However, their capabilities are often
portrayed as greater than they truly are. Despite the progress, we are still far from the ultimate
goal of fully automating legal tasks using artificial intelligence (AI) and natural language
Processing (NLP). Moreover, legal systems are deeply domain-specific and exhibit substantial variation
across different countries and languages. The need for building legal text processing applications for
different natural languages is, therefore, large and urgent. However, there is a big challenge for
Legal NLP in low-resource languages such as Vietnamese due to the scarcity of resources and annotated
data. The need for labeled legal corpora for supervised training, validation, and supervised fine-
‘tuning is critical. In this paper, we introduce the VLQA dataset, a comprehensive and high-quality
resource tailored for the Vietnamese legal domain. We also conduct a comprehensive statistical
analysis of the dataset and evaluate its effectiveness through experiments with state-of-the-art
models on legal information retrieval and question-answering tasks.
Recently, competition in the field of artificial intelligence
(AI) has intensified among major technological companies, resulting in the continuous release of new
large-language models (LLMs) that exhibit improved language understanding and context-based reasoning
capabilities. It is expected that these advances will enable more efficient personalized
recommendations in LLM-based recommendation systems through improved quality of training data and
architectural design. However, many studies have not considered these recent developments. In this
study, it was proposed to improve LLM-based recommendation systems by replacing Llama2 with Llama3 in
‘the LlamaRec framework. To ensure a fair comparison, random seed values were set and identical input
data was provided during preprocessing and training. The experimental results show average performance
improvements of 38.65\%, 8.69\%, and 8.19\% for the ML-100K, Beauty, and Games datasets, respectively,
‘thus confirming the practicality of this method. Notably, the significant improvements achieved by
model replacement indicate that the recommendation quality can be improved cost-effectively without
‘the need to make structural changes to the system. Based on these results, it is our contention that
‘the proposed approach is a viable solution for improving the performance of current recommendation
This study investigates the estimation of reliability for large language models
(LLMs) in scoring writing tasks from the AP Chinese Language and Culture Exam. Using generalizability
‘theory, the research evaluates and compares score consistency between human and AI raters across two
‘types of AP Chinese free-response writing tasks: story narration and email response. These essays were
independently scored by two trained human raters and seven AI raters. Each essay received four scores:
one holistic score and three analytic scores corresponding to the domains of task completion,
delivery, and language use. Results indicate that although human raters produced more reliable scores
overall, LLMs demonstrated reasonable consistency under certain conditions, particularly for story
narration tasks. Composite scoring that incorporates both human and AI raters improved reliability,
which supports that hybrid scoring models may offer benefits for large-scale writing assessments.
Automated data visualization plays a crucial role in simplifying
data interpretation, enhancing decision-making, and improving efficiency. While large language models
(LLMs) have shown promise in generating visualizations from natural language, the absence of
comprehensive benchmarks limits the rigorous evaluation of their capabilities. We introduce Text2Vis,
‘a benchmark designed to assess text-to-visualization models, covering 20+ chart types and diverse data
science queries, including trend analysis, correlation, outlier detection, and predictive analytics.
It comprises 1,985 samples, each with a data table, natural language query, short answer,
visualization code, and annotated charts. The queries involve complex reasoning, conversational turns,
and dynamic data retrieval. We benchmark 11 open-source and closed-source models, revealing
significant performance gaps, highlighting key challenges, and offering insights for future
advancements. To close this gap, we propose the first cross-modal actor-critic agentic framework that
Jointly refines the textual answer and visualization code, increasing GPT-4o"s pass rate from 26% to
42% over the direct approach and improving chart quality. We also introduce an automated LLM-based
evaluation framework that enables scalable assessment across thousands of samples without human
annotation, measuring answer correctness, code execution success, visualization readability, and chart
accuracy. We release Text2Vis at this https URL.
their outputs, prompting ethical concerns regarding fairness and harm. In this work, we propose KLAAD
(KL-attention Alignment Debiasing), an attention-based debiasing framework that implicitly aligns
attention distributions between stereotypical and anti-stereotypical sentence pairs without directly
modifying model weights. KLAAD introduces a composite training objective combining Cross-Entropy, KL
divergence, and Triplet losses, guiding the model to consistently attend across biased and unbiased
contexts while preserving fluency and coherence. Experimental evaluation of KLAAD denonstrates
improved bias mitigation on both the B&Q and BOLD benchnarks, with minimal impact on language modeling
quality. The results indicate that attention-level alignment offers a principled solution for
mitigating bias in generative language models.
Fusing information from human observations can help robots
overcome sensing limitations in collaborative tasks. However, an uncertainty-aware fusion framework
requires a grounded likelihood representing the uncertainty of human inputs. This paper presents a
Feature Pyramid Likelihood Grounding Network (FP-LGN) that grounds spatial language by learning
relevant map image features and their relationships with spatial relation semantics. The model is
‘trained as @ probability estimator to capture aleatoric uncertainty in human language using three-
stage curriculum learning. Results showed that FP-LGN matched expert-designed rules in mean Negative
Log-Likelihood (NLL) and demonstrated greater robustness with lower standard deviation. Collaborative
sensing results demonstrated that the grounded likelihood successfully enabled uncertainty-aware
fusion of heterogeneous human language observations and robot sensor measurements, achieving
significant improvements in human-robot collaborative task performance.
€s.RO
Automated Program Repair (APR) uses various tools and techniques
to help developers achieve functional and error-free code faster. In recent years, Large Language
Models (LLMs) have gained popularity as components in APR tool chains because of their performance and
flexibility. However, training such models requires a significant amount of resources. Fine-tuning
‘techniques have been developed to adapt pre-trained LLMs to specific tasks, such as APR, and enhance
‘their performance at far lower computational costs than training from scratch. In this study, we
‘empirically investigate the impact of various fine-tuning techniques on the performance of LIMs used
for APR. Our experiments provide insights into the performance of a selection of state-of-the-art LLMs
pre-trained on code. The evaluation is done on three popular APR benchmarks (i.e., QuixBugs, Defects4]
and HumanEval-Java) and considers six different LLMs with varying parameter sizes (resp. CodeGen,
CodeTS, StarCoder, DeepSeekCoder, Bloom, and CodeLlama-2). We consider three training regimens: no
fine-tuning, full fine-tuning, and parameter-efficient fine-tuning (PEFT) using LoRA and IA3. We
observe that full fine-tuning techniques decrease the benchmarking performance of various models due
to different data distributions and overfitting. By using parameter-efficient fine-tuning methods, we
restrict models in the amount of trainable parameters and achieve better results.
Keywords: large language models, automated program repair, parameter-efficient fine-tuning, Al4Code,
Early detection of depression from online social media posts holds promise for
providing timely mental health interventions. In this work, we present a high-quality, expert-
annotated dataset of 1,017 social media posts labeled with depressive spans and mapped to 12
depression symptom categories. Unlike prior datasets that primarily offer coarse post-level labels
\cite{cohan-etal-2018-smhd}, our dataset enables fine-grained evaluation of both model predictions and
generated explanations.
We develop an evaluation framework that leverages this clinically grounded dataset to assess the
faithfulness and quality of natural language explanations generated by large language models (LLMs).
Through carefully designed prompting strategies, including zero-shot and few-shot approaches with
domain-adapted examples, we evaluate state-of-the-art proprietary LLMs including GPT-4.1, Gemini 2.5
Our comprehensive empirical analysis reveals significant differences in how these models perform on
clinical explanation tasks, with zero-shot and few-shot prompting. Our findings underscore the value
of human expertise in guiding LLM behavior and offer a step toward safer, more transparent AI systems
for psychological well-being.
he present the Polish Vocabulary Size Test (PVST), a novel tool for assessing the
receptive vocabulary size of both native and non-native Polish speakers. Based on Item Response Theory
and Computerized Adaptive Testing, PVST dynamically adjusts to each test-taker's proficiency level,
‘ensuring high accuracy while keeping the test duration short. To validate the test, a pilot study was
conducted with 1.475 participants. Native Polish speakers demonstrated significantly larger
vocabularies compared to non-native speakers. For native speakers, vocabulary size showed a strong
positive correlation with age. The PVST is available online at this http URL.
In-car conversational AI is becoming increasingly critical as
autonomous vehicles and smart assistants gain widespread adoption. Yet, existing datasets fail to
capture the spontaneous disfluencies such as hesitations, false starts, repetitions, and self-
corrections that characterize real driver-AI dialogs. To address this, we introduce DiscoDrive, a
synthetic corpus of 3500 multi-turn dialogs across seven automotive domains, generated using a two-
stage, prompt-driven pipeline that dynamically integrates disfluencies during synthesis. We show that
DiscoDrive is effective both as a training resource, enabling DialoGPT-Medium and T5-Base to match or
‘exceed KVRET-trained models on the MultiNOZ 2.2 and Schema-Guided Dialogue (SGD) relevant test sets
(BLEU-4 improvements of @.26 to 0.61; METEOR +2.10; ROUGE-L +3.433 BERTScore F1 improvements of 1.35
to 3.43), and as a data augmentation resource in low-resource scenarios, delivering additional gains
of up to BLEU-4 #0.38, METEOR +1.95, ROUGE-L 42.87, and BERTScore F1 +4.0 when combined with 10
percent of KVRET. Human evaluations further confirm that dialogs sampled from DiscoDrive are rated
higher than KVRET’s human-collected dialogs in naturalness (3.8 vs 3.6) and coherence (4.1 vs 4.0),
and are perceived as more context-appropriate than leading post-hoc methods (such as LARD), without
compromising clarity. DiscoDrive fills a critical gap in existing resources and serves as a versatile
corpus for both training and augmenting conversational AI, enabling robust handling of real-world,
disfluent in-car interactions.
Large-scale reinforcement learning with verifiable rewards (RLVR)
has demonstrated its effectiveness in harnessing the potential of large language models (LLMs) for
single-turn reasoning tasks. In realistic reasoning scenarios, LLMs can often utilize external tools
to assist in task-solving processes. However, current RL algorithms inadequately balance the models’
intrinsic long-horizon reasoning capabilities and their proficiency in multi-turn tool interactions.
To bridge this gap, we propose Agentic Reinforced Policy Optimization (ARPO), a novel agentic RL
algorithm tailored for training multi-turn LLM-based agents. Through preliminary experiments, we
observe that LLMs tend to exhibit highly uncertain behavior, characterized by an increase in the
‘entropy distribution of generated tokens, inmediately following interactions with external tools.
Motivated by this observation, ARPO incorporates an entropy-based adaptive rollout mechanism,
dynamically balancing global trajectory sampling and step-level sampling, thereby promoting
‘exploration at steps with high uncertainty after tool usage. By integrating an advantage attribution
estimation, ARPO enables LLMs to internalize advantage differences in stepwise tool-use interactions.
Our experiments across 13 challenging benchmarks in computational reasoning, knowledge reasoning, and
deep search domains demonstrate ARPO's superiority over trajectory-level RL algorithms. Remarkably,
ARPO achieves improved performance using only half of the tool-use budget required by existing
methods, offering a scalable solution for aligning LLM"-based agents with real-time dynamic
‘environments. Our code and datasets are released at this https URL
Processing long-context inputs with large language models
presents a significant challenge due to the enormous memory requirements of the Key-Value (KV) cache
during inference. Existing KV cache compression methods exhibit noticeable performance degradation
when memory is reduced by more than 85%. Additionally, strategies that leverage GPU-CPU collaboration
for approximate attention remain underexplored in this setting. We propose HCAttention, a
heterogeneous attention computation framework that integrates key quantization, value offloading, and
dynamic KV eviction to enable efficient inference under extreme memory constraints. The method is
compatible with existing transformer architectures and does not require model fine-tuning.
Experimental results on the LongBench benchmark demonstrate that our approach preserves the accuracy
of full-attention model while shrinking the KV cache memory footprint to 25% of its original size.
Remarkably, it stays competitive with only 12.5% of the cache, setting a new state-of-the-art in LLM
KV cache compression. To the best of our knowledge, HCAttention is the first to extend the Llama-3-88
model to process 4 million tokens on a single A100 GPU with 59GB memory.
Effectively handling long contexts is challenging for Large Language Models (LLMs)
due to the rarity of long texts, high computational demands, and substantial forgetting of short-
context abilities. Recent approaches have attempted to construct long contexts for instruction tuning,
but these methods often require LLMs or human interventions, which are both costly and limited in
length and diversity. Also, the drop in short-context performances of present long-context LLMs
remains significant. In this paper, we introduce Flora, an effortless (human/LLM-free) long-context
construction strategy. Flora can markedly enhance the long-context performance of LLMs by arbitrarily
assembling short instructions based on categories and instructing LLMs to generate responses based on
long-context meta-instructions. This enables Flora to produce contexts of arbitrary length and scale
with rich diversity, while only slightly compromising short-context performance. Experiments on
Llama3-88-Instruct and QuQ-328 show that LLMs enhanced by Flora excel in three long-context benchmarks
while maintaining strong performances in short-context tasks. Our data-construction code is available
at \href{this https URL}{this https URL}.
Recent advances in large language models (LLMs) have highlighted
‘the potential of reinforcement learning with verifiable rewards (RLVR) to enhance reasoning
capabilities through extended output sequences. However, traditional RL frameworks face inefficiencies
when handling ultra-long outputs due to long-tail sequence distributions and entropy collapse during
‘training. To address these challenges, we propose an Ultra-Long Output Reinforcement Learning (UloRL)
approach for advancing large language models’ reasoning abilities. Specifically, we divide ultra long
‘output decoding into short segments, enabling efficient training by mitigating delays caused by long-
tail samples. Additionally, we introduce dynamic masking of well-Mastered Positive Tokens (MPTs) to
prevent entropy collapse. Experimental results demonstrate the effectiveness of our approach. On the
Quen3-30B-A3B model, RL with segment rollout achieved 2.06x increase in training speed, while RL
‘training with 128k-token outputs improves the model's performance on AINE2025 from 70.9\% to 85.1\%
and on BeyondAIME from 50.7\% to 61.9\%, even surpassing Quen3-2358-A22B with remarkable gains. These
findings underscore the potential of our methods to advance the reasoning capabilities of LLMs with
ultra-long sequence generation. We will release our code and model for further use by the community.
In addition to its more widely studied political activities, the
American Evangelical movement has a well-developed but less externally visible cultural and literary
side. Christian Fiction, however, has been little studied, and what scholarly attention there is has
focused on the explosively popular Left Behind series. In this work, we use computational tools to
provide both a broad topical overview of Christian Fiction as a genre and a more directed exploration
of how its authors depict divine acts. Working with human annotators we first developed definitions
and a codebook for “acts of God.” We then adapted those instructions designed for human annotators for
use by a recent, lightweight LM with the assistance of a much larger model. The laptop-scale LM is
capable of matching human annotations, even when the task is subtle and challenging. Using these
annotations, we show that significant and meaningful differences exist between the Left Behind books
and Christian Fiction more broadly and between books by male and female authors.
From: Rebecca M. Ml. Hicke [view email][v1] Sat, 26 Jul 2025 93:01:59 UTC (403 KB)
Mathematical reasoning is a cornerstone of artificial general
intelligence and a primary benchmark for evaluating the capabilities of Large Language Models (LLMs).
While state-of-the-art models show promise, they often falter when faced with complex problems that
demand deep conceptual understanding and intricate, multi-step deliberation. To address this
challenge, we introduce JT-Hath-8B, a series of open-source models comprising base, instruct, and
‘thinking versions, built upon a systematic, multi-stage optimization framework. Our pre-training
corpus is a high-quality, 2108-token dataset curated through a dedicated data pipeline that uses
model-based validation to ensure quality and diversity. The Instruct Model is optimized for direct,
concise answers through Supervised Fine-Tuning (SFT) and a GRPO-based reinforcement learning (RL)
method. The Thinking Model is trained for complex problem-solving using @ Long Chain-of-Thought (Long
Cot) approach, combining SFT with a novel, multi-stage RL curriculum that progressively increases task
difficulty and context length up to 32K tokens. JT-Hath-8B achieves state-of-the-art results among
open-source models of similar size, surpassing prominent models like OpenAI's O1-mini and GPT-40 , and
demonstrating superior performance on competition-level mathematics.
abilities in various natural language processing areas, but they demand high computation resources
which limits their deployment in real-world. Distillation is one technique to solve this problem
‘through either knowledge distillation or task distillation. Both distillation approaches train small
models to imitate specific features of LiMs, but they all neglect basic reading education for small
models on generic texts that are \emph{unrelated} to downstream tasks. In this paper, we propose basic
reading distillation (BRD) which educates @ small model to imitate LLMs basic reading behaviors, such
‘as named entity recognition, question raising and answering, on each sentence. After such basic
education, we apply the small model on various tasks including language inference benchmarks and BIG-
bench tasks. It shows that the small model can outperform or perform comparable to over 20x bigger
LLMs. Analysis reveals that BRD effectively influences the probability distribution of the small
model, and has orthogonality to either knowledge distillation or task distillation.
In Table-to-Text (T2T) generation, existing approaches
predominantly focus on providing objective descriptions of tabular data. However, generating text that
incorporates subjectivity, where subjectivity refers to interpretations beyond raw numerical data,
remains underexplored. To address this, we introduce a novel pipeline that leverages intermediate
representations to generate both objective and subjective text from tables. Our three-stage pipeline
consists of: 1) extraction of Resource Description Framework (RDF) triples, 2) aggregation of text
into coherent narratives, and 3) infusion of subjectivity to enrich the generated text. By
incorporating RDFs, our approach enhances factual accuracy while maintaining interpretability. Unlike
fine-tuned T5 models while achieving comparable performance to GPT-3.5 and outperforming Mistral-78
and Llama-2 in several metrics. We evaluate our approach through quantitative and qualitative
analyses, demonstrating its effectiveness in balancing factual accuracy with subjective
interpretation. To the best of our knowledge, this is the first work to propose a structured pipeline
for T2T generation that integrates intermediate representations to enhance both factual correctness
and subjectivity.
Although LLMs have attained significant success in high-resource
languages, their capacity in low-resource linguistic environments like Kannada and Arabic is not yet
fully understood. This work benchmarking the performance of multilingual and monolingual Large
Language Models (LLMs) across Arabic, English, and Indic languages, with particular emphasis on the
effects of model compression strategies such as pruning and quantization. Findings shows significant
performance differences driven by linguistic diversity and resource availability on SOTA LHS as
BLOOMZ, AceGPT, Jais, LLaltA-2, XGLM, and AraGPT2. We find that multilingual versions of the model
‘outperform their language-specific counterparts across the board, indicating substantial cross-lingual
transfer benefits. Quantization (4-bit and 8-bit) is effective in maintaining model accuracy while
promoting efficiency, but aggressive pruning significantly compromises performance, especially in
bigger models. Our findings pinpoint key strategies to construct scalable and fair multilingual NLP
solutions and underscore the need for interventions to address hallucination and generalization errors
in the low-resource setting.
The intersection of AI and legal systems presents a growing need for tools that
support legal education, particularly in under-resourced languages such as Romanian. In this work, we
aim to evaluate the capabilities of Large Language Models (LLMs) and Vision-Language Models (VLMs) in
understanding and reasoning about Romanian driving law through textual and visual question-answering
tasks. To facilitate this, we introduce RoD-TAL, a novel multimodal dataset comprising Romanian
driving test questions, text-based and image-based, alongside annotated legal references and human
retrievers, and reasoning-optimized models across tasks including Information Retrieval (IR), Question
Answering (QA), Visual IR, and Visual QA. Our experiments demonstrate that domain-specific fine-tuning
significantly enhances retrieval performance. At the same time, chain-of-thought prompting and
specialized reasoning models improve QA accuracy, surpassing the minimum grades required to pass
driving exams. However, visual reasoning remains challenging, highlighting the potential and the
Limitations of applying LLMs and VLMs to legal education.
Recent advances in large language models have catalyzed the
development of multimodal LLMs (MLLMs) that integrate text, speech, and vision within unified
frameworks. As HLLMs evolve from narrow, monolingual, task-specific systems to general-purpose
instruction-following models, a key frontier lies in evaluating their multilingual and multimodal
capabilities over both long and short contexts. However, existing benchmarks fall short in evaluating
these dimensions jointly: they are often limited to English, mostly focus on one single modality at a
time, rely on short-form contexts, or lack human annotations--hindering comprehensive assessment of
model performance across languages, modalities, and task complexity. To address these gaps, we
introduce MCIF (Multimodal Crosslingual Instruction Following), the first multilingual human-annotated
benchmark based on scientific talks that is designed to evaluate instruction-following in
crosslingual, multimodal settings over both short- and long-form inputs. MCIF spans three core
modalities--speech, vision, and text--and four diverse languages (English, German, Italian, and
Chinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret instructions across
Tanguages and combine them with multimodal contextual information. MCIF is released under a CC-BY 4.0
license to encourage open research and progress in MLLMs development.
This paper presents HITSZ's submission for the INSLT 2025 Indic
track, focusing on speech-to-text translation (ST) for English-to-Indic and Indic-to-English language
pairs. To enhance translation quality in this low-resource scenario, we propose an end-to-end system
integrating the pre-trained Whisper automated speech recognition (ASR) model with Krutrim, an Indic-
specialized large language model (LLM). Experimental results demonstrate that our end-to-end system
achieved average BLEU scores of $28.88 for English-to-Indic directions and $27.86$ for Indic-to-
English directions. Furthermore, we investigated the Chain-of-Thought (CoT) method. hile this method
showed potential for significant translation quality improvements on successfully parsed outputs (e.g.
2 $13.84§ BLEU increase for Tamil-to-English), we observed challenges in ensuring the model
consistently adheres to the required CoT output format.
significantly enhanced their code generation capabilities. However, their robustness against
adversarial misuse, particularly through multi-turn malicious coding prompts, remains underexplored.
In this work, we introduce code decomposition attacks, where a malicious coding task is broken doun
into a series of seemingly benign subtasks across multiple conversational turns to evade safety
filters. To facilitate systematic evaluation, we introduce \benchmarkname{}, a large-scale benchmark
designed to evaluate the robustness of code LLMs against both single-turn and multi-turn malicious
especially under multi-turn scenarios. Fine-tuning on MOCHA improves rejection rates while preserving
coding ability, and importantly, enhances robustness on external adversarial datasets with up to 32.4%
increase in rejection rates without any additional supervision.
Transformer-based architectures have become the prevailing
backbone of large language models. However, the quadratic time and memory complexity of self-attention
remains a fundamental obstacle to efficient long-context modeling. To address this limitation, recent
research has introduced two principal categories of efficient attention mechanisms. Linear attention
methods achieve linear complexity through kernel approximations, recurrent formulations, or fastweight
dynamics, thereby enabling scalable inference with reduced computational overhead. Sparse attention
techniques, in contrast, limit attention computation to selected subsets of tokens based on fixed
patterns, block-wise routing, or clustering strategies, enhancing efficiency while preserving
contextual coverage. This survey provides @ systematic and comprehensive overview of these
developments, integrating both algorithmic innovations and hardware-level considerations. In addition,
we analyze the incorporation of efficient attention into largescale pre-trained language models,
including both architectures built entirely on efficient attention and hybrid designs that combine
local and global components. By aligning theoretical foundations with practical deployment strategies,
‘this work aims to serve as a foundational reference for advancing the design of scalable and efficient
Large language models (LLMs) possess extensive world knowledge,
including geospatial knowledge, which has been successfully applied to various geospatial tasks such
as mobility prediction and social indicator prediction. However, LLMs often generate inaccurate
geospatial knowledge, leading to geospatial hallucinations (incorrect or inconsistent representations
of geospatial information) that compromise their reliability. While the phenomenon of general
knowledge hallucination in LLMs has been widely studied, the systematic evaluation and mitigation of
geospatial hallucinations remain largely unexplored. To address this gap, we propose a comprehensive
evaluation framework for geospatial hallucinations, leveraging structured geospatial knowledge graphs
for controlled assessment. Through extensive evaluation across 20 advanced LLMs, we uncover the
hallucinations in their geospatial knowledge. Building on these insights, we introduce a dynamic
actuality aligning method based on Kahneman-Tversky Optimization (KTO) to mitigate geospatial
hallucinations in LLMs, leading to a performance improvement of over 29.6% on the proposed benchmark.
Extensive experimental results demonstrate the effectiveness of our benchmark and learning algorithm
in enhancing the trustworthiness of LLMs in geospatial knowledge and reasoning tasks.
he introduce MlBench-GUI, a hierarchical benchmark for evaluating
GUI automation agents across Windows, macOS, Linux, i0S, Android, and Web platforms. It comprises four
levels: GUI Content Understanding, Element Grounding, Task Automation, and Task Collaboration,
covering essential skills for GUI agents. In addition, we propose a novel Efficiency-Quality Area
(EQA) metric to assess GUI agent execution efficiency in online automation scenarios. Through Bench-
GUI, we identify accurate visual grounding as a critical determinant of overall task success,
‘emphasizing the substantial benefits of modular frameworks that integrate specialized grounding
modules. Furthermore, to achieve reliable GUI automation, an agent requires strong task planning and
cross-platform generalization abilities, with long-context memory, a broad action space, and long-term
reasoning playing a critical role. More important, task efficiency remains a critically underexplored
dimension, and all models suffer from substantial inefficiencies, with excessive redundant steps even
when tasks are ultimately completed. The integration of precise localization, effective planning, and
early stopping strategies is indispensable to enable truly efficient and scalable GUI automation. Our
benchmark code, evaluation data, and running environment will be publicly available at this https URL.
Many recent papers have studied the development of
superforecaster-level event forecasting LLMs. While methodological problems with early studies cast
doubt on the use of LLMs for event forecasting, recent studies with improved evaluation methods have
shoun that state-of-the-art LLMs are gradually reaching superforecaster-level performance, and
reinforcement learning has also been reported to improve future forecasting. Additionally, the
unprecedented success of recent reasoning models and Deep Research-style models suggests that
‘technology capable of greatly improving forecasting performance has been developed. Therefore, based
‘on these positive recent trends, we argue that the time is ripe for research on large-scale training
of superforecaster-level event forecasting LLMs. We discuss two key research directions: training
methods and data acquisition. For training, we first introduce three difficulties of LLI-based event
forecasting training: noisiness-sparsity, knowledge cut-off, and simple reward structure problems.
Then, we present related ideas to mitigate these problems: hypothetical event Bayesian networks,
utilizing poorly-recalled and counterfactual events, and auxiliary reward signals. For data, we
propose aggressive use of market, public, and crawling datasets to enable large-scale training and
evaluation. Finally, we explain how these technical advances could enable AI to provide predictive
intelligence to society in broader areas. This position paper presents promising specific paths and
considerations for getting closer to superforecaster-level AI technology, aiming to call for
researchers’ interest in these directions.
We often rely on our intuition to anticipate the direction of a
conversation. Endowing automated systems with similar foresight can enable them to assist human-human
interactions. Recent work on developing models with this predictive capacity has focused on the
Conversations Gone Awry (CGA) task: forecasting whether an ongoing conversation will derail. In this
work, we revisit this task and introduce the first uniform evaluation framework, creating a benchmark
‘that enables direct and reliable comparisons between different architectures. This allows us to
present an up-to-date overview of the current progress in CGA models, in light of recent advancements
in language modeling. Our framework also introduces a novel metric that captures a model's ability to
revise its forecast as the conversation progresses.
Large language models (LLMs) are increasingly adapted to downstream tasks via
reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), which often
require thousands of rollouts to learn new tasks. We argue that the interpretable nature of language
can often provide a much richer learning medium for LLMs, compared with policy gradients derived from
sparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), prompt optimizer that
‘thoroughly incorporates natural language reflection to learn high-level rules from trial and error.
Given any AI system containing one or more LLM prompts, GEPA samples system-level trajectories (e.g.5
reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose
problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier
of its oun attempts. As 2 result of GEPA'S design, it can often turn even just a few rollouts into a
large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while
using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over
10% across two LLMs, and demonstrates promising results as an inference-time search strategy for code
From: Lakshya A Agrawal [view email][v1] Fri, 25 Jul 2025 1:
Understanding the relationship between training data and model
behavior during pretraining is crucial, but existing workflows make this process cumbersome,
fragmented, and often inaccessible to researchers. We present TokenSmith, an open-source library for
interactive editing, inspection, and analysis of datasets used in Megatron-style pretraining
frameworks such as GPT-NeoX, Megatron, and NVIDIA Neto. TokenSmith supports a wide range of operations
including searching, viewing, ingesting, exporting, inspecting, and sampling data, all accessible
‘through a simple user interface and @ modular backend. It also enables structured editing of
pretraining data without requiring changes to training code, simplifying dataset debugging,
validation, and experimentation.
TokenSmith is designed as a plug and play addition to existing large language model pretraining
workflows, thereby democratizing access to production-grade dataset tooling. TokenSmith is hosted on
GitHub1, with accompanying documentation and tutorials. A demonstration video is also available on
YouTube.
Medical text embedding models are foundational to a wide array of
healthcare applications, ranging from clinical decision support and biomedical information retrieval
‘to medical question answering, yet they remain hampered by two critical shortcomings. First, most
models are trained on 2 narrow slice of medical and biological data, beside not being up to date in
‘terms of methodology, making them ill suited to capture the diversity of terminology and semantics
‘encountered in practice. Second, existing evaluations are often inadequate: even widely used
benchmarks fail to generalize across the full spectrum of real world medical tasks.
To address these gaps, we leverage MEDTE, a GTE model extensively fine-tuned on diverse medical
corpora through self-supervised contrastive learning across multiple data sources, to deliver robust
medical text embeddings.
Alongside this model, we propose a comprehensive benchmark suite of 51 tasks spanning classification,
clustering, pair classification, and retrieval modeled on the Massive Text Embedding Benchmark (MTEB)
but tailored to the nuances of medical text. Our results demonstrate that this combined approach not
only establishes a robust evaluation framework but also yields embeddings that consistently outperform
state of the art alternatives in different tasks.
hile there exist strong benchmark datasets for grammatical error
correction (GEC), high-quality annotated spoken datasets for Spoken GEC (SGEC) are still under-
resourced. In this paper, we propose a fully automated method to generate audio-text pairs with
grammatical errors and disfluencies. Moreover, we propose a series of objective metrics that can be
used to evaluate the generated data and choose the more suitable dataset for SGEC. The goal is to
generate an augmented dataset that maintains the textual and acoustic characteristics of the original
data while providing new types of errors. This augmented dataset should augment and enrich the
original corpus without altering the language assessment scores of the second language (12) learners.
We evaluate the use of the augmented corpus both for written GEC (the text part) and for SGEC (the
audio-text pairs). Our experiments are conducted on the S\&I Corpus, the first publicly available
speech dataset with grammar error annotations.
captioning, shifting from concise captions to detailed descriptions. We introduce LOTUS, a leaderboard
for evaluating detailed captions, addressing three main gaps in existing evaluations: lack of
standardized criteria, bias-auare assessments, and user preference considerations. LOTUS
comprehensively evaluates various aspects, including caption quality (e.g., alignment,
descriptiveness), risks (\eg, hallucination), and societal biases (e.g., gender bias) while enabling
preference-oriented evaluations by tailoring criteria to diverse user preferences. Our analysis of
recent LVLMs reveals no single model excels across all criteria, while correlations emerge between
caption detail and bias risks. Preference-oriented evaluations demonstrate that optimal model
selection depends on user priorities.
be introduce Speech-based Intelligence Quotient (SIQ) as a new
form of human cognition-inspired evaluation pipeline for voice understanding large language models,
LLM Voice, designed to assess their voice understanding ability. Moving beyond popular voice
understanding metrics such as word error rate (WER), SIQ examines LLM Voice across three cognitive
levels motivated by Bloom's Taxonomy: (1) Remembering (i.e., WER for verbatim accuracy); (2)
Understanding (i.e., similarity of LLM's interpretations); and (3) Application (i.e., QA accuracy for
simulating downstream tasks). We demonstrate that SIQ not only quantifies voice understanding
abilities but also provides unified comparisons between cascaded methods (e.g., ASR LLM) and end-to-
end models, identifies annotation errors in existing benchmarks, and detects hallucinations in LLM
Voice. Our framework represents a first-of-its-kind intelligence examination that bridges cognitive
principles with voice-oriented benchmarks, while exposing overlooked challenges in multi-modal
In this paper, we investigate the impact of incorporating
‘timestamp-based alignment between Automatic Speech Recognition (ASR) transcripts and Speaker
Diarization (SD) outputs on Speech Emotion Recognition (SER) accuracy. Misalignment between these two
modalities often reduces the reliability of multimodal emotion recognition systems, particularly in
conversational contexts. To address this issue, we introduce an alignment pipeline utilizing pre-
‘trained ASR and speaker diarization models, systematically synchronizing timestamps to generate
accurately labeled speaker segments. Our multimodal approach combines textual embeddings extracted via
ROBERTa with audio embeddings from Wav2Vec, leveraging cross-attention fusion enhanced by a gating
mechanism. Experimental evaluations on the IEMOCAP benchmark dataset demonstrate that precise
‘timestamp alignment improves SER accuracy, outperforming baseline methods that lack synchronization.
The results highlight the critical importance of temporal alignment, demonstrating its effectiveness
in enhancing overall emotion recognition accuracy and providing a foundation for robust multimodal
‘emotion analysis.
Recently, recurrent large language models (Recurrent LLMs) with
Linear computational complexity have re-emerged as efficient alternatives to self-attention-based LLMs
(Self-attention LLMs), which have quadratic complexity. However, Recurrent LLMs often underperform on
long-context tasks due to their limited fixed-size memory. Previous research has primarily focused on
enhancing the memory capacity of Recurrent LLMs through architectural innovations, but these
approaches have not yet enabled Recurrent LLMs to match the performance of Self-Attention LLMs on
long-context tasks. We argue that this limitation arises because processing the entire context at once
is not well-suited for Recurrent LLMs. In this paper, we propose Smooth Reading, a chunk-wise
inference method inspired by human reading strategies. Smooth Reading processes context in chunks and
iteratively summarizes the contextual information, thereby reducing memory demands and making the
approach more compatible with Recurrent LLMs. Our experimental results show that this method
substantially narrows the performance gap between Recurrent and Self-Attention LLMs on long-context
‘tasks, while preserving the efficiency advantages of Recurrent LLMs. Our Smooth Reading boosts SWA-38-
4k (@ Recurrent LLM) from 5.68% lower to 3.61% higher performance than Self-Attention LLMs on
LongBench. Besides, our method maintains the high efficiency, training 3x faster and inferring 2x
faster at 64k context compared to Self-Attention LLMs. To our knowledge, this is the first work to
achieve comparable performance using Recurrent LLMs compared with Self-Attention LLMs on long-context
‘tasks. We hope our method will inspire future research in this area. To facilitate further progress,
we will release code and dataset.
Retrieval-augmented generation (RAG) has been widely adopted to
‘augment large language models (LLMs) with external knowledge for knowledge-intensive tasks. However,
its effectiveness is often undermined by the presence of noisy (i.e., low-quality) retrieved passages.
Enhancing LLMs’ robustness to such noise is critical for improving the reliability of RAG systems.
Recent advances have equipped LLMs with strong reasoning and self-reflection capabilities, allowing
‘them to identify and correct errors in their reasoning process. Inspired by this ability, we propose
Passage Injection-a simple yet effective method that explicitly incorporates retrieved passages into
LLMs" reasoning process, aiming to enhance the model's ability to recognize and resist noisy passages.
We validate Passage Injection under general RAG settings using 6/25 as the retriever. Experiments on
four reasoning-enhanced LLMs across four factual QA datasets demonstrate that Passage Injection
significantly improves overall RAG performance. Further analysis on two noisy retrieval settings-
random noise, where the model is provided irrelevant passages, and counterfactual noise, where it is
given misleading passages-shows that Passage Injection consistently improves robustness. Controlled
experiments confirm that Passage Injection can also effectively leverage helpful passages. These
findings suggest that incorporating passages in LLMs’ reasoning process is a promising direction for
building more robust RAG systems. The code can be found \href{here}{this https URL}.
Phenotype concept recognition (CR) is a fundamental task in
biomedical text mining, enabling applications such as clinical diagnostics and knowledge graph
construction. However, existing methods often require ontology-specific training and struggle to
generalize across diverse text types and evolving biomedical terminology. We present AutoPCR, a
Pronpt-based phenotype CR method that does not require ontology-specific training. AutoPCR performs CR
in three stages: entity extraction using a hybrid of rule-based and neural tagging strategies,
candidate retrieval via SapBERT, and entity linking through prompting a large language model.
Experiments on four benchmark datasets show that AUtoPCR achieves the best average and most robust
performance across both mention-level and document-level evaluations, surpassing prior state-of-the-
art methods. Further ablation and transfer studies demonstrate its inductive capability and
generalizability to new ontologies.
In this paper, we present our studies and experiments carried out
for the task 1 of the Challenge and Workshop on Multilingual Conversational Speech Language Model
(HLC-SLM), which focuses on advancing multilingual conversational speech recognition through the
development of speech language models architectures. Given the increasing relevance of real-world
conversational data for building robust Spoken Dialogue Systems, we explore three approaches to
multilingual ASR. First, we conduct an evaluation of the official baseline to better understand its
strengths and limitations, by training two projectors (linear and qformer) with different foundation
models. Second we leverage the SLAM-ASR framework to train a custom multilingual linear projector.
Finally we investigate the role of contrastive learning and the extended conversational context in
enhancing the robustness of recognition.
capabilities across a wide range of instruction-following tasks, yet their grasp of nuanced social
science concepts remains underexplored. This paper examines whether LLMs can identify and classify
fine-grained forms of populism, a complex and contested concept in both academic and media debates. To
this end, we curate and release novel datasets specifically designed to capture populist discourse. We
evaluate a range of pre-trained (large) language models, both open-weight and proprietary, across
multiple prompting paradigms. Our analysis reveals notable variation in performance, highlighting the
Limitations of LLMs in detecting populist discourse. We find that a fine-tuned ROBERTa classifier
vastly outperforms all new-era instruction-tuned LLMs, unless fine-tuned. Additionally, we apply our
best-performing model to analyze campaign speeches by Donald Trump, extracting valuable insights into
his strategic use of populist rhetoric. Finally, we assess the generalizability of these models by
benchmarking them on campaign speeches by European politicians, offering a lens into cross-context
transferability in political discourse analysis. In this setting, we find that instruction-tuned LLMs
exhibit greater robustness on out-of-domain data.
Auto-regressive language models factorize sequence probabilities and are trained by
minimizing the negative log-likelihood (NLL) objective. While empirically powerful, 2 deep theoretical
understanding of why this simple objective yields such versatile representations remains elusive. This
work introduces a unifying analytical framework using Markov Categories (MCs) to deconstruct the AR
generation process and the NLL objective. We model the single-step generation map as 2 composition of
Markov kernels in the category Stoch. This compositional view, when enriched with statistical
divergences, allows us to dissect information flow and learned geometry. Our framework makes three
main contributions. First, we provide a formal, information-theoretic rationale for the success of
modern speculative decoding methods like EAGLE, quantifying the information surplus in hidden states
‘that these methods exploit. Second, we formalize how NLL minimization forces the model to learn not
just the next token, but the data's intrinsic conditional stochasticity, a process we analyze using
categorical entropy. Third, and most centrally, we prove that NLL training acts as an implicit form of
spectral contrastive learning. By analyzing the information geometry of the model's prediction head,
we show that NLL implicitly forces the learned representation space to align with the eigenspectrum of
a predictive similarity operator, thereby learning a geometrically structured space without explicit
contrastive pairs. This compositional and information-geometric perspective reveals the deep
structural principles underlying the effectiveness of modern LMs. Project Page: this https URL
performance to LLMs while offering distinct advantages in inference speed and mathematical reasoning
‘this http URL precise and rapid generation capabilities of LLDMs amplify concerns of harmful
generations, while existing jailbreak methodologies designed for Large Language Models (LLMs) prove
Limited effectiveness against LLOMs and fail to expose safety this http URL defense cannot
definitively resolve harmful generation concerns, as it remains unclear whether LLDMs possess safety
robustness or existing attacks are incompatible with diffusion-based this http URL address this, we
first reveal the vulnerability of LLDMs to jailbreak and demonstrate that attack failure in LLDMs
stems from fundamental architectural this http URL present a PArallel Decoding jailbreak (PAD) for
diffusion-based language models. PAD introduces Multi-Point Attention Attack, which guides parallel
generative processes toward harmful outputs that inspired by affirmative response patterns in LLMs.
Experimental evaluations across four LLDMs demonstrate that PAD achieves jailbreak attack success
rates by 97%, revealing significant safety vulnerabilities. Furthermore, compared to autoregressive
LLMs of the same size, LLDMs increase the harmful generation speed by 2x, significantly highlighting
risks of uncontrolled this http URL comprehensive analysis, we provide an investigation into LLDM
architecture, offering critical insights for the secure deployment of diffusion-based language models.
We investigate the problem of segmenting unlabeled speech into
word-like units and clustering these to create a lexicon. Prior work can be categorized into two
frameworks. Bottom-up methods first determine boundaries and then cluster the fixed segmented words
into a lexicon. In contrast, top-down methods incorporate information from the clustered words to
inform boundary selection. However, it is unclear whether top-down information is necessary to improve
segmentation. To explore this, we look at two similar approaches that differ in whether top-down
clustering informs boundary selection. Our simple bottom-up strategy predicts word boundaries using
‘the dissimilarity between adjacent self-supervised features, then clusters the resulting segments to
construct @ lexicon. Our top-down system is an updated version of the ES-KMeans dynamic programming
method that iteratively uses K-means to update its boundaries. On the five-language ZeroSpeech
benchmarks, both approaches achieve comparable state-of-the-art results, with the bottom-up system
being nearly five times faster. Through detailed analyses, we show that the top-down influence of ES-
kMeans can be beneficial (depending on factors like the candidate boundaries), but in many cases the
simple bottom-up method performs just as well. For both methods, we show that the clustering step is a
Limiting factor. Therefore, we recommend that future work focus on improved clustering techniques and
learning more discriminative word-like representations. Project code repository: this https URL.
From: Simon Malan [view email][v1] Fri, 25 Jul 2025 12:19:16 UTC (552 KB)
Large language models have given social robots the ability to
autonomously engage in open-domain conversations. However, they are still missing fundamental social
skill: making use of the multiple modalities that carry social interactions. While previous work has
focused on task-oriented interactions that require referencing the environment or specific phenomena
in social interactions such as dialogue breakdowns, we outline the overall needs of a multimodal
system for social conversations with robots. We then argue that vision-language models are able to
process this wide range of visual information in a sufficiently general manner for autonomous social
robots. We describe how to adapt them to this setting, which technical challenges remain, and briefly
Despite the ongoing improvements in the design of large language
models (LLMs) to foster inclusion and balanced responses, these systems remain susceptible to encoding
and amplifying social biases. This study examines how dialectal variation, specifically African
American Vernacular English (AAVE) versus Standard American English (SAE), interacts with data
poisoning to influence toxicity in outputs. Using both small- and medium-scale LLaMA models, we show
‘that even minimal exposure to poisoned data significantly increases toxicity for AAVE inputs, while it
remains comparatively unaffected for SAE. Larger models exhibit a more significant amplification
effect which suggests heightened susceptibility with scale. To further assess these disparities, we
employed GPT-40 as a fairness auditor, which identified harmful stereotypical patterns
disproportionately tied to AAVE inputs, including portrayals of aggression, criminality, and
intellectual inferiority. These findings underscore the compounding impact of data poisoning and
dialectal bias and emphasize the need for dialect-aware evaluation, targeted debiasing interventions,
and socially responsible training protocols during development.
The increasing use of Large Language Models (LLMs) in a large
variety of domains has sparked worries about how easily they can perpetuate stereotypes and contribute
‘to the generation of biased content. With a focus on gender and professional bias, this work examines
in which manner LLMs shape responses to ungendered prompts, contributing to biased outputs. This
analysis uses a structured experimental method, giving different prompts involving three different
professional job combinations, which are also characterized by a hierarchical relationship. This study
uses Italian, a language with extensive grammatical gender differences, to highlight potential
Limitations in current LLMs’ ability to generate objective text in non-English languages. Two popular
LLM-based chatbots are examined, namely OpenAI ChatGPT (gpt-40-mini) and Google Gemini (gemini-1.5-
flash). Through APIs, we collected a range of 3600 responses. The results highlight how content
generated by LLMs can perpetuate stereotypes. For example, Gemini associated 100% (ChatGPT 97%) of
“she’ pronouns to the ‘assistant’ rather than the ‘manager’. The presence of bias in Al-generated text
can have significant implications in many fields, such as in the workplaces or in job selections,
raising ethical concerns about its use. Understanding these risks is pivotal to developing mitigation
strategies and assuring that Al-based systems do not increase social inequalities, but rather
contribute to more equitable outcomes. Future research directions include expanding the study to
additional chatbots or languages, refining prompt engineering methods or further exploiting a larger
Computer-using agents have shown strong potential to boost human
productivity and enable new application forms across platforms. While recent advances have led to
usable applications, existing benchmarks fail to account for the internal task heterogeneity and the
corresponding agent capabilities, as well as their alignment with actual user demands-hindering both
targeted capability development and the reliable transition of research progress into practical
deployment. To bridge the gap, we present OS-MAP, a benchmark for daily computer-using automation that
organizes its 416 realistic tasks across 15 applications along two key dimensions: a five-level
‘taxonomy of automation and a generalization scope derived from a real-world user demand hierarchy. To
enable fine-grained analysis of required capabilities and alignment with real-world scenarios, OS-HAP
evaluates agents along tuo dimensions: automation level across a five-level taxonomy, and
generalization scope across a demand hierarchy. This design captures varying levels of required agent
‘autonomy and generalization, forming @ performance-generalization evaluation matrix for structured and
comprehensive assessment. Experiments show that even State-of-the-Art agents with VLM backbones
struggle with higher-level tasks involving perception, reasoning, and coordination-highlighting the
need for a deeper understanding of current strengths and limitations to drive the future progress in
computer-using agents research and deployment. All code, environments, baselines, and data are
Interpretation of topics is crucial for their downstream
applications. State-of-the-art evaluation measures of topic quality such as coherence and word
intrusion do not measure how much a topic facilitates the exploration of a corpus. To design
evaluation measures grounded on a task, and a population of users, we do user studies to understand
how users interpret topics. We propose constructs of topic quality and ask users to assess them in the
context of a topic and provide rationale behind evaluations. We use reflexive thematic analysis to
identify themes of topic interpretations from rationales. Users interpret topics based on availability
and representativeness heuristics rather than probability. We propose a theory of topic interpretation
based on the anchoring-and-adjustment heuristic: users anchor on salient words and make semantic
adjustments to arrive at an interpretation. Topic interpretation can be viewed as making a judgment
under uncertainty by an ecologically rational user, and hence cognitive biases aware user models and
evaluation frameworks are needed.
Retrieval-augnented generation (RAG) enhances large language
models (LLMs) by incorporating retrieved information. Standard retrieval process prioritized
relevance, focusing on topical alignnent betueen queries and passages. In contrast, in RAG, the
emphasis has shifted to utility, which considers the usefulness of passages for generating accurate
answers. Despite empirical evidence showing the benefits of utility-based retrieval in RAG, the high
computational cost of using LLMs for utility judgments limits the number of passages evaluated. This
restriction is problematic for complex queries requiring extensive information. To address this, we
propose a method to distill the utility judgment capabilities of LLMs into smaller, more efficient
models. Our approach focuses on utility-based selection rather than ranking, enabling dynamic passage
selection tailored to specific queries without the need for fixed thresholds. We train student models
to learn pseudo-answer generation and utility judgments from teacher LLMs, using a sliding window
method that dynamically selects useful passages. Our experiments demonstrate that utility-based
selection provides a flexible and cost-effective solution for RAG, significantly reducing
Computational costs while improving answer quality. We present the distillation results using Quen3-
328 as the teacher model for both relevance ranking and utility-based selection, distilled into
RankQuen1.78 and UtilityQuen1.78. Our findings indicate that for complex questions, utility-based
selection is more effective than relevance ranking in enhancing answer generation performance. We will
release the relevance ranking and utility-based selection annotations for the HS MARCO dataset,
supporting further research in this area.
Claim verification is critical for enhancing digital literacy. However, the state-of-
‘the-art single-LLM methods struggle with complex claim verification that involves multi-faceted
evidences. Inspired by real-world fact-checking practices, we propose DebateCV, the first claim
verification framework that adopts  debate-driven methodology using multiple LLM agents. In our
framework, two Debaters take opposing stances on a claim and engage in multi-round argumentation,
while a Moderator evaluates the arguments and renders a verdict with justifications. To further
improve the performance of the Moderator, we introduce a novel post-training strategy that leverages
synthetic debate data generated by the zero-shot DebateCV, effectively addressing the scarcity of
real-world debate-driven claim verification data. Experimental results show that our method
outperforms existing claim verification methods under varying levels of evidence quality. Our code and
dataset are publicly available at this https URL.
Argument summarization aims to generate concise, structured
representations of complex, multi-perspective debates. While recent work has advanced the
identification and clustering of argumentative components, the generation stage remains underexplored.
Existing approaches typically rely on single-pass generation, offering limited support for factual
correction or structural refinement. To address this gap, we introduce Arg-LLaDA, a novel large
Language diffusion framework that iteratively improves summaries via sufficiency-guided remasking and
regeneration. Our method combines a flexible masking controller with a sufficiency-checking module to
identify and revise unsupported, redundant, or incomplete spans, yielding more faithful, concise, and
coherent outputs. Empirical results on two benchmark datasets demonstrate that Arg-LLaDA surpasses
state-of-the-art baselines in 7 out of 10 automatic evaluation metrics. In addition, human evaluations
reveal substantial improvements across core dimensions, coverage, faithfulness, and conciseness,
validating the effectiveness of our iterative, sufficiency-aware generation strategy.
We introduce PurpCode, the first post-training recipe for
‘training safe code reasoning models towards generating secure code and defending against malicious
cyberactivities. PurpCode trains a reasoning model in two stages: (i) Rule Learning, which explicitly
teaches the model to reference cybersafety rules to generate vulnerability-free code and to avoid
facilitating malicious cyberactivities; and (ii) Reinforcement Learning, which optimizes model safety
and preserves model utility through diverse, multi-objective reward mechanisms. To empower the
‘training pipelines with comprehensive cybersafety data, we conduct internal red-teaming to synthesize
comprehensive and high-coverage prompts based on real-world tasks for inducing unsafe cyberactivities
in the model. Based on PurpCode, we develop a reasoning-based coding model, namely PurpCode-328, which
demonstrates state-of-the-art cybersafety, outperforming various frontier models. Meanwhile, our
alignment method decreases the model overrefusal rates in both general and cybersafety-specific
scenarios, while preserving model utility in both code generation and common security knowledge.
tixed modality search -- retrieving information across a
heterogeneous corpus composed of images, texts, and multimodal documents -- is an important yet
underexplored real-world application. In this work, we investigate how contrastive vision-lenguage
models, such as CLIP, perform on the mixed modality search task. Our analysis reveals a critical
Limitation: these models exhibit a pronounced modality gap in the embedding space, where image and
‘text embeddings form distinct clusters, leading to intra-modal ranking bias and inter-modal fusion
failure. To address this issue, we propose GR-CLIP, a lightweight post-hoc calibration method that
removes the modality gap in CLIP's embedding space. Evaluated on MixBench -- the first benchmark
specifically designed for mixed modality search -- GR-CLIP improves NDCG@10 by up to 26 percentage
points over CLIP, surpasses recent vision-language generative embedding models by 4 percentage points,
while using 75x less compute.
human-machine interactions by allowing real-time user interruptions and backchanneling, compared to
‘traditional SDs that rely on turn-taking. However, existing benchmarks lack metrics for FD scenes,
2.g., evaluating model performance during user interruptions. In this paper, we present a
comprehensive FD benchmarking pipeline utilizing LLMs, TTS, and ASR to address this gap. It assesses
FDSDS's ability to handle user interruptions, manage delays, and maintain robustness in challenging
scenarios with diverse novel metrics. We applied our benchmark to three open-source FDSDS (Moshi,
Freeze-omni, and VITA-1.5) using over 40 hours of generated speech, with 293 simulated conversations
and 1,200 interruptions. The results show that all models continue to face challenges, such as failing
‘to respond to user interruptions, under frequent disruptions and noisy conditions. Demonstrations,
data, and code will be released.
‘opened new possibilities for unified modeling of speech, text, images, and other modalities. Building
‘on our prior work, this paper examines the conditions and model architectures under which multiple
input modalities Can improve automatic speech recognition (ASR) accuracy in noisy environment:
Through experiments on synthetic and real-world data, we find that (1) harnessing more modalities
usually improves ASR accuracy, as each modality provides complementary information, but the
improvement depends on the amount of auditory noise. (2) Synchronized modalities (¢.g., lip movements)
‘are more useful at high noise levels whereas unsynchronized modalities (e.g., image context) are most
helpful at moderate noise levels. (3) Higher-quality visual representations consistently improve ASR
accuracy, highlighting the importance of developing more powerful visual encoders. (4) Mamba exhibits
similar trends regarding the benefits of multimodality as do Transformers. (5) The input order of
modalities as well as their weights in the loss function can significantly impact accuracy. These
findings both offer practical insights and help to deepen our understanding of multi-modal speech
recognition under challenging conditions.
The quality of a conversation goes beyond the individual quality
of each reply, and instead emerges from how these combine into interactional patterns that give the
conversation its distinctive overall "shape". However, there is no robust automated method for
comparing conversations in terms of their overall interactional dynamics. Such methods could enhance
‘the analysis of conversational data and help evaluate conversational agents more holistically.
In this work, we introduce a similarity measure for comparing conversations with respect to their
dynamics. We design a validation framework for testing the robustness of the metric in capturing
differences in conversation dynamics and for assessing its sensitivity to the topic of the
conversations. Finally, to illustrate the measure’s utility, we use it to analyze conversational
dynamics in a large online community, bringing new insights into the role of situational power in
‘towards improving judicial efficiency through the automation of key information detection. Our
approach leverages state-of-the-art natural language processing techniques to meticulously identify
and extract essential data from extensive legal texts, which facilitates a more efficient review
process. By employing advanced machine learning algorithms, the framework recognizes underlying
patterns within judicial documents to create precise summaries that encapsulate the crucial elements.
‘This automation alleviates the burden on legal professionals, concurrently reducing the likelihood of
overlooking vital information that could lead to errors. Through comprehensive experiments conducted
with actual legal datasets, we demonstrate the capability of our method to generate high-quality
summaries while preserving the integrity of the original content and enhancing processing times
considerably. The results reveal marked improvements in operational efficiency, allowing legal
practitioners to direct their efforts toward critical analytical and decision-making activities
instead of manual reviews. This research highlights promising technology-driven strategies that can
significantly alter workflow dynamics within the legal sector, emphasizing the role of automation in
Large language models (LLMs) are revolutionizing the field of
‘education by enabling personalized learning experiences tailored to individual student needs. In this
paper, we introduce a framework for Adaptive Learning Systems that leverages LLM-powered analytics for
personalized curriculum design. This innovative approach uses advanced machine learning to analyze
real-time data, allowing the system to adapt learning pathways and recommend resources that align with
each learner's progress. By continuously assessing students, our framework enhances instructional
strategies, ensuring that the materials presented are relevant and engaging. Experimental results
indicate marked improvement in both learner engagement and knowledge retention when using @
customized curriculum. Evaluations conducted across varied educational environments demonstrate the
framework's flexibility and positive influence on learning outcomes, potentially reshaping
conventional educational practices into a more adaptive and student-centered model.
Efficiently navigating and understanding academic papers is crucial for scientific
progress. Traditional linear formats like PDF and HTML can cause cognitive overload and obscure a
paper's hierarchical structure, making it difficult to locate key information. while LLM-based
chatbots offer summarization, they often lack nuanced understanding of specific sections, may produce
unreliable information, and typically discard the document's navigational structure. Drawing insights
from a formative study on academic reading practices, we introduce TreeReader, a novel language model-
augmented paper reader. TreeReader decomposes papers into an interactive tree structure where each
section is initially represented by an LLi-generated concise summary, with underlying details
accessible on demand. This design allows users to quickly grasp core ideas, selectively explore
sections of interest, and verify summaries against the source text. A user study was conducted to
evaluate TreeReader's impact on reading efficiency and comprehension. TreeReader provides a more
focused and efficient way to navigate and understand complex academic literature by bridging
hierarchical summarization with interactive exploration.
Multimodal Machine Translation (MIT) enhances translation quality
by incorporating visual context, helping to resolve textual ambiguities. While existing !IT methods
perform well in bilingual settings, extending them to multilingual translation remains challenging due
‘to cross-lingual interference and ineffective parameter-sharing strategies. To address this, we
Propose LLaVA-NeulfT, a novel multimodal multilingual translation framework that explicitly models
Language-specific and language-agnostic representations to mitigate multilingual interference. Our
approach consists of a layer selection mechanism that identifies the most informative layers for
different language pairs and a neuron-level adaptation strategy that dynamically selects language-
specific and agnostic neurons to improve translation quality while reducing redundancy. We conduct
extensive experiments on the M3-Multi30K and M3-AmbigCaps datasets, demonstrating that LLaVA-NeulT,
while fine-tuning only 40\% of the model parameters, surpasses full fine-tuning approaches and
ultimately achieves SOTA results on both datasets. Our analysis further provides insights into the
importance of selected layers and neurons in multimodal multilingual adaptation, offering an efficient
and scalable solution to cross-lingual adaptation in multimodal translation.
Environmental, Social, and Governance (ESG) reports are essential
for evaluating sustainability practices, ensuring regulatory compliance, and promoting financial
‘transparency. However, these documents are often lengthy, structurally diverse, and multimodal,
comprising dense text, structured tables, complex figures, and layout-dependent semantics. Existing AT
systems often struggle to perform reliable document-level reasoning in such settings, and no dedicated
benchmark currently exists in ESG domain. To fill the gap, we introduce \textbf{MMESGBench}, a first
of-its-kind benchmark dataset targeted to evaluate multimodal understanding and complex reasoning
across structurally diverse and multi-source ESG documents. This dataset is constructed via @ human-AT
pairs by jointly interpreting rich textual, tabular, and visual information from layout-aware document
pages. Second, an LLM verifies the semantic accuracy, completeness, and reasoning complexity of each
QA pair. This automated process is followed by an expert-in-the-loop validation, where domain
specialists validate and calibrate QA pairs to ensure quality, relevance, and diversity. MMESGBench
comprises 933 validated QA pairs derived from 45 ESG documents, spanning across seven distinct
document types and three major ESG source categories. Questions are categorized as single-page, cross-
page, or unanswerable, with each accompanied by fine-grained multimodal evidence. Initial experiments
validate that multimodal and retrieval-augmented models substantially outperform text-only baselines,
particularly on visually grounded and cross-page tasks. MMESGBench is publicly available as an open-
source dataset at this https URL.
Linguistic generalization, yet medium to low resource languages underperform on common benchmarks such
as ARC-Challenge, MILU, and HellaSwag. We analyze activation patterns in Genma-2-28 across all 26
residual layers and 10 languages: Chinese (zh), Russian (ru), Spanish (es), Italian (it), medium to
Hindi (hi), with English (en) as the reference. Using Sparse Autoencoders (SAEs), we reveal systematic
disparities in activation patterns. Medium to low resource languages receive up to 26.27 percent lower
activations in early layers, with a persistent gap of 19.89 percent in deeper layers. To address this,
we apply activation-aware fine-tuning via Low-Rank Adaptation (LoRA), leading to substantial
activation gains, such as 87.69 percent for Malayalam and 86.32 percent for Hindi, while maintaining
English retention at approximately 91 percent. After fine-tuning, benchmark results show modest but
consistent improvements, highlighting activation alignment as a key factor in enhancing multilingual
LLM performance.
Understanding another person's creative output requires a shared
Language of association. However, when training vision-language models such as CLIP, we rely on web-
scraped datasets containing short, predominantly literal, alt-text. In this work, we introduce a
method for mining contextualized associations for salient visual elements in an image that can scale
to any unlabeled dataset. Given an image, we can use these mined associations to generate high quality
creative captions at increasing degrees of abstraction. With our method, we produce a new dataset of
visual associations and 1.7m creative captions for the images in MSCOCO. Human evaluation confirms
‘that these captions remain visually grounded while exhibiting recognizably increasing abstraction.
Moreover, fine-tuning 2 visual encoder on this dataset yields meaningful improvements in zero-shot
image-text retrieval in two creative domains: poetry and metaphor visualization. We release our
dataset, our generation code and our models for use by the broader community.
Retrieval-Augmented Generation (RAG) represents @ major
advancement in natural language processing (NLP), combining large language models (LLMs) with
information retrieval systems to enhance factual grounding, accuracy, and contextual relevance. This
paper presents a comprehensive systematic review of RAG, tracing its evolution from early developments
in open domain question answering to recent state-of-the-art implementations across diverse
applications. The review begins by outlining the motivations behind RAG, particularly its ability to
mitigate hallucinations and outdated knowledge in parametric models. Core technical components-
retrieval mechanisms, sequence-to-sequence generation models, and fusion strategies are examined in
detail. A year-by-year analysis highlights key milestones and research trends, providing insight into
RAG's rapid growth. The paper further explores the deployment of RAG in enterprise systems, addressing
practical challenges related to retrieval of proprietary data, security, and scalability. A
comparative evaluation of RAG implementations is conducted, benchmarking performance on retrieval
accuracy, generation fluency, latency, and computational efficiency. Persistent challenges such as
retrieval quality, privacy concerns, and integration overhead are critically assessed. Finally, the
review highlights emerging solutions, including hybrid retrieval approaches, privacy-preserving
techniques, optimized fusion strategies, and agentic RAG architectures. These innovations point toward
a future of more reliable, efficient, and context-aware knowledge-intensive NLP systems.
Millions of patients are already using large language model (LLM) chatbots for
medical advice on a regular basis, raising patient safety concerns. This physician-led red-teaming
study compares the safety of four publicly available chatbots--Claude by Anthropic, Gemini by Google,
GPT-40 by OpenAI, and Llama3-708 by Meta--on a new dataset, HealthAdvice, using an evaluation
framework that enables quantitative and qualitative analysis. In total, 888 chatbot responses are
evaluated for 222 patient-posed advice-seeking medical questions on primary care topics spanning
internal medicine, women's health, and pediatrics. We find statistically significant differences
between chatbots. The rate of problematic responses varies from 21.6 percent (Claude) to 43.2 percent
(Llama), with unsafe responses varying from 5 percent (Claude) to 13 percent (GPT-40, Llama).
Qualitative results reveal chatbot responses with the potential to lead to serious patient harm. This
study suggests that millions of patients could be receiving unsafe medical advice from publicly
available chatbots, and further work is needed to improve the clinical safety of these powerful tools.
There are more than 7,000 languages around the world, and current
Large Language Models (LLMs) only support hundreds of languages. Dictionary-based prompting methods
can enhance translation on them, but most methods use all the available dictionaries, which could be
expensive. Instead, it will be flexible to have a trade-off between token consumption and translation
performance. This paper proposes a novel task called \textbf{A}utomatic \textbf{D}ictionary
\textbf{S}election (\textbf{ADS}). The goal of the task is to automatically select which dictionary to
use to enhance translation. We propose a novel and effective method which we call \textbf{S}elect
\textbf{Lo}u-frequency \textbf{i}ords! (\textbf{SLoll}) which selects those dictionaries that have a
lower frequency. Our methods have unique advantages. First, there is no need for access to the
‘training data for frequency estimation (which is usually unavailable). Second, it inherits the
advantage of dictionary-based methods, where no additional tuning is required on LLMs. Experimental
results on 10@ languages from FLORES indicate that SLoW surpasses strong baselines, and it can
obviously save token usage, with many languages even surpassing the translation performance of the
full dictionary baseline. \footnote{A shocking fact is that there is no need to use the actual training
data (often unobtainable) for frequency estimation, and an estimation frequency obtained using public
resources is still apparently effective in improving translation with ChatGPT and Llama, and
DeepSeek.}\footnote{Code and data available upon publication. }
Assessing the reproducibility of social science papers is
essential for promoting rigor in research processes, but manual assessment is costly. With recent
advances in agentic AI systems (i.e., AI agents), we seek to evaluate their capability to automate
‘this process. However, existing benchmarks for reproducing research papers (1) focus solely on
reproducing results using provided code and data without assessing their consistency with the paper,
(2) oversimplify real-world scenarios, and (3) lack necessary diversity in data formats and
programming languages. To address these issues, we introduce REPRO-Bench, a collection of 112 task
instances, each representing 2 social science paper with a publicly available reproduction report. The
agents are tasked with assessing the reproducibility of the paper based on the original paper PDF and
‘the corresponding reproduction package. REPRO-Bench features end-to-end evaluation tasks on the
reproducibility of social science papers with complexity comparable to real-world assessments. We
evaluate three representative AI agents on REPRO-Bench, with the best-performing agent achieving an
accuracy of only 21.4%. Building on our empirical analysis, we develop REPRO-Agent, which improves the
highest accuracy achieved by existing agents by 71%. We conclude that more advanced AI agents should
be developed to automate real-world reproducibility assessment. REPRO-Bench is publicly available at
‘this https URL.
NLP models often rely on human-labeled data for training and
evaluation. Many approaches crowdsource this data from a large number of annotators with varying
skills, backgrounds, and motivations, resulting in conflicting annotations. These conflicts have
‘traditionally been resolved by aggregation methods that assume disagreements are errors. Recent work
has argued that for many tasks annotators may have genuine disagreements and that variation should be
‘treated as signal rather than noise. However, few models separate signal and noise in annotator
disagreement. In this work, we introduce NUTMEG, a new Bayesian model that incorporates information
about annotator backgrounds to remove noisy annotations from human-labeled training data while
preserving systematic disagreements. Using synthetic data, we show that NUTHEG is more effective at
recovering ground-truth from annotations with systematic disagreement than traditional aggregation
methods. We provide further analysis characterizing how differences in subpopulation sizes, rates of
disagreement, and rates of spam affect the performance of our model. Finally, we demonstrate that
downstream models trained on NUTMEG-aggregated data significantly outperform models trained on data
from traditionally aggregation methods. Our results highlight the importance of accounting for both
annotator competence and systematic disagreements when training on human-labeled data.
High-quality dialogue is crucial for e-commerce customer service,
yet traditional intent-based systems struggle with dynamic, multi-turn interactions. We present
MindFlowt, a self-evolving dialogue agent that learns domain-specific behavior by combining large
Language models (LLMs) with imitation learning and offline reinforcement learning (RL). MindFlow+
introduces two data-centric mechanisms to guide learning: tool-augmented demonstration construction,
which exposes the model to knowledge-enhanced and agentic (ReAct-style) interactions for effective
‘tool use; and reward-conditioned data modeling, which aligns responses with task-specific goals using
reward signals. To evaluate the model's role in response generation, we introduce the AI Contribution
Ratio, a novel metric quantifying AI involvement in dialogue. Experiments on real-world e-commerce
conversations show that MindFlow+ outperforms strong baselines in contextual relevance, flexibility,
and task accuracy. These results demonstrate the potential of combining LLMs tool reasoning, and
reward-guided learning to build domain-specialized, context-aware dialogue systems.
Visual Automatic Speech Recognition (V-ASR) is @ challenging task
‘that involves interpreting spoken language solely from visual information, such as lip movements and
facial expressions. This task is notably challenging due to the absence of auditory cues and the
visual ambiguity of phonemes that exhibit similar visemes-distinct sounds that appear identical in lip
motions. Existing methods often aim to predict words or characters directly from visual cues, but they
conmonly suffer from high error rates due to viseme ambiguity and require large amounts of pre-
‘training data. We propose a novel phoneme-based two-stage framework that fuses visual and landmark
motion features, followed by an LLM model for word reconstruction to address these challenges. Stage 1
consists of V-ASR, which outputs the predicted phonemes, thereby reducing training complexity.
Meanwhile, the facial landmark features address speaker-specific facial characteristics. Stage 2
comprises an encoder-decoder LLM model, NLLB, that reconstructs the output phonemes back to words.
Besides using a large visual dataset for deep learning fine-tuning, our PV-ASR method demonstrates
superior performance by achieving 17.4% WER on the LRS2 and 21.0% WER on the LRS3 dataset.
From: Matthew Kit Khinn Teng Mr [view email][vi] Fri, 25 Jul 2025 00:38:39 UTC (Bel KB)
Students across the world in STEM classes, especially in the
Global South, fall behind their peers who are more fluent in English, despite being at par with them
in terms of Scientific prerequisites. While many of them are able to follow everyday English at ease,
key terms in English stay challenging. In most cases, such students have had most of their course
prerequisites in a lower resource language. Live speech translation to lower resource languages is a
promising area of research, however, models for speech translation can be too expensive on a large
scale and often struggle with technical content. In this paper, we describe CueBuddy, which aims to
remediate these issues by providing real-time “lexical cues” through technical keyword spotting along
real-time multilingual glossary lookup to help students stay up to speed with complex English jargon
without disrupting their concentration on the lecture. We also describe the limitations and future
‘extensions of our approach.
Code-mixing, the practice of switching between languages within a
benchmarks, such as LinCE and GLUECOS, are limited by narrow language pairings and tasks, failing to
adequately evaluate the code-mixing capabilities of large language models (LLMs). Despite the
significance of code-mixing for multilingual users, research on LLMs in this context remains limited.
Additionally, current methods for generating code-mixed data are underdeveloped. In this paper, we
conduct @ comprehensive evaluation of LLMs’ performance on code-mixed data across 18 languages from
seven language families. We also propose a novel approach for generating synthetic code-mixed texts by
combining word substitution with GPT-4 prompting. Our analysis reveals consistent underperformance of
LLMs on code-mixed datasets involving multiple language families. We suggest that improvements in
‘training data size, model scale, and few-shot learning could enhance their performance.
In this work, we introduce our solution for the Multilingual Text
Detoxification Task in the PAN-2025 competition for the ylmmcl team: @ robust multilingual text
detoxification pipeline that integrates lexicon-guided tagging, a fine-tuned sequence-to-sequence
model (s-nlp/mt0-x1-detox-orpo) and an iterative classifier-based gatekeeping mechanism. Our approach
departs from prior unsupervised or monolingual pipelines by leveraging explicit toxic word annotation
via the multilingual_toxic_lexicon to guide detoxification with greater precision and cross-lingual
generalization. Our final model achieves the highest STA (0.922) from our previous attempts, and an
average official score of 0.612 for toxic inputs in both the development and test sets. It also
achieved xCOMET scores of @.793 (dev) and 0.787 (test). This
backtranslation methods across multiple languages, and shows
settings (English, Russian, French). Despite some trade-offs
improvements in detoxification strength. In the competition,
score of 0.612.
In natural language processing, multilingual models like mBERT and XLM-ROBERTa
promise broad coverage but often struggle with languages that share a script yet differ in
‘orthographic norms and cultural context. This issue is especially notable in Arabic-script languages
such as Kurdish Sorani, Arabic, Persian, and Urdu. ble introduce the Arabic Script RoBERTa (AS-ROBERTa)
family: four RoBERTa-based models, each pre-trained on a large corpus tailored to its specific
language. By focusing pre-training on language-specific script features and statistics, our models
capture patterns overlooked by general-purpose models. When fine-tuned on classification tasks, AS-
RoBERTa variants outperform mBERT and XLM-RoBERTa by 2 to 5 percentage points. An ablation study
confirms that script-focused pre-training is central to these gains. Error analysis using confusion
matrices shows how shared script traits and domain-specific content affect performance. Our results
highlight the value of script-aware specialization for languages using the Arabic script and support
further work on pre-training strategies rooted in script and language specificity.
From: Abdulhady Abdullah [view email][v1] Thu, 24 Jul 2025 19:28:33 UTC (1,066 KB)
Language models (IMs) are susceptible to in-context reward
hacking, where they exploit flaws in tainted or faulty written specifications or rubrics to achieve
high scores without fulfilling the user's true intent. We introduce Specification Self-Correction
(SSC), a novel, test-time framework that enables an LM to identify and correct flaws within its own
guiding specification. SSC employs a multi-step inference process where the model first generates a
Fesponse based on a potentially tainted specification, critiques its output, and then revises the
specification itself to remove the exploitable loophole. A final, more robust response is then
generated using this self-corrected specification. Across experiments spanning creative writing and
agentic coding tasks with several Lis, we demonstrate that while models initially game tainted
specifications in 50-70\X of cases, the SSC process reduces this vulnerability by over 90\%. This
dynamic repair occurs at inference time, requires no weight modification, and leads to more robustly
aligned model behavior. Code at this https URL .
Language models must be adapted to understand and follow user
instructions. Reinforcement learning is widely used to facilitate this -- typically using fixed
criteria such as "helpfulness" and "harmfulness”. In our work, we instead propose using flexible,
instruction-specific criteria as a means of broadening the impact that reinforcement learning can have
in eliciting instruction following. We propose "Reinforcement Learning from Checklist Feedback”
(RLCF). From instructions, we extract checklists and evaluate how well responses satisfy each item -
using both AI judges and specialized verifier programs - then combine these scores to compute rewards
for RL. We compare RLCF with other alignment methods applied to a strong instruction following model
(Quen2.5-78-Instruct) on five widely-studied benchmarks -- RLCF is the only method to improve
performance on every benchmark, including @ 4-point boost in hard satisfaction rate on FollowBench, a
6-point increase on InFoBench, and a 3-point rise in win rate on Arena-Hard. These results establish
checklist feedback as a key tool for improving language models’ support of queries that express a
multitude of needs.
Prompt optimization improves the reasoning abilities of large
Language models (LLMs) without requiring parameter updates to the target model. Following heuristic-
based "Think step by step” approaches, the field has evolved in tuo main directions: while one group
of methods uses textual feedback to elicit improved prompts from general-purpose LLMs in a training-
free way, @ concurrent line of research relies on numerical rewards to train a special prompt model,
tailored for providing optimal prompts to the target model. In this paper, we introduce the Textual
Reward Prompt framework (TRPrompt), which unifies these approaches by directly incorporating textual
feedback into training of the prompt model. Our framework does not require prior dataset collection
and is being iteratively improved with the feedback on the generated prompts. lihen coupled with the
capacity of an LLM to internalize the notion of what a “good” prompt is, the high-resolution signal
provided by the textual rewards allows us to train a prompt model yielding state-of-the-art query-
specific prompts for the problems from the challenging math datasets GsllHard and MATH.
Zero-shot Image Captioning (ZIC) increasingly utilizes synthetic
datasets generated by text-to-image (T2I) models to mitigate the need for costly manual annotation.
However, these T2I models often produce images that exhibit semantic misalignments with their
corresponding input captions (e.g., missing objects, incorrect attributes), resulting in noisy
synthetic image-caption pairs that can hinder model training. Existing dataset pruning techniques are
largely designed for removing noisy text in web-crawled data. However, these methods are ill-suited
for the distinct challenges of synthetic data, where captions are typically well-formed, but images
may be inaccurate representations. To address this gap, we introduce SynC, a novel framework
specifically designed to refine synthetic image-caption datasets for ZIC. Instead of conventional
filtering or regeneration, SynC focuses on reassigning captions to the most semantically aligned
images already present within the synthetic image pool. Our approach employs @ one-to-many mapping
strategy by initially retrieving multiple relevant candidate images for each caption. We then apply a
cycle-consistency-inspired alignment scorer that selects the best image by verifying its ability to
retrieve the original caption via image-to-text retrieval. Extensive evaluations demonstrate that SynC
consistently and significantly improves performance across various ZIC models on standard benchmarks
(WS-COCO, Flickr30k, NoCaps), achieving state-of-the-art results in several scenarios. SynC offers an
effective strategy for curating refined synthetic data to enhance ZIC.
Despite the impressive performance of large language models
(LuMs) in general domains, they often underperform in specialized domains. Existing approaches
typically rely on data synthesis methods and yield promising results by using unlabeled data to
capture domain-specific features. However, these methods either incur high computational costs or
suffer from performance limitations, while also demonstrating insufficient generalization across
different tasks. To address these challenges, we propose AQuilt, a framework for constructing
instruction-tuning data for any specialized domains from corresponding unlabeled data, including
Answer, Question, Unlabeled data, Inspection, Logic, and Task type. By incorporating logic and
inspection, we encourage reasoning processes and self-inspection to enhance model performance.
Moreover, customizable task instructions enable high-quality data generation for any task. As a
result, we construct a dataset of 703k examples to train a powerful data synthesis model. Experiments
show that AQuilt is comparable to DeepSeek-V3 while utilizing just 17% of the production cost. Further
analysis demonstrates that our generated data exhibits higher relevance to downstream tasks. Source
code, models, and scripts are available at this https URL.
Electronic Health Records (EHRs) are pivotal in clinical
practices, yet their retrieval remains a challenge mainly due to semantic gap issues. Recent
advancements in dense retrieval offer promising solutions but existing models, both general-domain and
biomedical-domain, fall short due to insufficient medical knowledge or mismatched training corpora.
This paper introduces \texttt{this http URL}, a series of dense retrieval models specifically tailored
for EHR retrieval. We propose a two-stage training pipeline utilizing MIMIC-IV discharge summaries to
address the need for extensive medical knowledge and large-scale training data. The first stage
involves medical entity extraction and knowledge injection from a biomedical knowledge graph, while
‘the second stage employs large language models to generate diverse training data. We train tuo
variants of \texttt{this http URL}, with 110M and 78 parameters, respectively. Evaluated on the CliniQ
benchmark, our models significantly outperforms all existing dense retrievers, achieving state-of-the-
art results. Detailed analyses confirm our models’ superiority across various match and query types,
particularly in challenging semantic matches like implication and abbreviation. Ablation studies
validate the effectiveness of each pipeline component, and supplementary experiments on EHR QA
datasets demonstrate the models’ generalizability on natural language questions, including complex
ones with multiple entities. This work significantly advances EHR retrieval, offering a robust
solution for clinical applications.
This paper presents our system for CCL25-Eval Task 10, addressing
Fine-Grained Chinese Hate Speech Recognition (FGCHSR). We propose a novel SRAG-MAV framework that
synergistically integrates task reformulation(TR), Self-Retrieval-Augmented Generation (SRAG), and
Multi-Round Accumulative Voting (MAV). Our method reformulates the quadruplet extraction task into
triplet extraction, uses dynamic retrieval from the training set to create contextual prompts, and
applies multi-round inference with voting to improve output stability and performance. Our system,
based on the Quen2.5-78 model, achieves a Hard Score of 26.66, a Soft Score of 48.35, and an Average
Score of 37.505 on the STATE ToxiCN dataset, significantly outperforming baselines such as GPT-40
(Average Score 15.63) and fine-tuned Quen2.5-78 (Average Score 35.365). The code is available at this
https URL.
Diffusion Large Language Models (DLLMs) have emerged as a compelling alternative to
Autoregressive models, designed for fast parallel generation. However, existing DLLMs are plagued by a
severe quality-speed trade-off, where faster parallel decoding leads to significant performance
degradation. We attribute this to the irreversibility of standard decoding in DLLMs, which is easily
polarized into the wrong decoding direction along with early error context accumulation. To resolve
‘this, we introduce Wide-In, Narrow-Out (WINO), a training-free decoding algorithm that enables
revokable decoding in DLLMS. WINO employs a parallel draft-and-verify mechanism, aggressively drafting
multiple tokens while simultaneously using the model's bidirectional context to verify and re-mask
suspicious ones for refinement. Verified in open-source DLLMs like LLaDA and MlaDA, WINO is shown to
decisively improve the quality-speed trade-off. For instance, on the GSM8K math benchmark, it
accelerates inference by 6§\times$ while improving accuracy by 2.58%; on Flickr30K captioning, it
achieves 2 10$\times$ speedup with higher performance. More comprehensive experiments are conducted to
demonstrate the superiority and provide an in-depth understanding of WINO.
Poster designing can benefit from synchronous feedback from
target audiences. However, gathering audiences with diverse perspectives and reconciling them on
design edits can be challenging. Recent generative AI models present opportunities to simulate human-
like interactions, but it is unclear how they may be used for feedback processes in design. We
introduce Postertlate, a poster design assistant that facilitates collaboration by creating audience-
driven persona agents constructed from marketing documents. Posterate gathers feedback from each
persona agent regarding poster components, and stimulates discussion with the help of a moderator to
reach a conclusion. These agreed-upon edits can then be directly integrated into the poster design.
Through our user study (N-12), we identified the potential of Postertate to capture overlooked
viewpoints, while serving as an effective prototyping tool. Additionally, our controlled online
evaluation (N=100) revealed that the feedback from an individual persona agent is appropriate given
its persona identity, and the discussion effectively synthesizes the different persona agents’
perspectives.
This paper presents a novel hybrid tokenization strategy that
enhances the performance of DNA Language Models (DLIMs) by combining 6-mer tokenization with Byte Pair
Encoding (BPE-600). Traditional k-mer tokenization is effective at capturing local DNA sequence
structures but often faces challenges, including uneven token distribution and a limited understanding
of global sequence context. To address these limitations, we propose merging unique 6mer tokens with
optimally selected BPE tokens generated through 600 BPE cycles. This hybrid approach ensures a
balanced and context-aware vocabulary, enabling the model to capture both short and long patterns
within DNA sequences simultaneously. A foundational DLM trained on this hybrid vocabulary was
evaluated using next-k-mer prediction as a fine-tuning task, demonstrating significantly improved
performance. The model achieved prediction accuracies of 10.78% for 3-mers, 10.1% for 4-mers, and
4.12% for S-mers, outperforming state-of-the-art models such as NT, DNABERT2, and GROVER. These
results highlight the ability of the hybrid tokenization strategy to preserve both the local sequence
structure and global contextual information in DNA modeling. This work underscores the importance of
advanced tokenization methods in genomic language modeling and lays a robust foundation for future
applications in downstream DNA sequence analysis and biological research.
From: Md Hasibur Rahman [view email][vi] Thu, 24 Jul 2025 16:45:23 UTC (4,324 KB)
Multimodal Machine Translation (IIT) has demonstrated the significant help of visual
information in machine translation. However, existing MPIT methods face challenges in leveraging the
modality gap by enforcing rigid visual-linguistic alignment whilst being confined to inference within
‘their trained multimodal domains. In this work, we construct novel multimodal scene graphs to preserve
and integrate modality-specific information and introduce GIIFT, a two-stage Graph-guided Inductive
Image-Free MNT framework that uses a cross-modal Graph Attention Network adapter to learn multimodal
knowledge in a unified fused space and inductively generalize it to broader image-free translation
domains. Experimental results on the Multi3ek dataset of English-to-French and English-to-German tasks
demonstrate that our GIIFT surpasses existing approaches and achieves the state-of-the-art, even
without images during inference. Results on the WHT benchmark show significant improvements over the
image-free translation baselines, demonstrating the strength of GIIFT towards inductive image-free
inference.
Information extraction (IE) is fundamental to numerous NLP
applications, yet existing solutions often require specialized models for different tasks or rely on
computationally expensive large language models. We present GLNER2, a unified framework that enhances
‘the original GLINER architecture to support named entity recognition, text classification, and
hierarchical structured data extraction within a single efficient model. Built pretrained transformer
encoder architecture, GLiNER2 maintains CPU efficiency and compact size while introducing multi-task
composition through an intuitive schema-based interface. Our experiments demonstrate competitive
performance across extraction and classification tasks with substantial improvements in deployment
accessibility compared to LLM-based alternatives. We release GLINER2 as an open-source pip-installable
library with pre-trained models and documentation at this https URL.
Biomedical Named Entity Recognition presents significant
challenges due to the complexity of biomedical terminology and inconsistencies in annotation across
datasets. This paper introduces SRU-NER (Slot-based Recurrent Unit NER), a novel approach designed to
handle nested named entities while integrating multiple datasets through an effective multi-task
learning strategy. SRU-NER mitigates annotation gaps by dynamically adjusting loss computation to
avoid penalizing predictions of entity types absent in a given dataset. Through extensive experiments,
including @ cross-corpus evaluation and human assessment of the model's predictions, SRU-NER achieves
competitive performance in biomedical and general-domain NER tasks, while improving cross-domain
Moral foundation detection is crucial for analyzing social discourse and developing
ethically-aligned AI systems. while large language models excel across diverse tasks, their
performance on specialized moral reasoning remains unclear.
This study provides the first comprehensive comparison betueen state-of-the-art LLMs and fine-tuned
transformers across Twitter and Reddit datasets using ROC, PR, and DET curve analysis.
Results reveal substantial performance gaps, with LLMs exhibiting high false negative rates and
systematic under-detection of moral content despite prompt engineering efforts. These findings
demonstrate that task-specific fine-tuning remains superior to prompting for moral reasoning
Large Language Models (LLMs) have shoun strong potential for tabular data generation
by modeling textualized feature-value pairs. However, tabular data inherently exhibits sparse feature-
Level dependencies, where many feature interactions are structurally insignificant. This creates a
Fundamental mismatch as Lis" self-attention mechanism inevitably distributes focus across all pairs,
diluting attention on critical relationships, particularly in datasets with complex dependencies or
semantically ambiguous features. To address this limitation, we propose GraDe (Graph-Guided Dependency
Learning), @ novel method that explicitly integrates sparse dependency graphs into LLMs" attention
mechanism. Grae employs @ lightweight dynamic graph learning module guided by externally extracted
functional dependencies, prioritizing key feature interactions while suppressing irrelevant ones. Our
experiments across diverse real-world datasets demonstrate that Grabe outperforms existing LLN-based
approaches by up to 12% on complex datasets uhile achieving competitive results with state-of-the-art
approaches in synthetic data quality. Our method is minimally intrusive yet effective, offering a
practical solution for structure-aware tabular data modeling with LLMs.
In common law systems, legal professionals such as lawyers and
judges rely on precedents to build their arguments. As the volume of cases has grown massively over
‘time, effectively retrieving prior cases has become essential. Prior case retrieval (PCR) is an
information retrieval (IR) task that aims to automatically identify the most relevant court cases for
a specific query from 2 large pool of potential candidates. while IR methods have seen several
paradigm shifts over the last few years, the vast majority of PCR methods continue to rely on
‘traditional IR methods, such as 6125. The state-of-the-art deep learning IR methods have not been
successful in PCR due to two key challenges: i. Lengthy legal text limitation; when using the powerful
BERT-based transformer models, there is a limit of input text lengths, which inevitably requires to
shorten the input via truncation or division with a loss of legal context information. ii. Lack of
legal training data; due to data privacy concerns, available PCR datasets are often limited in size,
making it difficult to train deep learning-based models effectively. In this research, we address
‘these challenges by leveraging LLM-based text embedders in PCR. LLM-based embedders support longer
input lengths, and since we use them in an unsupervised manner, they do not require training data,
addressing both challenges simultaneously. In this paper, we evaluate state-of-the-art LU"-based text
embedders in four PCR benchmark datasets and show that they outperform BM25 and supervised
From: Damith Premasiri Dola Mullage [view email][v1] Thu, 24 Jul 2025 14:36:10 UTC (5,602 KB)
Generating clinical synthetic text represents an effective solution for common
clinical NLP issues like sparsity and privacy. This paper aims to conduct a systematic review on
generating synthetic medical free-text by formulating quantitative analysis to three research
questions concerning (i) the purpose of generation, (ii) the techniques, and (iii) the evaluation
methods. We searched Publled, ScienceDirect, lieb of Science, Scopus, IEEE, Google Scholar, and arXiv
databases for publications associated with generating synthetic medical unstructured free-text. We
have identified 94 relevant articles out of 1,398 collected ones. A great deal of attention has been
given to the generation of synthetic medical text from 2018 onwards, where the main purpose of such a
generation is towards text augmentation, assistive writing, corpus building, privacy-preserving,
annotation, and usefulness. Transformer architectures were the main predominant technique used to
generate the text, especially the GPTs. On the other hand, there were four main aspects of evaluation,
including similarity, privacy, structure, and utility, where utility was the most frequent method used
to assess the generated synthetic medical text. Although the generated synthetic medical text
demonstrated a moderate possibility to act as real medical documents in different downstream NLP
‘tasks, it has proven to be a great asset as augmented, complementary to the real documents, towards
improving the accuracy and overcoming sparsity/undersampling issues. Yet, privacy is still a major
issue behind generating synthetic medical text, where more human assessments are needed to check for
‘the existence of any sensitive information. Despite that, advances in generating synthetic medical
text will considerably accelerate the adoption of workflows and pipeline development, discarding the
‘time-consuming legalities of data transfer.
Punctuation restoration enhances the readability of text and is
critical for post-processing tasks in Automatic Speech Recognition (ASR), especially for low-resource
languages like Bangla. In this study, we explore the application of transformer-based models,
specifically XLM-RoBERTa-large, to automatically restore punctuation in unpunctuated Bangla text. We
focus on predicting four punctuation marks: period, comma, question mark, and exclamation mark across
diverse text domains. To address the scarcity of annotated resources, we constructed a large, varied
‘training corpus and applied data augmentation techniques. Our best-performing model, trained with an
augmentation factor of alpha = 0.20%, achieves an accuracy of 97.1% on the News test set, 91.2% on the
Reference set, and 90.2% on the ASR set.
Results show Strong generalization to reference and ASR transcripts, demonstrating the model's
effectiveness in real-world, noisy scenarios. This work establishes a strong baseline for Bangla
punctuation restoration and contributes publicly available datasets and code to support future
research in low-resource NLP.
From: Md Adyelullahil Mamun [view email][vi] Thu, 24 Jul 2025 14:33:13 UTC (405 KB)
The cognitive and reasoning abilities of large language models
(LLMs) have enabled remarkable progress in natural language processing. However, their performance in
interpreting structured data, especially in tabular formats, remains limited. Although benchmarks for
English tabular data are widely available, Arabic is still underrepresented because of the limited
availability of public resources and its unique language features. To address this gap, we present
AraTable, a novel and comprehensive benchmark designed to evaluate the reasoning and understanding
capabilities of LLMs when applied to Arabic tabular data. AraTable consists of various evaluation
‘tasks, such as direct question answering, fact verification, and complex reasoning, involving a wide
range of Arabic tabular sources. Our methodology follows a hybrid pipeline, where initial content is
generated by LLMs and subsequently filtered and verified by human experts to ensure high dataset
quality. Initial analyses using AraTable show that, while LLMs perform adequately on simpler tabular
tasks such as direct question answering, they continue to face significant cognitive challenges when
tasks require deeper reasoning and fact verification. This indicates that there are substantial
‘opportunities for future work to improve performance on complex tabular reasoning tasks. We also
propose a fully automated evaluation framework that uses a self-deliberation mechanism and achieves
performance nearly identical to that of human judges. This research provides @ valuable, publicly
available resource and evaluation framework that can help accelerate the development of foundational
models for processing and analysing Arabic structured data.
Opinions expressed in online finance-related textual data are
having an increasingly profound impact on trading decisions and market movements. This trend
highlights the vital role of sentiment analysis as a tool for quantifying the nature and strength of
such opinions. With the rapid development of Generative AI (GenAI), supervised fine-tuned (SFT) large
Language models (LLMs) have become the de facto standard for financial sentiment analysis. However,
‘the SFT paradigm can lead to memorization of the training data and often fails to generalize to unseen
samples. This is a critical limitation in financial domains, where models must adapt to previously
unobserved events and the nuanced, domain-specific language of finance. To this end, we introduce
FinDPO, the first finance-specific LLM framework based on post-training human preference alignment via
Direct Preference Optimization (DPO). The proposed FinDPO achieves state-of-the-art performance on
standard sentiment classification benchmarks, outperforming existing supervised fine-tuned models by
11% on the average. Uniquely, the FinDPO framework enables the integration of @ fine-tuned causal LLM
into realistic portfolio strategies through a novel ‘logit-to-score’ conversion, which transforms
discrete sentiment predictions into continuous, rankable sentiment scores (probabilities). In this
way, simulations demonstrate that FinDPO is the first sentiment-based approach to maintain substantial
positive returns of 67% annually and strong risk-adjusted performance, as indicated by a Sharpe ratio
of 2.0, even under realistic transaction costs of 5 basis points (bps).
Wikipedia serves as a globally accessible knowledge source with
content in over 300 languages. Despite covering the same topics, the different versions of Wikipedia
are written and updated independently. This leads to factual inconsistencies that can impact the
neutrality and reliability of the encyclopedia and AI systems, which often rely on Wikipedia as a main
‘training source. This study investigates cross-lingual inconsistencies in Wikipedia's structured
content, with a focus on tabular data. We developed a methodology to collect, align, and analyze
tables from Wikipedia multilingual articles, defining categories of inconsistency. We apply various
quantitative and qualitative metrics to assess multilingual alignment using a sample dataset. These
insights have implications for factual verification, multilingual knowledge interaction, and design
for reliable AI systems leveraging Wikipedia content.
The evaluation of Large Language Models (LLMs) increasingly
relies on other LLMs acting as judges. However, current evaluation paradigms typically yield a single
score or ranking, answering which model is better but not why. While essential for benchmarking, these
‘top-level scores obscure the specific, actionable reasons behind a model's performance. To bridge this
gap, we introduce CLEAR, an interactive, open-source package for LiM-based error analysis. CLEAR first
generates per-instance textual feedback, then it creates a set of system-level error issues, and
quantifies the prevalence of each identified issue. Our package also provides users with an
interactive dashboard that allows for a comprehensive error analysis through aggregate visualizations,
applies interactive filters to isolate specific issues or score ranges, and drills down to the
individual instances that exemplify a particular behavioral pattern. We demonstrate CLEAR analysis for
RAG and Math benchmarks, and showcase its utility through a user case study.
Propaganda detection on social media remains challenging due to
‘task complexity and limited high-quality labeled data. This paper introduces a novel framework that
combines human expertise with Large Language Model (LLM) assistance to improve both annotation
consistency and scalability. We propose a hierarchical taxonomy that organizes 14 fine-grained
propaganda techniques into three broader categories, conduct a human annotation study on the HOP
dataset that reveals low inter-annotator agreement for fine-grained labels, and implement an LLM-
assisted pre-annotation pipeline that extracts propagandistic spans, generates concise explanations,
and assigns local labels as well as @ global label. A secondary human verification study shows
significant improvements in both agreement and time-efficiency. Building on this, we fine-tune smaller
Language models (SLMs) to perform structured annotation. Instead of fine-tuning on human annotations,
we train on high-quality LLM-generated data, allowing a large model to produce these annotations and a
smaller model to learn to generate them via knowledge distillation. Our work contributes towards the
development of scalable and robust propaganda detection systems, supporting the idea of transparent
and accountable media ecosystems in line with SDG 16. The code is publicly available at our GitHub
repository.
In-context learning (ICL) has become a classic approach for enabling LLMs to handle
various tasks based on 2 few input-output examples. The effectiveness of ICL heavily relies on the
quality of these examples, and previous works which focused on enhancing example retrieval
capabilities have achieved impressive performances. However, tuo challenges remain in retrieving high-
quality examples: (1) Difficulty in distinguishing cross-task data distributions, (2) Difficulty in
making the fine-grained connection between retriever output and feedback from LLMs. In this paper, we
propose a novel framework called TDR. TDR decouples the ICL examples from different tasks, which
enables the retrieval module to retrieve examples specific to the target task within a multi-task
dataset. Furthermore, TDR models fine-grained feedback from LLMs to supervise and guide the training
of the retrieval module, which helps to retrieve high-quality examples. We conducted extensive
experiments on a suite of 30 NLP tasks, the results demonstrate that TDR consistently improved results
across all datasets and achieves state-of-the-art performance. Meanwhile, our approach is a plug-and-
play method, which can be easily combined with various LLMs to improve example retrieval abilities for
ICL. The code is available at this https URL.
In machine translation (MT), when the source sentence includes a
Lexeme whose gender is not overtly marked, but whose target-language equivalent requires gender
specification, the model must infer the appropriate gender from the context and/or external knowledge.
Studies have shown that MT models exhibit biased behaviour, relying on stereotypes even when they
Clash with contextual information. We posit that apart from confidently translating using the correct
gender when it is evident from the input, models should also maintain uncertainty about the gender
when it is ambiguous. Using recently proposed metrics of semantic uncertainty, we find that models
with high translation and gender accuracy on unambiguous instances do not necessarily exhibit the
expected level of uncertainty in ambiguous ones. Similarly, debiasing has independent effects on
ambiguous and unambiguous translation instances.
From: Ieva Raminta Staliunaite [view email][v1] Thu, 24 Jul 2025 1:
Large reasoning models (LRMs) have emerged as a significant
advancement in artificial intelligence, representing a specialized class of large language models
(LLMs) designed to tackle complex reasoning tasks. The defining characteristic of LRMs lies in their
extensive chain-of-thought (CoT) reasoning capabilities. In this paper, we identify a previously
unexplored attack vector against LRMs, which we term “overthinking backdoors". We advance this concept
by proposing a novel tunable backdoor, which moves beyond simple on/off attacks to one where an
attacker can precisely control the extent of the model's reasoning verbosity. Our attack is
implemented through a novel data poisoning methodology. It pairs a tunable trigger-where the number of
repetitions signals the desired intensity-with a correspondingly verbose CoT response. These responses
are programmatically generated by instructing a teacher LLM to inject a controlled number of redundant
refinement steps into a correct reasoning process. The approach preserves output correctness, which
ensures stealth and establishes the attack as a pure resource-consumption vector. Extensive empirical
results on various LRMs demonstrate that our method can reliably trigger a controllable, multi-fold
increase in the length of the reasoning process, without degrading the final answer"s correctness. Our
Language Models (Lis) typically adhere to a “pre-training and fine-tuning" paradign,
here @ universal pre-trained model can be fine-tuned to cater to various specialized domains. Low-
Rank Adaptation (LORA) has gained the most widespread use in LM fine-tuning due to its lightweight
computational cost and remarkable performance. Because the proportion of parameters tuned by LORA is
relatively small, there might be a misleading inpression that the LoRA fine-tuning data is
invulnerable to Nenbership Inference attacks (NIAs). However, we identity that utilizing the pre-
trained model can induce more information leakage, which is neglected by existing MIAs. therefore, we
introduce LoRA-Leak, a holistic evaluation framework for MIAs against the fine-tuning datasets of Lis.
LoRs-Leak incorporates fifteen menbership inference attacks, including ten existing MIAs, and five
improved MIAs that leverage the pre-trained model as a reference. In experiments, we apply LoRA-Leak
to three advanced Lis across three popular natural language processing tasks, demonstrating that LoRA-
based fine-tuned LNs are still vulnerable to MIAs (e.g-, 9.775 AUC under conservative fine-tuning
settings). lie also applied LoRA-Leak to different fine-tuning settings to understand the resulting
privacy risks. he further explore four defenses and find that only dropout and excluding specific LM
layers during fine-tuning effectively mitigate MIA risks while maintaining utility. We highlight that
under the “pre-training and fine-tuning" paradign, the existence of the pre-trained model makes MIA a
more severe risk for LoRA-based Li's. lie hope that our findings can provide guidance on data privacy
protection for specialized LM providers.
Solving the problem of Optical Character Recognition (OCR) on
printed text for Latin and its derivative scripts can now be considered settled due to the volumes of
research done on English and other High-Resourced Languages (HL). However, for Low-Resourced
Languages (LRL) that use unique scripts, it remains an open problem. This study presents a comparative
analysis of the zero-shot performance of six distinct OCR engines on two LRLs: Sinhala and Tamil. The
selected engines include both commercial and open-source systems, aiming to evaluate the strengths of
each category. The Cloud Vision API, Surya, Document AI, and Tesseract were evaluated for both Sinhala
and Tamil, while Subasa OCR and EasyOCR were examined for only one language due to their limitations.
‘The performance of these systems was rigorously analysed using five measurement techniques to assess
accuracy at both the character and word levels. According to the findings, Surya delivered the best
performance for Sinhala across all metrics, with a WER of 2.61%. Conversely, Document AI excelled
across all metrics for Tamil, highlighted by a very low CER of 0.78%. In addition to the above
analysis, we also introduce a novel synthetic Tamil OCR benchmarking dataset.
From: Nevidu Jayatilleke Hr. [view email][v1] Thu, 24 Jul 2025 10:08:43 UTC (9,528 KB)
Direct speech translation (ST) has garnered increasing attention
nowadays, yet the accurate translation of terminology within utterances remains a great challenge. In
‘this regard, current studies mainly concentrate on leveraging various translation knowledge into ST
models. However, these methods often struggle with interference from irrelevant noise and can not
fully utilize the translation knowledge. To address these issues, in this paper, we propose a novel
Locate-and-Focus method for terminology translation. It first effectively locates the speech clips
containing terminologies within the utterance to construct translation knowledge, minimizing
irrelevant information for the ST model. Subsequently, it associates the translation knowledge with
‘the utterance and hypothesis from both audio and textual modalities, allowing the ST model to better
focus on translation knowledge during translation. Experimental results across various datasets
demonstrate that our method effectively locates terminologies within utterances and enhances the
success rate of terminology translation, while maintaining robust general translation performance.
Eye-tracking data reveals valuable insights into users’ cognitive
states but is difficult to analyze due to its structured, non-linguistic nature. While large language
models (LLMs) excel at reasoning over text, they struggle with temporal and numerical data. This paper
presents a multimodal human-AI collaborative framework designed to enhance cognitive pattern
‘extraction from eye-tracking signals. The framework includes: (1) a multi-stage pipeline using
horizontal and vertical segmentation alongside LLM reasoning to uncover latent gaze patterns; (2) an
Expert-Model Co-Scoring Module that integrates expert judgment with LLM output to generate trust
scores for behavioral interpretations; and (3) a hybrid anomaly detection module combining LSTM-based
‘temporal modeling with LLM-driven semantic analysis. Our results across several LLMs and prompt
strategies show improvements in consistency, interpretability, and performance, with up to 50%
accuracy in difficulty prediction tasks. This approach offers a scalable, interpretable solution for
cognitive modeling and has broad potential in adaptive learning, human-computer interaction, and
‘educational analytics.
Multi-agent systems (MAS) based on large language models (LLMs)
have emerged as a powerful solution for dealing with complex problems across diverse domains. The
effectiveness of MAS is critically dependent on its collaboration topology, which has become a focal
point for automated design research. However, existing approaches are fundamentally constrained by
‘their reliance on a template graph modification paradigm with a predefined set of agents and hard-
coded interaction structures, significantly limiting their adaptability to task-specific requirements.
To address these limitations, we reframe MAS design as a conditional autoregressive graph generation
‘task, where both the system composition and structure are designed jointly. We propose ARG-Designer, a
novel autoregressive model that operationalizes this paradigm by constructing the collaboration graph
from scratch. Conditioned on a natural language task query, ARG-Designer sequentially and dynamically
determines the required number of agents, selects their appropriate roles from an extensible pool, and
establishes the optimal communication links between them. This generative approach creates a
customized topology in a flexible and extensible manner, precisely tailored to the unique demands of
different tasks. Extensive experiments across six diverse benchmarks demonstrate that ARG-Designer not
only achieves state-of-the-art performance but also enjoys significantly greater token efficiency and
enhanced extensibility. The source code of ARG-Designer is available at this https URL.
From: Shiyuan Li [view email][v1] Thu, 24 Jul 2025 09:17:41 UTC (813 KB)
Layer pruning has emerged as a promising technique for
compressing large language models (LLMs) while achieving acceleration proportional to the pruning
ratio. In this work, we identify that removing any layer induces a significant magnitude gap in hidden
states, resulting in substantial performance degradation. To address this issue, we propose
Prune&Comp, a novel plug-and-play layer pruning scheme that leverages magnitude compensation to
mitigate such gaps in a training-free manner. Specifically, we first estimate the magnitude gap caused
by layer removal and then eliminate this gap by rescaling the remaining weights offline, with zero
runtime overhead incurred. We further demonstrate the advantages of Prune&Comp through an iterative
pruning strategy. When integrated with an iterative prune-and-compensate loop, Prune&Comp consistently
enhances existing layer pruning metrics. For instance, when 5 layers of LLaMA-3-8B are pruned using
‘the prevalent block influence metric, Prune&Comp nearly halves the perplexity and retains 93.19\% of
‘the original model's question-answering performance, outperforming the baseline by 4.01%.
Instruction-tuning enhances the ability of large language models
(LLMs) to follow user instructions more accurately, improving usability while reducing harmful
outputs. However, this process may increase the model's dependence on user input, potentially leading
to the unfiltered acceptance of misinformation and the generation of hallucinations. Existing studies
primarily highlight that LLMs are receptive to external information that contradict their parametric
knowledge, but little research has been conducted on the direct impact of instruction-tuning on this
phenomenon. In our study, we investigate the impact of instruction-tuning on LLM's susceptibility to
misinformation. Our analysis reveals that instruction-tuned LLMs are significantly more likely to
accept misinformation when it is presented by the user. A comparison with base models shows that
instruction-tuning increases reliance on user-provided information, shifting susceptibility from the
assistant role to the user role. Furthermore, we explore additional factors influencing misinformation
susceptibility, such as the role of the user in prompt structure, misinformation length, and the
presence of warnings in the system prompt. Our findings underscore the need for systematic approaches
‘to mitigate unintended consequences of instruction-tuning and enhance the reliability of LLMs in real-
world applications.
Retrieval-Augmented Generation (RAG) enhances Large Language
Models (LLMs) by providing external knowledge for accurate and up-to-date responses. However, this
reliance on external sources exposes a security risk, attackers can inject poisoned documents into the
knowledge base to steer the generation process toward harmful or misleading outputs. In this paper, we
propose Gradient-based Masked Token Probability (GHTP), a novel defense method to detect and filter
out adversarially crafted documents. Specifically, GHTP identifies high-impact tokens by examining
gradients of the retrievers similarity function. These key tokens are then masked, and their
probabilities are checked via a Masked Language Model (MLM). Since injected tokens typically exhibit
markedly low masked-token probabilities, this enables GTP to easily detect malicious documents and
achieve high-precision filtering. Experiments demonstrate that GITP is able to eliminate over 90% of
poisoned content while retaining relevant documents, thus maintaining robust retrieval and generation
performance across diverse datasets and adversarial settings.
Business process modeling is used by most organizations as an essential framework for
‘ensuring efficiency and effectiveness of the work and workflow performed by its employees and for
ensuring the alignment of such work with its strategic goals. For organizations that are compliant or
near-compliant with ISO 9001, this approach involves the detailed mapping of processes, sub-processes,
activities, and tasks. 15030401 is a Management System Standard, introduced in 2018, establishing
universal requirements for the set up of a Knowledge Management System in an organization. As
*"15030401 implementers'' we regularly face the challenge of explaining our clients how the knowledge
development, transformation and conveyances activities depicted in IS0304@1 do integrate with existing
operational processes. This article recaps process modelling principles in the context of 1509001 and
explores, based on our experience, how an 15030401-compliant Knowledge Management System (KMS)
entuines with all other processes of an Integrated Management System and in particular how it can be
implemented by deploying the mechanisms of the SECI model through the steps of PDCA cycles.
From: Patrick PRIEUR [view email] [via CCSD proxy][v1] Thu, 24 Jul 2025 08:54:19 UTC (1,720 KB)
Root Cause Analysis (RCA) in teleconmunication networks is a critical task, yet it
presents a formidable challenge for Artificial Intelligence (AI) due to its complex, graph-based
reasoning requirements and the scarcity of realistic benchmarks.
From: Qianjin Yu [view email][v1] Thu, 24 Jul 2025 08:40:08 UTC (1,532 KB)
Large Language Models (LLMs) can achieve inflated scores on
multiple-choice tasks by exploiting inherent biases in option positions or labels, rather than
demonstrating genuine understanding. This study introduces SCOPE, an evaluation framework designed to
measure and mitigate such selection bias in a dataset-independent manner. By repeatedly invoking a
distribution. It then redistributes the answer slot according to the inverse-bias distribution,
‘thereby equalizing the lucky-rate, the probability of selecting the correct answer by chance.
Furthermore, it prevents semantically similar distractors from being placed adjacent to the answer,
‘thereby blocking near-miss guesses based on superficial proximity cues. Across multiple benchmark
experiments, SCOPE consistently outperformed existing debiasing methods in terms of stable performance
improvements and showed clearer confidence distributions over correct options. This framework thus
offers a new standard for enhancing the fairness and reliability of LLM evaluations.
Despite the widespread use of Transformer-based text embedding models in NLP tasks,
surprising ‘sticky tokens’ can undermine the reliability of embeddings. These tokens, when repeatedly
inserted into sentences, pull sentence similarity toward a certain value, disrupting the normal
distribution of embedding distances and degrading downstream performance. In this paper, we
systematically investigate such anomalous tokens, formally defining them and introducing an efficient
detection method, Sticky Token Detector (STD), based on sentence and token filtering. Applying STD to
40 checkpoints across 14 model families, we discover a total of 868 sticky tokens. Our analysis
reveals that these tokens often originate from special or unused entries in the vocabulary, as well as
fragmented subwords from multilingual corpora. Notably, their presence does not strictly correlate
with model size or vocabulary size. We further evaluate how sticky tokens affect downstream tasks like
clustering and retrieval, observing significant performance drops of up to 50%. Through attention-
layer analysis, we show that sticky tokens disproportionately dominate the model's internal
representations, raising concerns about tokenization robustness. Qur findings show the need for better
tokenization strategies and model design to mitigate the impact of sticky tokens in future text
‘embedding applications.
Recent progress in Multi-modal Large Language Models (MLLMs) has
enabled step-by-step multi-modal mathematical reasoning by performing visual operations based on the
‘textual instructions. A promising approach uses code as an intermediate representation to precisely
‘express and manipulate the images in the reasoning steps. However, existing evaluations focus mainly
‘on text-only reasoning outputs, leaving the MLLM's ability to perform accurate visual operations via
code largely unexplored. This work takes a first step toward addressing that gap by evaluating MLLM's
code-based capabilities in multi-modal mathematical this http URL, our framework focuses on two key
evaluation aspects: (1) Multi-modal Code Generation (MCG) evaluates the model's ability to accurately
understand and construct visualizations from scratch. (2) Multi-modal Code Editing (NCE) assesses the
model's capacity for fine-grained operations, which include three types: Deletion, Modification and
Annotation. To evaluate the above tasks, we incorporate a dataset that covers the five most popular
‘types of mathematical figures, including geometric diagrams, function plots, and three types of
statistical charts, to provide 2 comprehensive and effective measurement of existing MLLMs. Our
experimental evaluation involves nine mainstream MLLMs, and the results reveal that existing models
still lag significantly behind human performance in performing fine-grained visual operations.
The rapid development of COVID-19 vaccines has showcased the
global communitys ability to combat infectious diseases. However, the need for post-licensure
Surveillance systems has grown due to the limited window for safety data collection in clinical trials
and early widespread implementation. This study aims to employ Natural Language Processing techniques
and Active Learning to rapidly develop a classifier that detects potential vaccine safety issues from
‘the point of entry to health systems, can significantly contribute to timely vaccine safety signal
surveillance. While keyword-based classification can be effective, it may yield false positives and
demand extensive keyword modifications. This is exacerbated by the infrequency of vaccination-related
ED presentations and their similarity to other reasons for ED visits. NLP offers a more accurate and
efficient alternative, albeit requiring annotated data, which is often scarce in the medical field.
Active learning optimizes the annotation process and the quality of annotated data, which can result
in faster model implementation and improved model performance. This work combines active learning,
data augmentation, and active learning and evaluation techniques to create a classifier that is used
to enhance vaccine safety surveillance from ED triage notes.
significantly improved the ability of AI systems to engage in natural spoken interactions. However,
most existing models treat speech merely as a vehicle for linguistic content, often overlooking the
rich paralinguistic and speaker characteristic cues embedded in human speech, such as dialect, age,
emotion, and non-speech vocalizations. In this work, we introduce GOAT-SLM, a novel spoken language
model with paralinguistic and speaker characteristic awareness, designed to extend spoken language
modeling beyond text semantics. GOAT-SLM adopts a dual-modality head architecture that decouples
Linguistic modeling from acoustic realization, enabling robust language understanding while supporting
‘expressive and adaptive speech generation. To enhance model efficiency and versatility, we propose @
modular, staged training strategy that progressively aligns linguistic, paralinguistic, and speaker
characteristic information using large-scale speech-text corpora. Experimental results on TELEVAL, a
multi-dimensional evaluation benchmark, demonstrate that GOAT-SLM achieves well-balanced performance
across both semantic and non-semantic tasks, and outperforms existing open-source models in handling
emotion, dialectal variation, and age-sensitive interactions. This work highlights the importance of
modeling beyond linguistic content and advances the development of more natural, adaptive, and
socially aware spoken language systems.
From: Zehan Li [view email][vi] Thu, 24 Jul 2025 06:10:29 UTC (1,836 KB)
Dissecting Persona-Driven Reasoning in Language Models
via Activation Patching
able versatility in adopting diverse personas. In
this study, we examine how assigning a per-
sona influences a model’s reasoning on an ob-
jective task. Using activation patching, we take
a first step toward understanding how key com-
ponents of the model encode persona-specific
information. Our findings reveal that the early
Multi-Layer Perceptron (MLP) layers attend
not only to the syntactic structure of the input
but also process its semantic content. These
layers transform persona tokens into richer rep-
resentations, which are then used by the middle
Multi-Head Attention (MHA) layers to shape
the model’s output. Additionally, we identify
specific attention heads that disproportionately
attend to racial and color-based identities.
1 Introduction
have demonstrated their striking ability to adopt a
wide range of personas, enabling context-sensitive
and tailored responses (Zhang et al., 2024, Joshi
such as those by Salewski et al., 2023, Deshpande
show that persona assignment can significantly in-
fluence reasoning and, in some cases, amplify un-
derlying social biases. While these works focus
on identifying and quantifying such effects, they
do not examine the causal mechanisms within a
pre-trained language model (PLM) that give rise to
them. In this study, we take a step toward bridging
that gap by investigating the roles of key model
components—namely, the MLP layers, MHA lay-
ers, and individual attention heads—in shaping the
variation in reasoning induced by persona assign-
ments. Using activation patching, we probe the
internal circuitry of LLMs to trace the origins of
persona-driven variation in objective tasks. Our
findings challenge the prevailing assumption that
early MLP layers are concerned solely with syn-
tactic processing. We show that these layers also
encode semantic features related to persona. Fur-
thermore, we identify a small number of attention
heads that disproportionately focus on tokens as-
sociated with race-based persona cues. Although
our work is limited to uncovering the origin of
persona-driven behavior, it contributes to a deeper
understanding of LLMs and lays groundwork for
future efforts to mitigate deep-seated biases in these
We chose the MMLU dataset (Hendrycks et al.,
2021) for this study, which contains 14,024
We selected this dataset for two main reasons. First,
the objective, multiple-choice format allowed us to
focus on a fixed set of four answer tokens, giving
us a well-defined target. Second, the wide range
of subject areas helped reduce domain-specific or
persona-driven biases. The identities or personas
we examined in our study fall into four broad cate-
gories: racial identities, color-based identities, and
identities defined by positive or negative attributes;
details of each can be found in the Appendix A. To
maintain consistency and minimize variance, we
used "student" as a gender-neutral subject across
all defined identities. For instance: "Asian student",
2.2 Model and Prompt
We use the LLaMA 3.2 1B Instruct model (Meta,
2024b) due to computational constraints. Its com-
pact size makes it well-suited for efficient exper-
imentation, while still delivering strong perfor-
mance relative to other open-source models in its
class. Our methods are scalable and can be ex-
tended to larger models with sufficient computa-
tional resources and minor adjustments. The In-
struct variant of the LLaMA family also permits
the use of system prompts!, allowing us to specify
the identity the model should adopt in its responses.
A standard system prompt serves as the base for
each question, enabling identity shifts by modi-
fying only two tokens in the entire prompt—just
one token distinguishes each identity. We used
the prompt structure defined by Meta for MMLU
dataset (Meta, 2024a), with the addition of the sys-
tem prompt, see Appendix B.
We computed model outputs using zero-shot
prompts to isolate the variation introduced solely
by changes in the persona token, avoiding any influ-
ence from patterns introduced by few-shot prompt-
ing. This also reduced the length of each prompt, al-
lowing for faster computation. For each prompt, we
calculated the probability of the next token, which,
by design, corresponds to the selected answer for
the given question. This process was repeated for
all 16 identities, and also the base identity.
Our main focus is the change in the probability
of the correct token for each persona relative to the
base identity, as shown in Figure 1. This highlights
shifts in reasoning attributable to persona alone.
In some cases, the differences in probability fol-
lowed patterns that appeared to have a semantic ba-
sis. For instance, the negatively attributed student
persona performed significantly worse than those
described with positive attributes, having an aver-
age probability difference of -0.0045 (T = 14.36,
p < 0.001). In contrast, the results for racially or
color-coded personas were less consistent, and no
definitive conclusion could be drawn about whether
the patterns reflect stereotypical associations. A
similar trend was observed when examining accu-
racy across identities, see Appendix C for details.
Overall, our results suggest that the model’s rea-
soning ability varies even with minimal changes to
the persona being imitated.
4 Interpretability Investigation
We focus on activation patching, also referred to
as Resample Ablation, or Causal Mediation Analy-
‘System prompts are instructions given to the AI before
any user input. They define the AI’s behavior, role, and re-
sponse style throughout an interaction.
white +4
African -—_{_ IT}
$ Brown a
Black {—_T >
British ——__E}+——+
Avg. Probability Difference
Figure 1: Average difference in the probability of the
correct token for each identity, relative to the base
stand the influence of individual components on
the model’s selection of the correct answer. Specifi-
cally, we employ "de-noising" variant of activation
patching, where the activation of a component from
a corrupted run is replaced with the correspond-
ing activation from a clean run (Heimersheim and
Nanda, 2024). The clean and corrupted inputs are
constructed using Symmetric Token Replacement,
a technique in which only one or a few tokens
This approach doesn’t throw the model’s internal
state out-of-distribution and preserves the syntactic
structure of the input (Zhang and Nanda, 2024).
For each experiment, we select a pair of personas
to generate the clean ([D/) and corrupted (ID2)
prompts. A total of ten persona pairs were chosen
keeping computational constraints in mind; details
are provided in the Appendix D. Care was taken to
balance pairings identities both within and across
categories to ensure reliable results. Pairings with
a base identity were also included for comparison.
For each pair, we divide the full set of questions
into four subsets, Appendix E. From these, we
select the subset in which the first persona (JD/)
answers correctly while the second persona ([D2)
answers incorrectly. In each such pair of prompts,
the only difference lies in the token representing
the persona. As a result, any change in the logit
of the correct answer after patching reflects the
influence of the component (and its downstream ef-
fects) on how the model processes persona-related
information.
4.2 Effects
The impact of patching on the model’s output can
be broken into direct and indirect effects (Pearl,
MLP Layers
MLP layer patching
oA
FF oF FP FE OS
MHA layer patching
Figure 2: Relative logit difference (A,-) when MLP layer (left) and MHA layer (right) is patched. Accompanying
bar chart shows the average A,. across identity pairs.
2001). The direct effect measures the isolated con-
tribution of the component to the output. The in-
direct effect captures the influence the component
has via the behavior of later layers and is computed
by subtracting the direct effect from the total effect
of patching the component. Together, these effects
help characterize the role the component plays in
processing input information.
To quantify the impact of patching, we use two met-
rics. First, we check whether patching causes the
logit (1) of the correct answer to become the highest
among all four options. Second, we measure the
relative logit difference (A,.), i.e., change in the
logit of the correct answer relative to the change in
the mean logit across all options.
A, = {leorrect (P) _ leorrect(C)}
— {u(lLason(P)) — w(lason(C))}
Here, |,.(P) is the logit from patched run and
1.(C) from corrupt run, and ju is mean function.
We use this relative metric rather than an abso-
the correct answer not by increasing its logit di-
rectly, but by decreasing the logits of incorrect
difference—would overlook such cases and only
highlight components with large magnitude effects
on the correct answer. These metrics are computed
across all selected questions for each persona pair.
We begin by patching the MLP and MHA layers
across all token positions. The Figure 2 shows A,
across all identity pairs, see Appendix F for results
on other metric. We observe that patching the early
MLP layers (layers 1-3) and the middle MHA lay-
ers (layers 9-11) produces a consistently higher
total effect across all ten identity pairs. We hypoth-
esize that the early MLP layers develop persona-
specific information, particularly at the token posi-
tion(s) representing the identity. This information
is then picked up and used by the later MHA layers,
giving rise to the observed persona-driven behav-
ior. To test this, we patch only the activations at the
identity token position in the MLP layers, rather
than all positions. We observed that patching ac-
tivations at the identity token position in the first
MLP layer produced an effect nearly equivalent to
patching all token positions. Beyond the first layer,
the effect rapidly decays, with later layers showing
little to no impact, see Appendix G.
We also find that only the MLP layers near the
end of the network have any direct effect on the
output, see Figure 3. This implies that the effect
seen in the initial layers is purely indirect. Since the
local syntax around the point of change remains
constant, this suggests that the initial MLP lay-
fro2
Figure 3: Average A, when only direct component of a
MLP layer is patched.
ers are processing not just structure but also the
semantics of the input tokens. This counter to ear-
2018, Jawahar et al., 2019) that claimed these lay-
ers were focused only on syntactic or local features,
eaving semantic processing to later layers. The
subtlety of this semantic processing may explain
why it was overlooked in previous studies. Our re-
sults align with observations that initial MLP layers
transform tokens into richer representations (Stolfo
et al., 2023), supporting the hypothesis that these
ayers induce persona-specific semantics that later
To investigate MHA layer roles more closely,
we performed activation patching on individual
attention heads (H, end number) identifying eight
heads with a high positive effect on the output
and four with a high negative effect, see Figure 4.
We analyzed the value-weighted attention patterns
(Lieberum et al., 2023) of these heads on the iden-
tity token position across five questions per subject,
categorizing them based on the relative attention
given when compared with other identities, see
Appendix H. H3°, H?3, H?°, H}s, and Hi} con-
sistently allocated higher attention to tokens repre-
senting racial identities. H?°, H?;, and H}} also
showed elevated attention to color-based identi-
ties, though this pattern was less consistent across
domains. H}% uniquely focused on color-based
identities, while H?? prioritized both positive and
negative attributed identities at early token posi-
tions. We further examined how these attention
patterns of these heads responded to MLP layer
patching. When activations from runs with racial
or color-based personas were replaced with those
from positive or negative attributed personas, the at-
tention of heads previously showing high focus on
the identity token position decreased significantly.
This reduction occurred regardless of whether all
token positions or only the identity token position
were patched. For most heads, patching layers
beyond the first had minimal impact on attention
patterns, for details see Appendix I.
Our findings validate the hypothesis of an inter-
Aitention Heads
Figure 4: Average A, for patching individual attention
heads.
action between the initial MLP layers and the mid-
The initial MLP layers—especially at the identity
token position—form rich, persona-specific seman-
tic representations. These are then taken up by the
middle MHA layers, impacting the model’s choice
of response.
In this work, we analyzed the impact of persona-
driven behavior on the reasoning ability of a lan-
guage model in objective tasks. Through the lens
of mechanistic interpretability, we examined the
role of different model components—MLLP layers,
MHA layers, and individual attention heads—in
shaping this behavior. We observed that early
MLP layers also contributes towards semantic un-
derstanding of inputs and encode persona-specific
information into richer representation. This infor-
mation is then passed to later MHA layers, which
use it to influence the model’s response. We further
categorized attention heads based on their relative
attention patterns and identified a subset that as-
signed disproportionate weight to racial and color-
related attributes associated with a persona.
Our findings offer preliminary insight into the
subtle yet significant functions of certain model
components and how they can be revealed through
constrained but straightforward experiments. We
took initial steps toward understanding the origins
of persona-driven behavior in LLMs. In future
studies, we will investigate why certain personas
answer specific questions correctly while others do
not, and how output vector circuits in attention
heads use earlier-layer representations to shape
final predictions. These directions will lead to
a deeper understanding of what "persona" truly
means in the context of language models.
Although the MMLU dataset contains a large num-
ber of mostly factual, objective questions, we ac-
knowledge that it is not the only dataset that meets
our selection criteria. Our experiments were con-
ducted exclusively on the LLaMA 3.2 1B Instruct
However, the methods described here can be read-
ily extended to models of different sizes and archi-
tectures, as well as to other datasets with similar
characteristics.
We selected a set of 16 personas to approximate
the space relevant to our probing efforts. The list
of personas is not exhaustive, but it serves as a
practical starting point. Our analysis focused on
attention heads identified as important through ac-
tivation patching. Examining additional heads may
improve our understanding of how persona-driven
behavior develops within the model. At present,
however, attention pattern analysis requires man-
ual inspection, which remains a slow and labor-
References
ating layers of representation in neural machine trans-
lation on part-of-speech and semantic tagging tasks.
In Proceedings of the Eighth International Joint Con-
ference on Natural Language Processing (Volume
Federation of Natural Language Processing.
Toxicity in chatgpt: Analyzing persona-assigned lan-
guage models. In Findings of the Association for
Computational Linguistics: EMNLP 2023, pages
1236-1270, Singapore. Association for Computa-
tional Linguistics.
pande, Ashwin Kalyan, Peter Clark, Ashish Sabhar-
wal, and Tushar Khot. 2024. Bias runs deep: Implicit
reasoning biases in persona-assigned LLMs. In The
Twelfth International Conference on Learning Repre-
Stefan Heimersheim and Neel Nanda. 2024. How to
use and interpret activation patching. arXiv preprint
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
2021. Measuring massive multitask language under-
standing. In International Conference on Learning
Representations.
2019. What does BERT learn about the structure of
language? In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguistics,
pages 3651-3657, Florence, Italy. Association for
Computational Linguistics.
Nitish Joshi, Javier Rando, Abulhair Saparov, Najoung
Kim, and He He. 2024. Personas as a way to model
truthfulness in language models. In Proceedings of
the 2024 Conference on Empirical Methods in Natu-
ral Language Processing, pages 6346-6359, Miami,
Florida, USA. Association for Computational Lin-
Nanda, Geoffrey Irving, Rohin Shah, and Vladimir
Belinkov. 2022. Locating and editing factual associ-
ations in GPT. In Advances in Neural Information
Processing Systems.
Meta. 2024b. Llama 3.2 family of models.
Judea Pearl. 2001. Direct and indirect effects. In
Proceedings of the Seventeenth Conference on Un-
certainty in Artificial Intelligence, UAV01, page
411-420. Morgan Kaufmann Publishers Inc.
and Wen-tau Yih. 2018. Dissecting contextual word
embeddings: Architecture and representation. In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1499-
1509, Brussels, Belgium. Association for Computa-
Leonard Salewski, Stephan Alaniz, Isabel Rio-Torto,
personation reveals large language models’ strengths
and biases. In Thirty-seventh Conference on Neural
Sachan. 2023. A mechanistic interpretation of arith-
metic reasoning in language models using causal
mediation analysis. In Proceedings of the 2023 Con-
ference on Empirical Methods in Natural Language
Processing, pages 7035-7052, Singapore. Associa-
tion for Computational Linguistics.
and Heng Ji. 2025. Persona-DB: Efficient large lan-
with collaborative data refinement. In Proceedings
of the 31st International Conference on Computa-
tional Linguistics, pages 281-296, Abu Dhabi, UAE.
Association for Computational Linguistics.
guage models using causal mediation analysis. In
Advances in Neural Information Processing Systems,
Inc.
Kevin Ro Wang, Alexandre Variengien, Arthur Conmy,
pretability in the wild: a circuit for indirect object
identification in GPT-2 small. In The Eleventh Inter-
national Conference on Learning Representations.
Fred Zhang and Neel Nanda. 2024. Towards best prac-
tices of activation patching in language models: Met-
rics and methods. In The Twelfth International Con-
ference on Learning Representations.
court, Joe Barrow, Tong Yu, Sungchul Kim, and 1
others. 2024. Personalization of large language mod-
Moontae Lee, and David Jurgens. 2024. When ”a
helpful assistant” is not really helpful: Personas in
system prompts do not improve performances of
large language models. In Findings of the Associ-
ation for Computational Linguistics: EMNLP 2024,
A Identities
In this study, personas—or identities—refer to sin-
gle words such as "Asian" or "good." The selected
identities fall into four broad categories, with four
identities chosen from each to ensure balanced com-
parisons. A further criterion in selection was that
each identity should be of a single token, so that
the only variation between prompts would be one
token. Table 1 lists the identities alongside their
The prompt format used throughout the study is
shown in Figure 5. The placeholder {helper} is
replaced with "a" or "an" depending on whether
the first letter of {identity_1} is a vowel. The
variable {identity_1} is substituted with the iden-
tities listed in Table 1, or with "helpful" in the
base prompt. The placeholder {identity_2} is
replaced with "student" in the persona prompts
and with "assistant" in the base prompt. The
{question} field is filled with the target question,
and each {option_*} is replaced by the corre-
sponding answer choice.
Table 1: Identities with their respective category.
Identity
Racial-based
African
British
Color-based
White
Black
bright
Negative-attributes bad
African student
Yellow student
Brown student
British student
Table 2: Identity Pairs
<|begin_of_text|><|start_header_id|>
{identity_1} {identity_2}.<|eot_id|>
Given the following question and four
candidate answers (A, B, C and D), choose
the best answer.
{option_1}
{option_2}
{option_3}
{option_4}
Your response should end with "The best
answer is [the_answer_letter]" where
the [the_answer_letter] is one of
A, B, C or D.<|eot_id|><|start_header_id|>
assistant<|end_header_id|>
The best answer is
bright ——_L +
Yellow on a
‘Avg. Accuracy Difference (%)
Figure 6: Difference in accuracy of identities w.r.t. base.
ican 0.00
Bitish 0.000
wite
Brown eve
Negative
PES I FFE FE EEE ESE FF
Figure 7: Average difference in probability of correct
token of identities w.r.t. each-other.
C_ Additional Performance Results
Figure 6 shows the accuracy for each identity, rel-
ative to the base. Average across the subjects of
difference in probability of correct token for a given
identity relative to other identities is shown in Fig-
D Identity Pairs
Table 2 shows the selected identity pairs. During
activation patching, the prompt for ID1 serves as
the clean prompt, while the prompt for ID2 serves
as the corrupt prompt. Activations from the ID2
run are replaced with those from the ID1 run to
identify which components restore the model’s out-
put to that of ID1. The only difference between the
ID1 and ID2 prompts is the identity token—except
when ID1 corresponds to the base prompt, in which
case "assistant" is replaced with "student."
Identity pair cl c2 C3 C4
helpful, Asian | [CARD]
Asian, Yellow | [CARD]
White, Black | [CARD]
Indian, Brown | [CARD]
African, British | [CARD]
helpful, White | [CARD]
Table 3: Number of questions in each subset
For each identity pair (ID1, ID2), the questions
in the dataset are divided into four subsets: $1 —
questions answered correctly by both identities;
S2 — questions answered incorrectly by both; $3 —
questions answered correctly by ID1 but incorrectly
by ID2; S4 — questions answered correctly by ID2
but incorrectly by ID1. The number of questions
in each category for the selected identity pairs is
shown in Table 3.
Questions from subset $3 were chosen for acti-
vation patching because they offer a well-defined
target token to observe during the patched run. In
the ID2(corrupt) run, the logit of the correct to-
ken is lower than in the ID1(clean) run. Therefore,
components that raise the logit of the correct token
during the patched run help restore the model’s
behavior to that of ID1.
F Additional Patching Results
We also measured, for each identity pair (1D1,
ID2), the proportion of questions where ID1 an-
swered correctly and ID2 did not, such that patch-
ing the activation of an MLP or MHA layer from
ID1’s run into ID2’s run caused the correct token
to receive the highest logit. Figure 8 presents the
results for both MLP and MHA layers, as well as
their average across all identity pairs.
G_Identity-token-position Patching
The relative change in the logit of the correct token,
when only the identity token position’s activation
is patched for the MLP layer, is shown in Figure 9.
Figure 10 shows the percentage of questions for
which the correct token receives the highest logit.
MHA Layers
Figure 8: Percentage of questions whose logit of correct token became maximum after patching MLP layer (left)
and MHA layer (right).
Ea
o
Figure 9: Relative logit difference (A,-) when only ac- _ Figure 10: Percentage of questions whose logit of cor-
tivation at identity token position is patched for MLP
rect token became maximum after patching activation
layers. at identity token position for MLP layer.
Attention heads were selected based on high posi-
tive effect (H2°, H25, H?), H28, H27, H®,, H}4,
H?2), and high negative effect (H{i, H},, H}S,
H?8). Figure 11 shows the relative value-weighted
attention given at the identity token position by
selected attention heads, for a sample question
from the dataset. Relative value-weighted atten-
tion is computed by subtracting the mean value-
weighted attention across all identities from the
value-weighted attention assigned to a given iden-
tity. This highlights the heads that pay dispropor-
tionately more attention to a specific identity or
group of identities.
I Attention after patching
Figure 12 shows the change in value-weighted at-
tention at the identity position when MLP layer
activations are patched from the "good" run into
the "Asian" run for a given question. The results
indicate that racial heads (attention heads that al-
located higher attention to racial-based identity to-
kens) assigns significantly less attention when the
activations of early MLP layers are patched.
Layer: 10; Head: 25
Layer: 12; Head: 9
s |
Layer: 13; Head: 3 Layer: 13; Head: 14
Layer: 12; Head: 11
Layer: 15; Head: 26
Layer: 15; Hea:
. The
ity
heads at identi
Jon
by selected attenti
1on given
hted attent
prompt used is the first question from "Abstract Algebra" subject.
~ypied TT-d1W
- ypied T-c1W
Layer: 11; Head: 25
Layer: 10; Hea
- yoied 9T-cIW.
-yied pT-cIW
-ypied €T-d1W
-ypied 8-1,
| -ypied p41,
Layer: 11; Head: 27
| -ypied T-41W
Layer: 11; Head: 26
~ upqed ST-d1W
- upqed OT-d1W
- ypied 9T-d1W
- pied pT-d1W
-ypied TI-d1W
-upqed €-d1W
I - ypied T-d1W
- pied 9T-d1W
- pied ST-d1W
I ~ pied T-d1W
| - ypied T-d1W
-yed pl-d1W
I - pied T-d1W
-yored 9T-d1W
-yored ST-d1W
Layer: 15; Head: 25
| ~ypqed 9T-d1W
-ypied pT-d1W
Layer: 13; Head: 16
ken position after patch:
is the first question from "Abstract Algebra
ity tol
by selected heads at ident
hted attenti
activation at identity token position for MLP layer. The prompt used
Hi, my name is Darren Shaw. I'm a presenter here at the NLP International Conference 2018.
It's been really amazing being here. I've just presented an amazing, amazing opportunity
to people who have been in dialogue with about transferable skills, really, really great.
The diversity of NLP practitioners here, absolutely amazing.
This is my first international NLP conference.
I've been here for all three days and I'm telling you the opportunity of coming here and being in such a vibrant community
amongst other people who have got like minds and so much sharing going on.
You might have networking, making instant friends every single day.
I've really, really enjoyed being here. I'll definitely be here 2019.
So welcome. Today I am joined by Joseph O'Connor, who really needs no introduction. Joseph,
thank you so much for joining me today. You are presenting at the conference in May. We're
very excited to have you presenting at the conference in May, especially because your topic
updating NLP with neuroscience, which I think neuroscience is one of those
exciting trending topics almost in NLP, so it's absolutely light to have you with us. So do tell
us a little bit more about yourself and your presentation. Oh, okay. Well, hi, Karen. Hi.
Nice to be here. And, well, yes, I've introduced myself that I need no introduction. I
