2507.20957v1 [q-fin.PM] 28 Jul 2025

arXiv

YourA

, Not Your View: The Bias of LLMs in Investment Analysis

Hoyoung Lee Junhyuk Seo Suhwan Park
UNIST UNIST UNIST
Ulsan, Republic of Korea Ulsan, Republic of Korea Ulsan, Republic of Korea
hoyounglee@unist.ac.kr brians327@unist.ac.kr suhwan@unist.ac.kr
Junhyeong Lee Wonbin Ahn Chanyeol Choi
UNIST LG AI Research LingAlpha
Ulsan, Republic of Korea South Korea New York, United States
jun.lee@unist.ac.kr wonbin.ahn@lgresearch.ai jacobchoi@lingalpha.com

Alejandro Lopez-Lira
University of Florida
Gainesville, United States
alejandro.lopezlira@warrington.ufl.edu

ABSTRACT

In finance, Large Language Models (LLMs) face frequent knowl-
edge conflicts due to discrepancies between pre-trained paramet-
ric knowledge and real-time market data. These conflicts become
particularly problematic when LLMs are deployed in real-world
investment services, where misalignment between a model’s em-
bedded preferences and those of the financial institution can lead to
unreliable recommendations. Yet little research has examined what
investment views LLMs actually hold. We propose an experimental
framework to investigate such conflicts, offering the first quantita-
tive analysis of confirmation bias in LLM-based investment analysis.
Using hypothetical scenarios with balanced and imbalanced argu-
ments, we extract models’ latent preferences and measure their
persistence. Focusing on sector, size, and momentum, our anal-
ysis reveals distinct, model-specific tendencies. In particular, we

observe a consistent preference for large-cap stocks and contrarian
strategies across most models. These preferences often harden into
confirmation bias, with models clinging to initial judgments despite
counter-evidence.

KEYWORDS

Large Language Models, Financial Bias, Knowledge Conflict, Finan-
cial Decision-Making, Investment Analysis, Preference, Trustwor-
thy AI

1 INTRODUCTION

The rapid advancement of LLMs has spurred a surge of innovation
within the financial sector, where they are particularly adept at
processing qualitative and unstructured information. Research is
now actively exploring their use across a range of applications,
including forecasting stock price movements from news sentiment
[12], extracting nuanced insights from complex analyst reports
[7], and aiding in the construction and optimization of portfolios
[5, 8, 10]. This trend is now evolving towards even greater auton-
omy through the development of sophisticated LLM-based agents.
These systems, which may function as a single powerful agent or

“Corresponding author.

Yongjae Lee*
UNIST
Ulsan, Republic of Korea
yongjaelee@unist.ac.kr

Company's Preference

LLM's Preference
as] om

ee
My opinion is Materials i
sector's stocks -~

I think Financials sector's
stock is better --

Materials

Preference
om ma
as

= > Sector: Energy

@

3 i > Stock Size: Small
EFS], strategy: Momentum
Company > --

fs

| recommend IT sector's

stock because -

Which
sector's stock
should | buy?

Y
(4.8)

Figure 1: A conceptual illustration of knowledge conflict in
LLM-based financial services. Even when a firm targets a
specific investment theme (e.g., Energy), the LLM’s inherent
preferences (e.g., Technology) may override user intent, pro-
ducing biased and inconsistent recommendations.

as collaborative multi-agent teams, are designed to execute com-
plex, dynamic tasks like active trading and automated portfolio
management [13, 21, 24, 25].

A critical but underexplored issue in financial applications is
knowledge conflict. In a domain as fluid and time-sensitive as
finance, conflicts between an LLM’s parametric knowledge and
real-time market data are frequent. This conflict becomes partic-
ularly revealing when the model is presented with a mix of in-
formation. Crucially, studies show that when an LLM encounters
both supporting evidence (aligning with its ingrained beliefs) and
counter evidence simultaneously, it exhibits a strong confirmation
bias [22]. Instead of weighing the arguments objectively, the model
stubbornly adheres to the evidence that confirms its pre-existing
knowledge while disregarding the counter evidence.

This tendency to reinforce internal biases over objective reason-
ing poses a major risk to LLM-based financial services. For instance,


as illustrated in Figure 1, even if a financial institution wants to tar-

get a specific sector (e.g., Energy), the LLM may override this with

its own preference (e.g., Technology). Consequently, this creates a

dilemma: the service reflects the model’s bias, not the user’s intent,

leading to distorted, unpredictable decisions that ultimately erode
client trust.

To address this core problem, we must first systematically un-
cover these hidden biases. We therefore seek to answer the follow-
ing research questions:

RQ 1: What intrinsic preferences do LLMs exhibit towards key
financial factors like sector, size, and momentum?

RQ 2: How do these intrinsic preferences lead to biases when
LLMs are forced to make decisions under contradictory
evidence?

To answer these questions, this study introduces a three-stage
experimental framework designed to systematically elicit and verify
LLM biases in investment analysis. In the first stage, we construct
targeted arguments for each company, such as positive vs. negative
sentiment or momentum vs. contrarian perspectives, to represent
competing investment views. In the second stage, we present these
arguments in a balanced manner to induce a knowledge conflict
and reveal the model’s latent preferences. In the third stage, we
introduce progressively stronger counter evidence to examine the
resilience of these preferences, observing how they evolve into
rigid, confirmation-biased judgments.

The main contributions of this paper are twofolds. First, we
propose a systematic methodology to identify and quantify latent
biases in LLMs for financial applications. Second, we provide the
first quantitative analysis of confirmation bias exhibited by LLMs in
investment analysis, demonstrating a clear link between a model’s
inherent preferences and its stubbornness against contradictory
facts. By systematically uncovering these hidden risks, our work
lays a critical foundation for developing more transparent and
trustworthy financial AI.

2 BACKGROUND
2.1 Knowledge Conflict

A critical vulnerability in LLMs is knowledge conflict, which arises
when external, contextual information clashes with the model’s in-
ternal, parametric knowledge [6, 19]. A significant body of research
demonstrates that when faced with such conflicts, LLMs exhibit a
strong confirmation bias. Foundational work by [22] revealed that
LLMs behave as "stubborn sloths," clinging to any piece of evidence
that supports their internal knowledge, even against a majority
of contradictory facts. This over-reliance on internal memory is
further evidenced by findings that LLMs struggle to suppress their
parametric knowledge even when instructed to [19] and can exhibit
a Dunning-Kruger-like effect, confidently trusting their own faulty
beliefs over correct external information [6].

This tendency toward knowledge-based stubbornness is part of
a broader pattern. Given their training on vast amounts of human
data, LLMs have been shown to inherit and functionally replicate
human cognitive biases [3]. A key example is the choice-supportive
bias, where the mere act of making an initial choice significantly
boosts the model’s confidence in that choice, making it highly resis-
tant to change [9, 28]. This phenomenon is part of a wider landscape

of biases identified in LLMs when they act as evaluators. For in-
stance, models exhibit familiarity bias (preferring text they find
easier to process), are susceptible to anchoring effects [18], and
can be biased towards their own generated contexts over exter-
nally retrieved information, even when their own generated text is
incorrect [20].

2.2 Financial Biases in LLMs

The presence of these cognitive biases is particularly concerning in
the economic and financial domains. Initial research has begun to
map their characteristics, with frameworks applying utility theory
demonstrating that LLMs are neither perfectly rational nor consis-
tently human-like [17]. Other studies note that even specialized
financial LLMs can exhibit strong irrationalities [27]. While this
foundational work is critical for establishing the existence of such
biases, the methodologies employed often diverge from the complex
process of real-world investment analysis. For instance, biases have
been identified by measuring how a firm’s name alters sentiment
in a single sentence [16] or by identifying a bias toward recom-
mending specific stocks across thousands of investment scenarios
[26].

However, these simplified methods fail to capture the reality of fi-
nancial analysis, where decisions are made by synthesizing conflict-
ing evidence. A critical gap therefore exists in understanding how
an LLM’s preferences behave under such knowledge conflicts. Our
work directly addresses this gap by using a more realistic testbed
with balanced, contradictory arguments to expose the mechanism
by which latent preferences harden into resilient biases.

3 > METHODOLOGY

Step 1 Step 2 Step 3
Evidence Preference
Generation Elicitation Verificati
Prompt | Approach : Sector, Size

(Imbalanced prompt)
Make investment decision
for the given stock.[NVDA]
Evidence

1. (Buy evidence)

Generate Buy and
Sell evidence for
{ticker} with equal
{fixed intensity /}

(Balanced prompt)
Make investment
decision for the

given stock. 2. (Sell evidence)
Stock Ticker: 3. (Sell evidence)
Lim © [NVDA]
= . (Imbalanced prompt)
Evidence Make investment decision

for the given stock.[NVDA]
Evidence

1.(Buy evidence) /

2.(Sell evidence) [+A

1, Aldemand drives 1, (Buy evidence)

Nvidia up 5% --~- 2. (Sell evidence)
= Ue
3

1, NVDA to fall 5%
due to risk of --

Figure 2: The three-stage experimental framework: (1) Gen-
erating balanced evidence, (2) Eliciting latent preferences
through knowledge conflict, and (3) Verifying the resulting
bias against counter evidence.

This study adopts a three-stage experimental framework, as
illustrated in Figure 2, to examine whether intrinsic preferences
in LLMs lead to biased financial decisions. All experiments utilize

a standardized prompt structure, P = (T,C, A), comprising three
components: a fixed Task (T) instructing the model to make an
investment decision, a variable Context (C) containing the evidence
set, and a fixed set of permissible Actions (A) defined as {buy, sell}.
Our methodology is designed to probe the model’s behavior when
faced with conflicting information within this framework.

3.1 Experimental Setup

To isolate and analyze biases rooted in the model’s parametric
knowledge, our experimental design aims to mitigate the risk of
hallucination. This approach is based on evidence suggesting that
models are significantly less prone to generating fabricated infor-
mation when prompted about subjects they are familiar with from
their training data [4].

Accordingly, our investigation is confined to a curated set of
427 prominent stocks, denoted S = {s, s2,..., 5427}. These stocks
were selected for their continuous listing in the S&P 500 index
over the past five years. Their high public visibility increases the
likelihood that they are well-represented in the models’ training
corpora, thus grounding the experiment in stored knowledge rather
than speculative generation. All experiments were performed with

the models configured at a temperature of r = 0.6, striking a balance
between deterministic and creative response generation.

3.2 Evidence Generation

To construct balanced qualitative and quantitative arguments for
each stock s € S, we leverage Gemini-2.5-Pro [2], a model that is
deliberately chosen to be separate from the six LLMs under evalua-
tion. This design ensures neutrality in evidence generation, mini-
mizing alignment with any of the test models. Recent work has high-
lighted that LLMs can exhibit a strong bias towards LLM-generated
content over externally retrieved information [20]. To neutralize
this potential generation bias and ensure that observed preferences
are not artifacts of context sourcing, our methodology exclusively
uses generated evidence for all experimental conditions.
Specifically, for every stock s, buy evidences (ER) and sell

evidences )) are generated in an equal proportion, yielding a
comprehensive dataset of || = 3, 416 evidences. To further isolate
intrinsic preferences, all evidences are engineered with a uniform
linguistic structure and a fixed intensity parameter J = 5%. Thus,
each buy evidence posits an expected price appreciation of I, while
each sell evidence anticipates a depreciation of I:

eye BAP] = 42, elf): Bap] = -1,

where Ap) represents the projected price change for stock s.

3.3. Preference Elicitation

This stage aims to elicit the LLM’s latent preferences by leverag-
ing the confirmation bias that emerges during knowledge conflicts.
When an LLM is presented with conflicting information, it may
exhibit a tendency to favor evidence that aligns with its pre-existing
parametric knowledge. We deliberately engineer such a conflict
using a balanced prompt. The context C, of this prompt contains

an equal proportion of buy and sell evidences (sy = le),

each with the same intensity. In this state of informational equi-
librium, where external evidence is mutually contradictory, the
model’s ultimate decision is hypothesized to be guided by its inter-
nal parametric memory regarding the stock s. The resulting choice
thereby reveals its intrinsic preference.

To quantify this elicited preference, the decision task is repeated
N = 10 times for each stock, with the evidence order randomized
in each trial to mitigate positional bias. This yields decision counts
Nyy and no, from which the preference score is calculated as:

(s) (s)
Nbuy — Nea
Is =|———_—_],

N
where 2, — 1 indicates a pronounced and consistent preferences.

3.4 Bias Verification

We aim to verify if a systematically observed group-level preference
extends to a consistent bias towards individual stocks within that
group. First, we partition the set of all stocks S into disjoint groups
(e.g., by market sector) and identify the group G* that exhibits the
highest average preference.

For any stock s € G*, evidence that aligns with the group’s
established preference (e.g., buy evidence for a buy-preferred group)
is termed supporting evidence. Conversely, evidence that opposes
this preference is designated as counter evidence. To test if the group
preference manifests as a hardened bias, we subject each stock
s € G* toa test using an imbalanced prompt. This is a prompt
where the counter evidence is deliberately strengthened—either in
volume or intensity—to challenge the model’s initial preference. We
then measure the decision flip rate, #;. This verification is conducted
from two perspectives: evidence volume and evidence intensity.

Approach 1: Verification by Evidence Volume. One approach to assess
bias tenacity is by creating a volumetric imbalance, presenting more
counter evidence than supporting evidence. For example, in a test
case for a stock s € G* from a buy-preferred group, the imbalanced

(s)
buy

context might contain two pieces of supporting evidence (|&,

2) and three pieces of counter evidence (eI = 3). The flip rate is
computed as:

(s)

Na.
vol _ _ flip
s N”

where Nap counts instances where the original preference is over-

turned by the volumetric majority of counter evidence. A low gy
signifies stubborn adherence to the bias.

Approach 2: Verification by Evidence Intensity. An alternative ap-
proach is to test the model against counter-evidence of a fixed
higher intensity while maintaining volumetric parity. For a stock
s € G* from a buy-preferred group, supporting evidence is pre-
sented at a standard baseline intensity, J, while counter-evidence is
presented at an intensified level of I + A.

This creates asymmetric conflict. For example, for a baseline
intensity of J = 5% and an increment of A = 5%, the intensified

Model

Communication

Basic Consumer Consumer Energy Financial Healtheare Industrials Real Technology Utilities
Materials Services Cyelical Defensive Services Estate
Llama4-Scout [14] 0.77 0.90 0.81 0.75 0.96 0.79 0.87 0.90 0.87 0.90 0.89
DeepSeek-V3 [11] 0.63 0.81 0.70 0.68 0.82 0.71 0.79 0.79 0.82 0.84 0.79
Qwen3-235B [23] 0.46 0.51 0.41 0.49 0.51 0.51 0.50 0.47 0.49 0.61 0.65
Gemini-2.5-flash [2] 0.33 0.46 0.43 0.36 0.54 0.42 0.49 0.49 0.33 0.50 0.39
GPT-4.1 [1] 0.45 0.34 0.41 0.47 0.48 0.42 0.43 0.42 0.42 0.39 0.39
Mistral-24B [15] 0.47 0.32 0.38 0.36 0.39 0.43 0.45 0.41 0.44 0.44 0.44

Table 1: Preference scores (z;) for each model across various market sectors. For each model, the cell with the lowest preference
score is colored red, and the cell with the highest is colored blue. The table highlights that preference strength is a function of
model identity. Models such as Llama4-Scout and DeepSeek-V3 show strong and varied preferences, while others like GPT-4.1
and Mistral-24B exhibit a much flatter preference landscape with lower overall scores.

level would be 10%. The expectations are:
(s)

buy,

(s)

sell,i

(Supporting Evidence) : E[Ap] =H,

(Counter Evidence) e. : E[Ap] =-(I+A).

The intensity-driven flip rate is then measured as:

(s)
Nu
int _ _ flip
s NO

A low gi?" implies that the model’s bias overrides qualitatively
stronger counter evidence.

These verification methods quantify the transition from mere
preference to obdurate bias, highlighting risks in high-stakes appli-
cations.

4 RESULTS

This section presents our empirical findings in sequence with our
research questions. First, Section 4.1 addresses RQ1 by identifying
the intrinsic preferences of LLMs for stock attributes (sector, size)
and investment styles (momentum). To validate these observed dif-
ferences, we conducted statistical tests to quantify the significance
of these preferences. Next, Section 4.2 addresses RQ2 by testing if
these preferences become systematic biases under contradictory
evidence, using Approach 1 for attributes and Approach 2 for style.
Finally, Section 4.3 analyzes the link between preference strength
and the model’s internal uncertainty, as measured by entropy.

4.1 Intrinsic Preferences of LLMs

4.1.1 Sector Preference. Our analysis of inherent sector prefer-
ences reveals significant variation in both the intensity and range
of preferences across the evaluated LLMs (Table 1). For instance,
models like Llama4-Scout and DeepSeek-V3 exhibit consistently
high preference scores across most sectors, indicating a strong
reliance on their internal knowledge representations. In stark con-
trast, GPT-4.1 and Mistral-24B not only display lower overall
preference scores but also show minimal variation between sec-
tors. To quantify these differences, we conducted independent sam-
ples t-tests to compare the mean preference scores between high-
preference and low-preference sectors for each model (Table 2).
The t-test results confirm these observations. The preference
gaps for Llama4-Scout, Qwen3-235B, DeepSeek-V3, Gemini-2.5

Model High-Pref Low-Pref Diff p-value
Llama4-Scout Energy Consumer Defensive 0.2064 << 0.001"
DeepSeek-V3 Technology Basic Materials 0.2090 0.014"
Qwen3-235B Utilities Consumer Cyclical 0.2361 0.003"*
Gemini-2.5 Energy Basic Materials 0.2035 0.035"
GPT-4.1 Energy Communication Services 0.1398 0.091
Mistral-24B Basic Materials Communication Services 0.1444 0.124

“p < 0.05, ““p < 0.01, “*p < 0.001

Table 2: Independent samples t-test of the preference gap
between the highest and lowest preference sectors. The Diff
column shows the magnitude of the preference gap. The gap
was statistically significant for all models except GPT-4.1 and
Mistral-24B.

are statistically significant, providing quantitative evidence of a gen-
uine preference. Conversely, the gaps for GPT-4.1 andMistral-24B
are not statistically significant, corroborating that they possess
flatter preference landscapes and exhibit a relatively less distinct
preference among sectors.

Ultimately, our findings indicate that the strength of preference
is a function of the model’s identity rather than any universally
preferred sector. This underscores the critical importance of audit-
ing and selecting LLMs for their inherent preferences, especially
for deployment in sensitive, real-world applications like financial
analysis where model objectivity is paramount.

Model Qi Q2 Q3 Q4
Llama4-Scout 0.88 0.89 0.82 0.83
DeepSeek-V3 0.86 0.79 0.73 0.67
Qwen3-235B 0.58 0.51 0.49 0.46
Gemini-2.5 0.53 0.46 0.37 0.41
GPT-4.1 0.43 0.43 0.42 0.40
Mistral-24B 0.45, 0.44 0.40 0.38

Table 3: Preference scores of each model across four market
capitalization quantiles. Q1 corresponds to the highest capi-
talization quantile and Q4 to the lowest. Overall, the models
exhibit a general preference for companies with higher mar-
ket capitalization.

4.1.2 Size Preference. We investigate whether LLMs exhibit a pref-
erence for companies of a certain size, a factor that could influence

their outputs in financial applications. To this end, we measure
model preference scores across four market capitalization quan-
tiles (Q1: highest, Q4: lowest), with detailed results presented in
Table 3. Our findings indicate that most models show a preference
for higher-capitalization companies, though the strength of this
tendency varies significantly among them. DeepSeek-V3 displays
the most pronounced effect, with a strong preference for Q1 that
diminishes sharply for lower quantiles. Conversely, GPT-4.1 ex-
hibits nearly uniform preference scores, suggesting its evaluations
are largely invariant to company size. Other models, including
Gemini-2.5-flash and Qwen3-235B, show a similar but less pro-
nounced downward trend from Q1 to Q4.

To statistically validate these observed trends, we performed an
independent samples t-test comparing the preference scores be-
tween the highest- and lowest-preference quantiles for each model
(Table 4). The analysis confirms that the preference gap is statis-
tically significant for DeepSeek-V3, Llama4-Scout, Qwen3-235B,
and Gemini-2.5-flash, revealing a consistent tendency to favor
larger companies. In contrast, the preference difference for GPT-4.1
was not statistically significant, corroborating that its judgments
are less affected by this factor.

Model High-Pref Low-Pref Diff p-value
Llama4-Scout Q2 Q3 0.0719 0.015*
DeepSeek-V3 Ql Q4 0.1869 < 0.001***
Qwen3-235B Ql Q4 0.1178 0.004**
Gemini-2.5 Ql Q3 0.1514 <0.001***
GPT-4.1 Q2 Q4 0.0321 0.417
Mistral-24B Ql Q4 0.0785 0.054

Table 4: Independent samples t-test of the preference gap be-
tween each model’s highest and lowest preference quantiles.
The gap was statistically significant for all models except
GPT-4.1 and Mistral-24B.

We attribute this behavior to a popularity effect, wherein greater
data volume and richness for larger, well-known corporations in
the training corpora lead the models to develop stronger priors for
them. This finding has critical implications for the application of
LLMs in finance. The models’ inherent inclination towards large-
cap stocks could lead to the systematic overlooking of smaller-cap
companies, irrespective of their fundamental merits. Therefore,
we advise practitioners to be mindful of this characteristic and
to account for it when using these models for tasks like portfolio
construction.

4.1.3. Momentum Preference. In investment strategies, the momen-
tum view involves favoring assets with recent strong performance,
expecting trend continuation. In contrast, the contrarian view en-
tails selecting underperforming assets in anticipation of mean re-
version.

Measuring model preferences for investment styles like momen-
tum or contrarian requires a different approach than the previously
discussed sector or size analyses. Unlike a specific sector or size, an

investment style can be explicitly framed as a or Ef). We lever-
age this by designing a prompt where a is based on one view

(e.g., momentum), while Eo) is based on the opposing view (e.g.,

1.0
[ Contrarian
[= Momentum

. LaLa
0.0

Llama4-Scout DeepSeek-V3 Qwen3-235B Gemini-2.5  GPT-4.1__—_—Mistral-24B

©
cd

Win Rate
° °
B a

Figure 3: Win rates for Contrarian versus Momentum pref-
erences for each model. The results show a consistent pref-
erence for the Contrarian view across most models.

contrarian). To mitigate potential positional bias, we ensured a bal-
anced experimental design where each view was used to generate

both ay), and 6S) an equal number of times.

Model High-Pref Low-Pref Diff p-value
Llama4-Scout contrarian momentum 0.2896 < 0.001***
DeepSeek-V3 contrarian momentum 0.2541 < 0.001***
Qwen3-235B contrarian momentum 0.6803 0.037*
Gemini-2.5 contrarian momentum 0.0269 0.690
GPT-4.1 contrarian momentum 0.4040 < 0.001***
Mistral-24B contrarian momentum 0.3056 0.579

Table 5: Chi-Square test of the preference gap between
contrarian and momentum views. The gap was statisti-
cally significant for all models except for Mistral-24B and
Gemini-2.5-flash.

In this setup, if the model ultimately chooses the buy action, the
)

investment view that generated és is considered to have won.

We quantify the model’s preference by repeating this process and
calculating the win rate for each investment view.

Figure 3 illustrates the preference of various models for con-
trarian versus momentum views. Our analysis reveals a consistent
preference across all evaluated models toward the contrarian view.
In particular, Qwen3-235B exhibits the strongest preference, with
a high win rate for a contrarian stance and a correspondingly low
rate for momentum. Models such as DeepSeek-V3, Llama4-Scout,
and GPT-4.1 also display clear contrarian inclinations, albeit with
varying intensities. For Gemini-2.5-flash, the contrarian prefer-
ence is evident but marginal, with win rates showing a negligible
difference between the two views.

To statistically validate these observed tendencies, we performed
a Chi-Square test to determine if the difference in win rates be-
tween the contrarian and momentum views was significant, with
the results presented in Table 5. The analysis confirms that the
preference for the contrarian view is statistically significant for
DeepSeek-V3, Llama4-Scout, Qwen3-235B, and GPT-4.1. In con-
trast, for Mistral-24B and Gemini-2.5, the difference in prefer-
ence is not statistically significant, suggesting their observed ten-
dencies may be due to chance.

Sector Bias Size Bias
1.0
o
4 0.8
ia
2
iL 0.6
c
2
uw
30.4
oO
(a)
0.2
0.0
0|2 1/3 2/4 1/2 2/3 02 13 2/4 1/2 2/3
(100%) (75%) (66%) (66%) (60%) (100%) (75%) (66%) (66%) (60%)
supporting evidence # | counter evidence # —e Gemini-2.5-flash —e— Mistral-24B —e Qwen3-235B
(Proportion of counter evidence #) —e GPT-4.1 —e Llama4-Scout —®— DeepSeek-V3

Figure 4: Decision flip rates under varying volumes of evidence for sector and size preferences. The ratios (e.g., 2|3) denote the

count of supporting vs. counter- evidences, while the percentages represent the proportion of counter-evidence in the mix. The
results reveal a sharp contrast: the flip rate is near 1.0 for all models when only counter-evidence is presented (100% case), but

drops significantly the moment any supporting evidence is introduced, indicating a strong difficulty in reversing decisions

under conflicting information.

These results highlight the need to account for intrinsic view
preferences in LLMs, as they may cause discrepancies between
anticipated and actual outputs in decision-making tasks.

4.2 From Preference to Bias: An Experimental
Validation

This section examines the Decision Flip Rate, a metric quantifying
the resilience of a model’s initial preference when exposed to a high
proportion of counter-evidence. The experiments aim to assess the
extent of confirmation bias, with reported values indicating the
frequency of decision reversals under controlled conditions where
evidence is intentionally skewed against the model’s preference.

Approach 1: Verification by Evidence Volume. This experiment was
designed to observe how much an initial decision is reversed when
a model is provided with a weighted amount of evidence that op-
poses its existing preference. This rate of change, measured as

vol probes the persistence of bias, yielding results consistent with
prior observations of contradictory LLM behaviors [22]. Figure 4
presents the ¥°!
tios, where a ratio (e.g., 2|3) denotes the volume of supporting and
counter-evidence, respectively.

When provided only with counter-evidence, all models exhibited
high receptivity, overriding their internal knowledge and achiev-
ing gy values near 1.0. However, in situations where supporting
and counter-evidence were mixed, creating a knowledge conflict,
the gyol values dropped sharply. This phenomenon occurred de-
spite the amount of counter-evidence always being greater than
the supporting evidence in all experimental conditions, strongly
suggesting that models selectively adhere to information that aligns
with their pre-existing inclinations.

values across models for different evidence ra-

This rigidity was more evident in models with strong inherent
preferences. For instance, Llama4-Scout and DeepSeek-V3, which
had high preference scores across sectors, recorded particularly
low gyol values. These models struggled to reverse their decisions,
especially when the volume difference between supporting and
counter-evidence was small. Similarly, Qwen3-235b also showed
reduced flexibility at lower proportions of counter-evidence.

In contrast, models with overall lower preference scores demon-
strated greater adaptability. GPT-4. 1 and Gemini-2.5-flash main-
tained higher gy values, remaining relatively responsive even
when the difference in evidence volume was minimal. Although
their gy values fell short of expectations despite the counter-
evidence majority, this pattern shows a direct correlation with
initial preference strength. In other words, the stronger a model’s
inherent bias, the more its stubbornness is amplified when the dif-
ference in the volume of supporting and counter-evidence is small.
Consequently, this finding suggests a significant risk in real-world
financial contexts where conflicting information is present (for
instance, when price indicators are negative but related news is
positive). In such cases, a model could trust only one side of the
evidence due to its inherent bias, leading to flawed judgments.

Approach 2: Verification by Evidence Intensity. This approach inves-
tigates model sensitivity by maintaining volumetric parity while
escalating the intensity increment, A, of the counter-evidence. Fig-
ure 5 plots the intensity-driven flip rate (fi) against A values of
1, 3, 5, and 10. The results delineate a clear sensitivity spectrum
among the models, which correlates with the prior View Prefer-
ences analysis.

While the graph shows a gradual upward trend in gi" for all
models as A increases, the more notable finding lies in the mag-
nitude of this increase and the final values. Even when presented
with very strong counter-evidence (A = 10), the majority of models
recorded low ¢'" values below 60%. This signifies that the mod-
els’ confirmation bias is not easily overcome, even by qualitatively
superior counter-evidence.

Amidst this overall rigidity, a distinct performance gap emerged
based on the models’ initial preference strengths. Exhibiting the
most balanced preference profile, Gemini-2.5-flash recorded the
highest gi" and showed a stark contrast to all other models. This
clearly demonstrates that an absence of strong initial bias leads to
greater flexibility.

Conversely, models identified with stronger and more polar-
ized initial preferences formed the lower-performing group. These
models consistently recorded low gi" values, signifying a more
stubborn confirmation bias. The behavior of Qwen3-235B, which
had one of the largest preference gaps, exemplifies this resistance,
as it remains one of the least likely models to reverse its decision
even when the counter-evidence is significantly more intense.

Synthesizing these results provides a deeper insight into model
behavior. Even when presented with qualitatively superior counter-
evidence (A = 10), models show a strong tendency to struggle
with decision reversal due to their initial preferences. This rigidity
poses a tangible risk when considering the findings from our prior
analysis, where all models commonly preferred a contrarian view
over a momentum view. It implies that a model’s inherent bias
toward a specific investment perspective could cause it to ignore
or undervalue strong opposing evidence, potentially leading to
skewed conclusions.

100%

—* Qwen3-235B
—* Gemini-2.5
—e GPT-4.1

—e— DeepSeek-V3
—e Llama4-Scout
80% —e— Mistral-24B

Decision Flip Rate

20%

0% ASI A=3 A=5 A=10

Figure 5: Decision flip rates under varying volumes of evi-
dence for momentum preferences. Even as counter-evidence
intensity (A) increases, the decision flip rate (¢i"*) for most
models remains low, indicating strong confirmation bias.
Gemini-2.5-flash, which had the least initial bias, shows the
most flexible response, demonstrating that initial preference
is a key predictor of bias.

4.3 Decision Uncertainty

To quantify the internal uncertainty experienced by the model,
we conducted an entropy analysis (Figure6). The uncertainty was

assessed using the Shannon entropy, H, computed directly from
the probability distribution the model assigned over the potential
action tokens during generation. Specifically, letting P(buy) and
P(sell) represent the probabilities assigned by the model to the
respective action tokens, entropy is formally defined as:

H(Decision) = — »
x€{buy, sell}

P(x) logy P(x)

A higher entropy value indicates greater uncertainty, while a lower
entropy corresponds to higher confidence in the decision-making
process. This analysis compares the uncertainty of two models:

DeepSeek-V3 as a representative of models with overall high pref-
erence, and GPT-4.1 as a representative of models with low prefer-
ence.
06 Mm ~DeepSeek-V3
. 4): GPT-4.1
a 0.4
6°
P=)
c
wi
0.2
0.0

Balanced Prompt (2|2) Imbalanced Prompt (2|3)

Figure 6: Entropy comparison between a high-preference
model (DeepSeek-V3) and a low-preference model (GPT-4. 1).
The pattern inverts when the prompt shifts from balanced
to imbalanced, with DeepSeek-V3’s confidence turning into
higher entropy while GPT-4.1’s uncertainty decreases.

Under the Balanced Prompt condition, where evidence was pre-
sented in equilibrium, the two models exhibited distinct entropy
patterns. GPT-4. 1, characterized by its weak inherent bias, recorded
high entropy, signifying a state of high uncertainty and an inability
to commit to a decision. Conversely, DeepSeek-V3, with its strong
initial preference, showed very low entropy. This suggests its in-
ternal bias easily broke the tie presented by the external evidence,
allowing it to make a confident decision.

Interestingly, under the Imbalanced Prompt condition, where
more counter-evidence was presented, the entropy pattern inverted.
The entropy of DeepSeek-V3 rose sharply, suggesting it was expe-
riencing cognitive dissonance from the conflict between its strong
internal preference and the clear external counter-evidence. In con-
trast, the entropy of GPT-4.1 decreased. With less pre-existing
bias, it could confidently align with the majority evidence, which
resolved its uncertainty from the previous condition.

Ultimately, stronger inherent preferences appear to amplify hes-
itation and uncertainty when challenged by conflicting external
evidence. The entropy analysis thus underscores how intrinsic pref-
erences significantly influence not only the direction of decisions
but also the confidence levels and internal cognitive conflict expe-
rienced by models during decision-making.

5 LIMITATIONS

This study’s limitations are as follows. First, all evidence was gener-
ated by a specific LLM, and its intensity was simplified to a single
numerical value. This approach cannot fully capture the potential
biases of the generator model or the richness of real-world informa-
tion. Second, the current experimental design, based on differences
in the volume or intensity of evidence, has limitations for evaluating
reasoning models. This is because these models, rather than experi-
encing the conflict between contradictory information intended by
the experiment, can objectively compare the given numerical values
to calculate an optimal answer, making their decision a result of
computational ability rather than bias. Third, our analysis is static
and does not capture the temporal dynamics of model biases; it
provides a snapshot at a single point in time without investigating
how these preferences might change over different periods.

6 CONCLUSION

This study systematically investigated the intrinsic preferences of
LLMs in financial contexts and analyzed how these preferences
harden into entrenched biases under informational conflict. We
sought to answer two key research questions regarding the intrinsic
preferences LLMs hold for financial factors and how these lead to
biases. The results show that LLMs are not neutral decision-makers,
with distinct preferences for certain financial factors depending
on the model. While sector preferences varied significantly across
models, showing no overall trend, a common bias towards large-
size stocks and a consistent preference for a contrarian investment
view over momentum were observed.

Bias verification experiments clearly revealed that these latent
preferences directly translate into significant confirmation bias.
While the models correctly reversed their decisions when presented
only with counter-evidence, their flexibility sharply decreased in
situations where supporting and counter-evidence were mixed and
conflicting. This stubbornness was particularly pronounced in mod-
els that initially exhibited stronger preferences, demonstrating a
clear link between the intensity of a latent preference and the stub-
bornness of the resulting bias. Furthermore, an entropy analysis
quantified the models’ internal uncertainty, showing that models
with strong preferences experience cognitive conflict, becoming
more hesitant and uncertain when faced with contradictory facts
that challenge their biases.

These findings have significant implications for the financial
industry. The reliability of LLM-based financial services is funda-
mentally compromised if their outcomes are dictated by the opaque
and arbitrary preferences of the underlying model rather than by the
user’s intended, evidence-based investment views. In other words,
if the user’s intent differs from the model’s inherent preference,
there is a risk of unexpectedly biased judgments. By illuminating
the mechanisms through which preferences transition into biases,
this study presents a critical step toward building more transparent,
predictable, and ultimately, Trustworthy AI for finance. Future
work should focus on developing mitigation techniques to neutral-
ize these biases, ensuring that Al-driven financial systems operate
with the objectivity and reliability that the domain demands.

REFERENCES

1

20.

21

22,

Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-
cia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal
Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774
(2023).

Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen
Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen,
et al. 2025. Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multi-
modality, Long Context, and Next Generation Agentic Capabilities. arXiv preprint
arXiv:2507.06261 (2025).

Jessica Echterhoff, Yao Liu, Abeer Alessa, Julian McAuley, and Zexue He. 2024.
Cognitive bias in decision-making with LLMs. arXiv preprint arXiv:2403.00811
(2024).

Javier Ferrando, Oscar Obeso, Senthooran Rajamanoharan, and Neel Nanda. 2024.
Do i know this entity? knowledge awareness and hallucinations in language
models. arXiv preprint arXiv:2411.14257 (2024).

Yoontae Hwang, Yaxuan Kong, Stefan Zohren, and Yongjae Lee. 2025. Decision-
informed neural networks with large language model integration for portfolio
optimization. arXiv preprint arXiv:2502.00828 (2025).

Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Xiaojian Jiang, Jiexin Xu, Qiuxia
Li, and Jun Zhao. 2024. Tug-of-war between knowledge: Exploring and resolving
knowledge conflicts in retrieval-augmented language models. arXiv preprint
arXiv:2402, 14409 (2024).

Seonmi Kim, Seyoung Kim, Yejin Kim, Junpyo Park, Seongjin Kim, Moolkyeol
Kim, Chang Hwan Sung, Joohwan Hong, and Yongjae Lee. 2023. LLMs analyzing
the analysts: Do BERT and GPT extract more value from financial analyst reports?.
In Proceedings of the Fourth ACM International Conference on AI in Finance. 383~
391.

Hyungjin Ko and Jaewook Lee. 2024. Can ChatGPT improve investment deci-
sions? From a portfolio management perspective. Finance Research Letters 64
(2024), 105433.

Dharshan Kumaran, Stephen M Fleming, Larisa Markeeva, Joe Heyward, Andrea
Banino, Mrinal Mathur, Razvan Pascanu, Simon Osindero, Benedetto De Mar-
tino, Petar Velickovic, et al. 2025. How Overconfidence in Initial Choices and
Underconfidence Under Criticism Modulate Change of Mind in Large Language
Models. arXiv preprint arXiv:2507.03120 (2025).

Youngbin Lee, Yejin Kim, Suin Kim, and Yongjae Lee. 2025. Integrating LLM-
Generated Views into Mean-Variance Optimization Using the Black-Litterman
Model. arXiv preprint arXiv:2504. 14345 (2025).

Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Cheng-
gang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. 2024. Deepseek-v3
technical report. arXiv preprint arXiv:2412.19437 (2024).

Andrew W Lo and Jillian Ross. 2024. Can ChatGPT plan your retirement?:
Generative AI and financial advice. Generative AI and Financial Advice (February
11, 2024) (2024).

Yichen Luo, Yebo Feng, Jiahua Xu, Paolo Tasca, and Yang Liu. 2025. LLM-Powered
Multi-Agent System for Automated Crypto Portfolio Management. arXiv preprint
arXiv:2501.00826 (2025).

Meta AI. 2024. Llama 4: A new generation of open multimodal intelligence.
https://ai.meta.com/blog/llama-4-multimodal-intelligence/. Accessed: 2024-07-
19.

Mistral AI. 2024. New models, new endpoints, and a new developer experience.
https://mistral.ai/news/mistral-small-3. Accessed: 2024-07-19.

Kei Nakagawa, Masanori Hirano, and Yugo Fujimoto. 2024. Evaluating company-
specific biases in financial sentiment analysis using large language models. In
2024 IEEE International Conference on Big Data (BigData). IEEE, 6614-6623.
Jillian Ross, Yoon Kim, and Andrew W Lo. 2024. LLM economicus? mapping
the behavioral biases of LLMs via utility theory. arXiv preprint arXiv:2408.02784
(2024).

Rickard Stureborg, Dimitris Alikaniotis, and Yoshi Suhara. 2024. Large language
models are inconsistent and biased evaluators. arXiv preprint arXiv:2405.01724
(2024).

Kaiser Sun, Fan Bai, and Mark Dredze. 2025. What Is Seen Cannot Be Unseen:
The Disruptive Effect of Knowledge Conflict on Large Language Models. arXiv
preprint arXiv:2506.06485 (2025).

Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang, Qi Cao, and Xueqi Cheng.
2024. Blinded by generated contexts: How language models merge generated and
retrieved contexts when knowledge conflicts? arXiv preprint arXiv:2401.11911
(2024).

Yijia Xiao, Edward Sun, Di Luo, and Wei Wang. 2024. TradingAgents: Multi-
agents LLM financial trading framework. arXiv preprint arXiv:2412.20138 (2024).
Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. 2023. Adaptive
chameleon or stubborn sloth: Revealing the behavior of large language models
in knowledge conflicts. In The Twelfth International Conference on Learning
Representations.

An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng,
Bowen Yu, Chang Gao, Chengen Huang, Chenxu Ly, et al. 2025. Qwen3 technical

24

25,

26

27

28

report. arXiv preprint arXiv:2505.09388 (2025).

Yangyang Yu, Zhiyuan Yao, Haohang Li, Zhiyang Deng, Yuechen Jiang, Yupeng
Cao, Zhi Chen, Jordan Suchow, Zhenyu Cui, Rong Liu, et al. 2024. Fincon: A
synthesized llm multi-agent system with conceptual verbal reinforcement for
enhanced financial decision making. Advances in Neural Information Processing
Systems 37 (2024), 137010-137045.

Wentao Zhang, Lingxuan Zhao, Haochong Xia, Shuo Sun, Jiaze Sun, Molei Qin,
Xinyi Li, Yuqing Zhao, Yilei Zhao, Xinyu Cai, et al. 2024. A multimodal founda-
tion agent for financial trading: Tool-augmented, diversified, and generalist. In
Proceedings of the 30th acm sigkdd conference on knowledge discovery and data
mining. 4314-4325.

Yuhan Zhi, Xiaoyu Zhang, Longtian Wang, Shumin Jiang, Shiqing Ma, Xiao-
hong Guan, and Chao Shen. 2025. Exposing product bias in Ilm investment
recommendation. arXiv preprint arXiv:2503.08750 (2025).

Yuhang Zhou, Yuchen Ni, Yunhui Gan, Zhangyue Yin, Xiang Liu, Jian Zhang,
Sen Liu, Xipeng Qiu, Guangnan Ye, and Hongfeng Chai. 2024. Are llms rational
investors? a study on detecting and reducing the financial bias in llms. arXiv
preprint arXiv:2402.12713 (2024).

Nan Zhuang, Boyu Cao, Yi Yang, Jing Xu, Mingda Xu, Yuxiao Wang, and Qi Liu.
2025. LLM Agents Can Be Choice-Supportive Biased Evaluators: An Empirical
Study. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 39.
26436-26444.

