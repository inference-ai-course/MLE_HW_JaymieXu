arXiv:2507.21009v1 [cs.CL] 28 Jul 2025

AI Safety: Memorization in
Fine-Tuned LLMs

Danil Savine
PRAIRIE Research Institute, PSL University

Supervisors: Prof. Jamal Atif, Prof. Olivier Cappé,
Dr. Muni Sreenivas Pydi

Reviewers: Prof. Benjamin Negrevergne

Version: Submitted

September 1, 2024

Abstract

This study investigates the mechanisms and factors influencing memorization in fine-
tuned large language models (LLMs), with a focus on the medical domain due to its
privacy-sensitive nature. We examine how different aspects of the fine-tuning process
affect a model’s propensity to memorize training data, using the PHEE dataset of phar-
macovigilance events.

Our research employs two main approaches: a membership inference attack to detect
memorized data, and a generation task with prompted prefixes to assess verbatim repro-
duction. We analyze the impact of adapting different weight matrices in the transformer
architecture, the relationship between perplexity and memorization, and the effect o
increasing the rank in low-rank adaptation (LoRA) fine-tuning.

Key findings include: (1) Value (WY) and Output (W°) matrices contribute more
significantly to memorization compared to Query (W®) and Key (W*) matrices; (2
Lower perplexity in the fine-tuned model correlates with increased memorization; (3
Higher LoRA ranks lead to increased memorization, but with diminishing returns at
higher ranks.

These results provide insights into the trade-offs between model performance an
privacy risks in fine-tuned LLMs. Our findings have implications for developing more
effective and responsible strategies for adapting large language models while managing
data privacy concerns.


Contents

Abstract 1
1 Introduction and research problem 4

2 Background and Related Work 6
Le 6
ees 6
Le 6

L. 3 xenerationl. ©... ee 6

2.1.4 Lae Architecturd. 2... 2... ee 7

2.1.5 aMA-2). 0... 8
Dees 9
[2.3 Data Extraction Attacks)... 2... ee 10
2.3.1 Overview] .. 2... 2... ee 10

2.3.2 | Measuring successful memorization| 10
Le 11

[2.3.4 Membership Inference Among Candidates] ..... 2.0.2... 12

24 Differential Privacy)... .. 2. ee 13

15
3.1 Settings) 2... 15
3.2_Membership Inference Attack] ... 2.2... 22.000. ..0..000.. 15
3.3 Generation with Prompted Prefix]... ..............0000. 16
B.4 Results). 2. ee 17

- - —~ 17

18

21

4 Conclusion and Future Work' 22
Le 22
4.2 Future Work]... 2... 0. 2 ee 22
Appendix A: Examples of generated text) 27

28

List of Figures

2.1 Illustration of the attention mechanism from https://sebastianraschka.com/| 7

3.1 Limits of the n-gram discriminative function: all suffix generated reveal

information but their score might range from 50% to 100%

lexity in original and fine-tuned model. Each point represents a sequence
in the dataset generated given its first half. The sequence is zpeated 8

| imes. A rank of LORA adaptation of 8 is used on Wy matrices] . . 19
3.4 Share of n-grams completed in the suffix prompted vs. perplexity in the

Se 20
3.0 Share of n-grams completed in the sufhx prompted vs. perplexity in the

Se 20

3.6 MIA performance vs. rank}... 2... 0.000002... 0...00000. 21

List of Tables

[ mance for the Canary |
PREFIX: "isi, OBJ ECTIVE: To report 2 cases of serotonin syndrome with


Chapter 1

Introduction and research problem

Large Language Models (LLMs) have demonstrated remarkable capabilities in various
natural language processing tasks. However, their tendency to memorize training data
poses significant privacy concerns. This research problem focuses on understanding the
mechanisms and factors influencing memorization in LLMs, particularly after the fine-
tuning process.

We focus on the medical domain as a significant number of documents are fine-tuned
on patient-doctor dialogues that create an increasing risk of privacy concerns.

Memorization in fine-tuned pre-trained LLMs

Memorization occurs when a model learns to reproduce specific training examples,
rather than generalizing from the data. In the context of LLMs and fine-tuning, this
phenomenon raises several critical questions:

e How do the fine-tuning parameters influence propensity for memorization?
e What are the characteristics of the data that is more likely to be memorized?

e What defences can be used to protect against memorization while keeping model’s
performance?

Understanding these aspects is crucial for developing more robust and privacy-preserving
language models.

Several factors may influence the extent and nature of memorization in LLMs during
fine-tuning. This research seeks to investigate the impact of:

1. Adapted matrices: Which weight matrices in the model architecture are most
susceptible to memorization when adapted? Are certain components more prone
to memorizing training data?

2. Perplexity of fine-tuned text: How does the perplexity of the training data
correlate with its likelihood of being memorized? Does lower perplexity (i-e., more
predictable text) lead to increased memorization?

3. Rank of adaptation: In the context of low-rank adaptation methods like LoRA
{17], how does the rank of the adaptation matrices affect memorization?

CHAPTER 1. INTRODUCTION AND RESEARCH PROBLEM 5

In order to assess memorization, we use two attacks. A membership inference attack
when given a set of text within and out of training set but coming from a similar distri-
bution. And a generation aimed at assessing the model ability to complete phrases from
the training set when given its beginning.

Chapter 2

Background and Related Work

2.1 Large Language Models

2.1.1 Language Models: Foundations and Innovations

Language models (LMs) serve as state-of-the-art in contemporary natural language pro-
cessing [9] [15] [22) (23) [25]. A prevalent approach in LM training employs a ”next-step
prediction” objective [3] [23]. This method constructs a generative model of the
distribution Pr(a1,v2,...,@n), where 71, %2,..., 2p, represents a token sequence from vo-
cabulary V, utilizing the chain rule of probability:

n
Pr(21,%2,-.-,2n) = Il Pr(a;|@1,..., Ui-1)
i=l

State-of-the-art LMs employ neural networks to estimate this probability distribution.
Let fo(wilv1,..., 2-1) denote the likelihood of token x; when evaluating neural network
f with parameters 6. While recurrent neural networks (RNNs) [13] [19] were once pre-
dominant, attention-based models [2], particularly Transformer LMs [30], now dominate
the field.

2.1.2 Training Objective

LM training aims to maximize the probability of the data in a training set X by mini-
mizing the loss function:

n
L(@) =— log | [ fo(wilai1,..-, 2-1)
i=1
Theoretically, the optimal solution involves memorizing the next token for every prefix

in the training set. However, the use of massive datasets in state-of-the-art LMs mitigates
significant memorization, resulting in nearly identical training and test losses [5} (24) [25].

2.1.3. Text Generation

LMs generate text by iteratively sampling:

Bist ~ fo(inier,..-, 21)

CHAPTER 2. BACKGROUND AND RELATED WORK 7

d,
4,
_—
Lea / Q
n
cD
Embedding
ve 4, .
—s d= la ——- <——~>
<_!
be * T T
Inputs ‘Attention vn
d,
4, —
—_—> n A
t wl >| VW
«=D n
d,y
mm
‘
Z
n
{
=>

Figure 2.1: Illustration of the attention mechanism from https://sebastianraschka.com/

and feeding 4,41 back into the model. Variations include greedy sampling (selecting
the most probable token) and top-n sampling!] (1.

2.1.4 Transformer Architecture

The Transformer has revolutionized natural language processing. It follows an
encoder-decoder structure, replacing recurrent layers with multi-head self-attention layers
and position-wise feed-forward networks. Both encoder and decoder comprise stacks of
identical layers, enabling parallel processing and capturing long-range dependencies more
effectively than traditional RNNs.

Attention mechanism. The attention mechanism is the core component of the
Transformer architecture. It allows the model to focus on different parts of the input
sequence when producing each element of the output sequence.

The basic attention function is called ”Scaled Dot-Product Attention”. It operates
on queries (Q), keys (KK), and values (V). The attention output is computed as:

. Qk?
Attention(Q, K,V) = softmax ( ) Vv (2.1)
Vdk
where d;, is the dimension of the keys. The Figure illustrates this computation.
The scaling factor a is introduced to counteract the effect of the dot products growing
large in magnitude for large values of dy.
Instead of performing a single attention function, the Transformer employs multi-
head attention, which allows the model to jointly attend to information from different
representation sub-spaces at different positions. Multi-head attention is defined as:

TAlso known as nucleus sampling.

CHAPTER 2. BACKGROUND AND RELATED WORK 8

MultiHead(Q, K,V) = Concat(heady, ..., head,)W? (2.2)
where head; = Attention(QW, KW*,VW,’) (2.3)

re . Q dmodel X Ak K dmodel X Ak

Here, the projections are parameter matrices W;* € Rémodl*@e, WS € Rémodel Xe |
WY € RamoaetXdy and W? € Rhdexdmoaet
f; .

2.1.5 LLaMA-2.

LLaMA-2 (Large Language Model Meta AI 2), developed by [29], represents a more recent
advancement in language models. Notable characteristics include:

e Available in 7B, 13B, and 70B parameter versions
e Trained on 2 trillion tokens of diverse data
e Has a 4096-token context

e Incorporates constitutional AI techniques for improved safety

LLaMA-2 builds upon the GPT-3 architecture with modifications such as RM-
SNorm for layer normalization, SwiGLU activation function, and rotary positional em-
beddings. It demonstrates strong performance across a wide range of NLP tasks and has
been released under a permissive license for research and commercial use.

e LlamaForCausalLM implemeted by HuggingFace

model LlamaModel

— embed_tokens: Embedding(32000, 4096)
— layers: ModuleList
* (0-31): 32 x LlamaDecoderLayer
self_attn LlamaSdpaAttention
- q- proj: Linear(in_features=4096, out_features=4096, bias=False)
- k proj: Linear(in_features=4096, out _features=4096, bias=False)
- v_proj: Linear(in_features=4096, out _features=4096, bias=False)
- o_proj: Linear(in_features=4096, out _features=4096, bias=False)
- rotary_emb: LlamaRotaryEmbedding()
mlp LlamaMLP
- gate_proj: Linear(in_features=4096, out_features=11008, bias=False)

- up_proj: Linear(in_features=4096, out_features=11008, bias=False)
- down_proj: Linear(in_features=11008, out_features=4096, bias=False)
- act-fn: SiLU()

input_layernorm LlamaRMSNorm()

post_attention_layernorm LlamaRMSNorm()

8

CHAPTER 2. BACKGROUND AND RELATED WORK 9

— norm: LlamaRMSNorm()
— rotary_emb: LlamaRotaryEmbedding()

Im_head Linear(in_features=4096, out features=32000, bias=False)

2.2 Fine-Tuning and Low Rank Adaptations

Fine-Tuning While pre-trained models demonstrate impressive capabilities across var-
ious tasks, they can be further improved for specific applications through fine-tuning.
Fine-tuning involves continuing the training of a pre-trained model on a smaller, task-
specific dataset to adapt it to a particular domain or task [15].

The process of fine-tuning typically involves selecting a pre-trained model, preparing
a task-specific dataset, adjusting the model’s parameters using the new data, evaluating
the fine-tuned model on the target task.

However, fine-tuning large language models presents challenges, including computa-
tional resources, potential for catastrophic forgetting, and the need for large amounts of
task-specific data.

LoRA: Low-Rank Adaptation of Large Language Models

To address the challenge of computation resources associated with fine-tuning large
language models, [16] introduced LoRA (Low-Rank Adaptation). LoRA is an efficient
fine-tuning method that significantly reduces the number of trainable parameters while
maintaining model performance.

Key features of LoRA include:

e Freezing the pre-trained model weights
e Adding trainable low-rank decomposition matrices to each layer
e Significantly reducing the number of parameters to be fine-tuned

e Enabling faster training and lower memory requirements

The LoRA approach can be formalized as follows. For a pre-trained weight matrix
W ¢€ R®™*, LoRA defines its update as:

W+AW=W+BA (2.4)

where B € R?" and A € R’** are low-rank matrices, and r is the rank, typically
much smaller than d and k.

LoRA has shown promising results in adapting large language models to specific tasks
with minimal computational overhead. It allows for efficient fine-tuning of models making
them more accessible for task-specific applications and research.

CHAPTER 2. BACKGROUND AND RELATED WORK 10

2.3. Data Extraction Attacks

2.3.1 Overview

We follow the approach proposed by [7] to explore extraction techniques. Let Deraining =
{x;}?_, be the corpus of the training set, where each sequence 2; is divided into 2; = [p; ||
s;], with p; being a length-k prefix and s; being the suffix. We define P = {p;}"_, as the
set of all prefixes and S = {s;}'_, as the set of all suffixes. The value of k is chosen as
the midpoint of each sequence length.

Following [31], we define memorization as:

Definition 2.3.1 (Prefix-Suffix Memorization). Given a model function f, memorization
occurs when the model output f(p;) contains information of any s; € S, formalized as
D(f (pi), 8;) = True, where D is a discriminative function assessing the similarity between
two texts.

An alternative definition of memorization comes from citing [12].

Definition 2.3.2 (Counterfactual Memorization). Given a training algorithm A that
maps a training dataset D to a trained model f, and a measure M(f, x) of the performance
of f on a specific example x, the counterfactual memorization of a training example x in
D is given by

. A Gu ni Ui
mem(t)= = Egcpwes[M(A(S),2)]  — Escp,e¢s'|M(A(S"),2)] — , (2.5)
-—_——_—__—" —e____———"
performance on x when trained with x performance on x when not trained with x
where S and S’ are subsets of training examples sampled from D. The expectation is

taken with respect to the random sampling of S and S’, as well as the randomness in the
training algorithm A.

Now that we have defined memorization, let’s dive into language model data extraction
attacks. They usually follow from a two-step procedure as in [8]:

1. Generate candidate text with the model (sampling)

2. Predicting which generated text is actually memorized text (membership inference)

Subsequently, we employ a membership inference attack to eliminate generated sam-
ples unlikely to contain memorized text.

2.3.2 Measuring successful memorization

Various discriminative functions D (as mentioned in Definition
to measure successful memorization:

e Specific keywords: Let K be a set of predefined keywords. The discriminative
function can be defined as:

True, ifakeK:ke f(p)Ak Es;
False, otherwise

Dyeyworas(f (Di) 8;) =

10

CHAPTER 2. BACKGROUND AND RELATED WORK 11

Verbatim memorization [8]: Let LCS(a,b) be the longest common substring

function. The discriminative function can be defined as:

if ILCS(f(pi),83)|
min(|f(pi)[1831)

False, otherwise

True, >T

Dyerbatim f (pi); 83) =

where 7 is a predefined threshold.

BLEU distance: Let BLEU(a,b) be the BLEU []] score function. The discrimi-
native function can be defined as:

True, if BLEU(f(p;), s;) > @

False, otherwise

Darev(f (pi); 83) =

where @ is a predefined threshold.

LLM-based reformulation detection [31]: Let LLM(a,6) be a function that
returns True if an LLM determines a is a reformulation or summary of b. The
discriminative function can be defined as:

Dum(f (pi), 83) = LLM(f (pi), 53)

Plagiarism detection with cosine similarity: Let TF-IDF(a) be the TF-IDF
vector of text a, and cos(u, v) be the cosine similarity between vectors u and v. The
discriminative function can be defined as:

True, if cos(TF-IDF(f(p;)), TF-IDF(s;)) > A

False, otherwise

Dptagiarism (f (pi) 5; ) =

where X is a predefined threshold.

2.3.3 Sampling of Candidates

Sampling Strategy

In large language models text generation is accomplished through iterative sampling:
Bint ~ fo(isilei,..-, 2a)

Top-n sampling: Setting all but the top-n probabilities to zero and renormalizing
before sampling. Sampling over several tokens in beam-search tree is computation-
ally expensive and in practice yields a similar result to the top-n sampling.

Temperature: One can artificially “flatten” this probability distribution to make
the model less confident by replacing the output softmax(z) with softmax(z/t), for
t > 1. Here, t is called the temperature. The softmax function with temperature is
defined as: softmax(z/t); = ee A higher temperature causes the model to
be less confident and more diverse in its output. This is because as t increases, the
differences between the logits z; become less pronounced, leading to a more uniform
probability distribution over the possible next tokens. Greedy sampling (drawing
the most probably token) would be an extreme case where the temperature would
be set to 0.

11

CHAPTER 2. BACKGROUND AND RELATED WORK 12

e Decaying temperature: To balance exploration and exploitation, an option is to
use a decaying softmax temperature. For example, the temperature t could start
at 10 and decays to 1 over the first 20 tokens (approximately 10% of the sequence
length). This approach allows for diverse prefix exploration while enabling the
model to follow high-confidence paths.

Prefix Selection

Prefix selection can significantly impact the extraction process. Options include:

e Beginning of sequence token (empty)

e Text from a similar distribution (e.g., same domain like medical for medical
LLMs, random internet text)

”

e Known text from the dataset (e.g., ”my social security number is ...”, ” John

Doe suffers from” )

2.3.4 Membership Inference Among Candidates

Given a set of samples from the model, the problem of training data extraction can
be reduced to membership inference: predicting whether each sample was present in
the training data [27]. Basic membership inference attacks rely on the observation that
models tend to assign higher confidence to examples present in the training data [26].

For language models, we use perplexity as a natural likelihood measure. Given a
sequence of tokens 21,...,@p, the perplexity is defined as:

1 n

A low perplexity indicates that the model is not ’surprised” by the sequence and has
assigned, on average, a high probability to each subsequent token.

Comparison Techniques

To refine our membership inference, we ”contrast” the perplexity of the model with other
metrics. The intuition comes from the definition of counterfactual memorization [32].
Changing the ”contrastive” algorithm:

e Comparing to other neural language models: use smaller models (e.g., GPT-
2 Small and Medium) trained on the same dataset, exploiting their lower capacity
for memorization.

e In the case of fine-tuned model, comparing to the base model

e Comparing to zlib compression: compute the zlib entropy of the text and use
the ratio of the target model perplexity to zlib entropy as a membership inference
metric. This was mostly used in [8] in order to disqualify candidates with low
perplexity sequences generated by GPT-2 which had a tendency to repeat the same

short sequences several times (e.g. ”I love you, I love you, I love you”)

12

CHAPTER 2. BACKGROUND AND RELATED WORK 13

Exposure Metric

A way to have an idea in advance on whether a particular sequence in our training set will
have a big probability of being extracted, we can use the exposure metric proposed by
[20]. This metric involves inserting a secret (canary) of a certain format into the training
data and calculating its vulnerability to extraction. We select a random document from
our dataset as the canary.

Exposure is defined as the negative log-rank of the inserted secret in terms of model
likelihood:

Exposure(r) = log, | R| — log, rank(r) (2.7)

where r € R is the secret and R is the set of all possible secrets. The exposure value
ranges from 0 to log, |R|, with a larger exposure corresponding to a more noticeable
secret.

2.4 Differential Privacy

In the era of big data and machine learning, the need to protect individual privacy while
allowing for meaningful data analysis has become increasingly important. Differential Pri-
vacy (DP) has emerged as a robust framework for quantifying and limiting the privacy
risks in data analysis tasks. Introduced by [10], differential privacy provides a mathe-
matical definition of privacy that offers strong guarantees against re-identification and
inference attacks.

Definition and Concept Differential privacy is based on the idea that the output of
an analysis should not be significantly affected by the presence or absence of any single
individual in the dataset. More formally, a randomized algorithm M is said to be e-
differentially private if for all datasets D, and D2 that differ on a single element, and for
all possible outputs S:

Pr{M(D,) € S] < e&- PrlM(Dz) € S| +5 (2.8)

where € is the privacy budget that controls the trade-off between privacy and utility,
and 6 is a small probability of failure to achieve the «privacy guarantee. A smaller ¢€
provides stronger privacy guarantees but potentially at the cost of reduced utility. The
parameter 6 allows for a relaxation of the privacy guarantee, permitting a small probabil-
ity of more significant information leakage. Typically, 6 is chosen to be very small (e.g.,
O< Fa where n is the number of records in the dataset) to ensure that the probability of
a privacy breach remains negligible.

Differentially Private Stochastic Gradient Descent (DP-SGD) [Ij is an adap-
tation of the standard SGD algorithm that provides differential privacy guarantees during
the training of machine learning models. The key idea behind DP-SGD is to add cali-
brated noise to the gradients during the training process, thereby limiting the influence
of any single training example on the learned model parameters.

The DP-SGD algorithm can be described as follows:

1. Compute per-example gradients: For each example in the mini-batch, compute
the gradient with respect to the model parameters.

13

CHAPTER 2. BACKGROUND AND RELATED WORK 14

2. Clip gradients: To bound the sensitivity of the computation, clip the £2 norm of
each per-example gradient to a maximum value C:

C
g=g-min(1, iui) (2.9)
where g is the original gradient and @ is the clipped gradient.
3. Add noise: Add Gaussian noise to the sum of clipped gradients:
1(2
~ ~ 272
I= (>: G+N(0,0°?C ») (2.10)
where B is the batch size, o is the noise multiplier, and I is the identity matrix.
4. Update parameters: Update the model parameters using the noisy gradients:
On. =O — 9 (2.11)
where 77 is the learning rate.

DP-SGD allows for training machine learning models with provable privacy guaran-
tees, albeit often at the cost of some reduction in model performance. The trade-off
between privacy and utility can be controlled by adjusting the noise multiplier ¢ and the
gradient clipping threshold C.

14

Chapter 3

Memorization in Fine-tuned Large
Language Models

3.1 Settings

Dataset. We use the PHEE dataset [28] a public patient dataset to fine-tune our mod-
els. It is comprised of over 5000 annotated pharmacovigilence events from medical case
reports and biomedical literature. We use approximately 3000 events for the fine-tuning
and 1000 for testing. Each event is on average 36-token long, which is approximately
25 words. In order to test for the impact of repetition on memorisation, we choose ran-
domly one of the events and replace n-other by the chosen events, intuitively increasing
its weight in the dataset by n. The choice of the dataset is primarily motivated by its
medical domain. Indeed, an increasing number of LLMs are fine-tuned on patient-doctor
dialogues that create an increasing risk of privacy concerns.
Before choosing the PHEE dataset we also have explored the Augmented Clinical Notes
dataset [4], which contains 30,000 triplets of real clinical notes, synthetic patient-doctor
dialogues, and structured patient information. This dataset combines clinical summaries
rom PMC-Patients, synthetically generated dialogues using GPT-3.5, and structured
patient information extracted using GPT-4.

Models. We study memorization in fine-tuning Huggingface’s pre-trained GPT-2
for fast iteration. For actual experiments, we study larger models: Meta’s pre-trained
LLAMA 2 7B quantized at 8 bits. We use a pre-trained but not fine-tuned model as the
reference model for our membership inference attack.

Fine-tuning. For fine-tuning, we'll use the LORA implementation in the PEFT
library by Huggingface.

3.2. Membership Inference Attack

Attack description: We use the attack introduced by [6]. For each sample 2 whose
membership in the training set we want to determine, we feed it to the fine-tuned model,
M, and get its likelihood, Prjs(a). We also feed it to a reference model, R, a pre-trained

model that is not fine-tuned, and get the likelihood Prp(x). We then use LR(x) = pes,

15

CHAPTER 3. MEMORIZATION IN FINE-TUNED LARGE LANGUAGE MODEL$&6

the likelihood ratio, to determine if x is a training sample. If LR(x) is smaller than
threshold t, we classify it as a training set member. Otherwise, we classify it as a non-
member. We then compute the ROC AUC of this attack by varying the threshold on a
dataset composed on training and validation data.

In an actual attack, we would determine the threshold t by calculating LR(s) for all s
in the validation set, and then choose the threshold to be the highest threshold such that
the false positive rate over validation members would not exceed 10%, using the same
threshold as [20].

3.3. Generation with Prompted Prefix

Attack description: We implement an attack to assess the model’s tendency to repro-
duce training data verbatim. The process is as follows:

1. We randomly select a sample (canary) from the training set.

2. The selected sample is split into two halves: prefix and suffix.

3. The prefix is used as a prompt for the fine-tuned model to generate text.
4. We compare the generated text with the original suffix using the following metrics:

e Largest N-gram: We calculate the length of the largest common n-gram
between the generated text and the original suffix. This metric is normalized
by dividing it by the length of the suffix to get a ”share of ngram” score.

e Perplexity: We calculate the perplexity of the original canary using both the
pre-trained and fine-tuned models. Additionally, we compute the perplexity
of the generated text using the fine-tuned model.

This attack is repeated for various configurations, including different randomly cho-
sen canaries, number of repetition of canaries, LORA ranks, and temperatures 1 for text
generation.

Share of n-gram
matching the

suffix Generated sequence

100% Perinatal vasoconstrictive renal insufficiency associated with maternal nimesulide use,
85% Perinatal vasoconstrictive renal insufficiency associated with maternal nimesulide IAtake
54% Perinatal vasoconstrictive renal insufficiency associated with maternal ISS)OnnimesUnag

Figure 3.1: Limits of the n-gram discriminative function: all suffix generated reveal
information but their score might range from 50% to 100%

16

CHAPTER 3. MEMORIZATION IN FINE-TUNED LARGE LANGUAGE MODELS$7

3.4 Results

3.4.1 Which Adapted Matrices Lead to More Memorization?

Rank Adapted Weights ROC AUC Trainable Params Trainable %

1 we 68 + 0.02 262,144 AN
1 w* 68 + 0.02 262,144 AN
1 wY 0.80 + 0.01 262,144 0.4%
1 we 77 = 0.02 262,144 AN
1 (Ww? W*] .76 + 0.02 524,288 8%
2 we .70 + 0.02 524,288 8%
2 w* 71 + 0.02 524,288 8%
2 wY 0.82 + 0.01 524,288 0.8%
2 we .79 + 0.01 524,288 8%
2 (we, W] 77 + 0.02 1,048,576 1.6%
4 we .72 + 0.02 1,048,576 1.6%
4 w* 71 + 0.02 1,048,576 1.6%
4 wy 0.83 + 0.01 1,048,576 1.6%
4 we 80 + 0.01 1,048,576 1.6%
4 (we, Ww] .79 + 0.01 2,097,152 3.1%

Table 3.1: LLaMA 2 fine-tuning on the original PHEE training dataset. 3 epochs. Con-
fidence intervals for the AUC are computed using [14]

For L K For LORArank=4
Query Weight adapted Query Weight adapted

Receiver Operating Characteristic (ROC) Curve Receiver Operating Characteristic (ROC) Curve

— ROC curve (area = 0.70)

8

oa °
False Positive Rate

Figure 3.2: Membership inference attack receiver-operator curves illustrated for Matrix
we

The table presented in Result 1 provides insights into the effects of adapting different
weight matrices in the LLaMA 2 model. Several key observations can be made:

The adaptation of Value (WY) and Output (W°) matrices contributes more signif-
icantly to memorization compared to Query (W@) and Key (W*) matrices, given an
equal number of parameters and rank. This is evident from the consistently higher ROC
AUC scores for WY and W® across all ranks.

This pattern suggests that the Value projection plays a crucial role in the model’s ca-
pacity to memorize training data, followed closely by the Output projection. The Query

17

CHAPTER 3. MEMORIZATION IN FINE-TUNED LARGE LANGUAGE MODELS$8

and Key projections, while still important, have a comparatively lesser impact on mem-
orization.

An interesting observation is that for the same total number of parameters, it appears
that more memorization is generated by adapting both Query and Key matrices together
at a lower rank than to adapt only one of them at a higher rank. For example:

e Rank 1, (W°, W*]: ROC AUC = 0.76, Trainable Params = 524,288
e Rank 2, [W®] or [W*]: ROC AUC = 0.70 or 0.71, Trainable Params = 524,288

It implies that the interaction between these two projections might be more important
for memorization than the individual capacity of either projection alone.

These findings have several implications for the design and training of large language
models:

1. Prioritizing the adaptation of Value and Output projections could lead to more
memorization with fewer parameters.

2. When computational resources are limited, adapting both Query and Key projec-
tions at a lower rank might be more effective than adapting only one at a higher
rank.

Further research could explore the reasons behind the varying impacts of different weight
matrices on memorization, and investigate whether these patterns hold for different model
architectures and tasks.

Our findings on the memorization properties of different weight matrices in trans-
former models are consistent with results reported in the LoRA paper regarding task
performance Table

Weight Type W, We We Wo Wy, We Wa,We Wa, We, Wo, Wo
Rank r 8 8 8 8 4 4 2
WikiSQL (40.5%) 70.4 70.0 73.0 73.2 71.4 73.7 73.7
MultiNLI (+0.1%) 91.0 90.8 91.0 91.3 91.3 91.3 91.7

Table 3.2: From [17] accuracy on WikiSQL and MultiNLI after applying LoRA to different
types of attention weights in GPT-3, given the same number of trainable parameters.
Adapting both W, and W, gives the best performance overall. We find the standard
deviation across random seeds to be consistent for a given dataset, which we report in
the first column.

Comparing the memorization from our study and task performance from the original
LORA paper unfortunately doesn’t provide a clear trade-off, as the matrices that generate
the most memorization are also the ones that contribute the most to the task performance.

3.4.2. How Does Perplexity Influence Memorization?

In this section we use the prompted generation described in We can see from the
Figure|3.3]and[3.4|that better memorization is linked to lower perplexity in the fine-tuned
model. This result justifies the use of perplexity for the membership inference attack and

18

CHAPTER 3. MEMORIZATION IN FINE-TUNED LARGE LANGUAGE MODEL$9

as a proxy for memorization.

However we see no correlation between perplexity in the base model and memorization
in the fine-tuned model contrary to [18]. Our result might be linked with the homogeneous
distribution of the PHEE dataset focusing on medical domain. A more diverse dataset
might have produce more variance in perplexity in the base model and potentially more
differentiation in memorization.

6 10
9
Z
5
8
a
2£
5 e $
08's
z
2
3 a
3 5
e4 e a
= &
3 5
€ e bo
a 062
& e e e g
© e e e 3
g e @
. % .
g @
Fs ote chee ° z
$ % 0° e e 3
ha eee e e 4s
2 Y ce £
5 ° tee e e 8
Bi H ® e 2
G7) 8 Gage . §
Ey
“| 8m 8 . z
e e 2
rd
e g
026
Ey
Fs
S
5
a

0 0.0

5 10 15 20 25 30
Perplexity of sequence in LLAMA-2

Figure 3.3: Prompted generation accuracy in the LLAMA-2 fine-tuned model vs. per-
plexity in original and fine-tuned model. Each point represents a sequence in the dataset
generated given its first half. The sequence is repeated 8 times. A rank of LORA adap-
tation of 8 is used on Wy matrices.

19

CHAPTER 3. MEMORIZATION IN FINE-TUNED LARGE LANGUAGE MODEL30

— Regression line (R* = 0.135)

Share of N-gram

15 2.0 25 3.0 35 4.0 45 5.0
Perplexity in the fine-tuned model

Figure 3.4: Share of n-grams completed in the suffix prompted vs. perplexity in the
fine-tuned model

— Regression line (R* = 0.004)
10 wo epee co ee °
°
08 $ ° .
°
e ° °
°
06 e
&
5 °
2 ° bd
ba eo 0
6
2 e
z ° ee
fa
04 °
° @
oe % é
eo °
e
- % °
O e
oo 8 e @ eee
02 0.2. °
© pe ° e
o
° ° eo, e °
°
° oe % oe Ad e
°
0.0 e
By qo 25 30

Fey
Perplexity in the base model

Figure 3.5: Share of n-grams completed in the suffix prompted vs. perplexity in the base
model

20

CHAPTER 3. MEMORIZATION IN FINE-TUNED LARGE LANGUAGE MODEL381

3.4.3 LORA Rank Increases Model Memorization - but Plateaus

The diminishing returns at higher ranks suggest that there might be an optimal rank for
adaptation, beyond which the benefits may not justify the increased computational cost.

In the Figure we compare the performance of the membership attack
the LORA adaptation. Two hypothesis might explain these diminishing re

o the rank of
turns.

The number of parameters might exceed the training data: This is unfortu-

nately not the case. We can see that from a rank of 50, adapting all the m,

attention mechanism, memorization doesn’t improve with the rank. An in
be to compare the number of parameters adapted for a rank of 50 which
(32 layers * 4096 context * 2 decomposition in B and A * 4 matrices adap

atrices in the
tuition would
is 52,428,800
ed * 50 rank)

to the dimension of the training data which is 557,936,640 (2898 sequences * 38 tokens

per sequences * 5120 embedding dimension).

The plateau is linked to the inherent low intrinsic rank of the

weight ma-

trices: This hypothesis is more aligned with the prior presented in the original LORA

paper [16] and

0.945 <== ° .
. f—
0.935
y 0930
$
2
Q
S
© 0925
°
0.920
0915
0910 @ Measured AUC
o — Trend line
0 100 200 300 400 500

Figure 3.6: MIA performance vs. rank

21

Chapter 4

Conclusion and Future Work

4.1

Summary of Contributions

This study has provided several key insights into the mechanisms of memorization in
fine-tuned large language models:

1.

4.2

We demonstrated that the adaptation of Value (WY) and Output (W°) matri-
ces contributes more significantly to memorization compared to Query (W®) and
Key (W*) matrices, given an equal number of parameters and rank. This con-
firms results from [17] showing that adapting these matrices leads to higher model
performance on benchmarks.

. Our results showed a clear inverse relationship between perplexity and memoriza-

tion, with lower perplexity in the fine-tuned model correlating with increased mem-
orization. However we have seen no correlation between perplexity in the base
model and memorization in the fine-tuned model contrary to

. We observed that increasing the LoRA rank leads to increased memorization, but

with diminishing returns at higher ranks, which is also shown for model performance

in

Future Work

Building on the findings of this study, several promising directions for future research

emerge:

1.

Differential Privacy Defenses: Implement and evaluate differential privacy tech-
niques, particularly DP-SGD, as a defense against memorization. Compare the ef-
fectiveness of DP-SGD with other baseline approaches, such as setting a minimum
temperature for generation. This could provide insights into practical strategies for
mitigating privacy risks in fine-tuned models.

. Performance Metrics: Expand the analysis to include comprehensive perfor-

mance metrics for fine-tuned models, such as perplexity over a test set. This would
allow for a more nuanced understanding of the trade-offs between model perfor-
mance and memorization, helping to identify optimal fine-tuning strategies that
balance utility and privacy.

22

CHAPTER 4. CONCLUSION AND FUTURE WORK 23

3. Relaxed Memorization Metrics: Implement and evaluate more relaxed defini-
tions of memorization, such as using the BLEU score as a discriminative function.
This could provide a more nuanced understanding of different levels or types of
memorization, beyond verbatim reproduction.

4. Interpretability: Investigate the relationship between model interpretability and
memorization. This could involve developing techniques to visualize or quantify
which parts of the model contribute most to memorization, potentially leading to
more targeted mitigation strategies.

By pursuing these research directions, we can work towards developing large language
models that are not only powerful and adaptable but also respectful of data privacy.
This balanced approach is crucial for the responsible advancement of AI , particularly in
sensitive domains like healthcare.

23

Bibliography

[1] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and

L. Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM
SIGSAC' Conference on Computer and Communications Security, CCS’16. ACM,
Oct. 2016.

D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning
to align and translate. In [CLR, 2015.

Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural probabilistic language
model, 2003.

A. Bonnet, P. Boulenger, H. Wu, M. Conti, J. Prado, O. El Malki, N. De Sabbata,
H. da Silva Gameiro, Y. Xu, F. Boukil, A. Faure, A. A. Sarijaloui, Y. Niu, Z. Chen,
A. Bosselut, and M. Jaggi. Medinote: Automated clinical notes. 2024.

T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakan-
an, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners.
arXwv preprint arXiv:2005.14165, 2020.

N. Carlini, S. Chien, M. Nasr, S. Song, A. Terzis, and F. Tramer. Membership
inference attacks from first principles, 2022.

N. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. Tramer, and C. Zhang. Quantifying
memorization across neural language models.

N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts,
T. Brown, D. Song, U. Erlingsson, A. Oprea, and C. Raffel. Extracting training data
from large language models.

J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding, 2019.

C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity
in private data analysis. In Theory of Cryptography Conference, pages 265-284.
Springer, 2006.

A. Fan, M. Lewis, and Y. Dauphin. Hierarchical neural story generation. In ACL,
2018.

V. Feldman. Does learning require memorization? a short tale about a long tail,
2021.

24

BIBLIOGRAPHY 25

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

A. Graves. Generating sequences with recurrent neural networks. arXiv preprint
arXtv:1308.0850, 2013.

J. A. Hanley and B. J. McNeil. The meaning and use of the area under a receiver
operating characteristic (roc) curve. Radiology, 143(1):29-36, 4 1982.

J. Howard and S. Ruder. Universal language model fine-tuning for text classification,
2018.

E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen.
LoRA: Low-rank adaptation of large language models.

E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen.
Lora: Low-rank adaptation of large language models, 2021.

M. Meeus, I. Shilov, M. Faysse, and Y.-A. de Montjoye. Copyright traps for large
language models, 2024.

T. Mikolov, M. Karafidt, L. Burget, J. Cernocky, and S. Khudanpur. Recurrent
neural network based language model. In Interspeech, 2010.

F. Mireshghallah, A. Uniyal, T. Wang, D. Evans, and T. Berg-Kirkpatrick. Memo-
rization in nlp fine-tuning methods, 2022.

Kk. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic
evaluation of machine translation. In P. Isabelle, E. Charniak, and D. Lin, editors,
Proceedings of the 40th Annual Meeting of the Association for Computational Lin-
guistics, pages 311-318, Philadelphia, Pennsylvania, USA, July 2002. Association
for Computational Linguistics.

M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettle-
moyer. Deep contextualized word representations, 2018.

A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language
understanding by generative pre-training, 2018.

A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language
models are unsupervised multitask learners. 2019.

C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li,
and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer, 2020.

A. Salem, Y. Zhang, M. Humbert, P. Berrang, M. Fritz, and M. Backes. Ml-leaks:
Model and data independent membership inference attacks and defenses on machine
learning models, 2018.

R. Shokri, M. Stronati, C. Song, and V. Shmatikov. Membership inference attacks
against machine learning models, 2017.

Z. Sun, J. Li, G. Pergola, B. C. Wallace, B. John, N. Greene, J. Kim, and Y. He.
Phee: A dataset for pharmacovigilance event extraction from text, 2022.

25

BIBLIOGRAPHY 26

[29] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaci, N. Bashlykov,

[30]

[31]

[32]

S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen,
G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami,
N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez,
M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril,
J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Moly-
bog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva,
E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X.
Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Ro-
driguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and
fine-tuned chat models, 2023.

A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser,
and I. Polosukhin. Attention is all you need. In Advances in Neural Information
Processing Systems, pages 5998-6008, 2017.

S. Zeng, Y. Li, J. Ren, Y. Liu, H. Xu, P. He, Y. Xing, S. Wang, J. Tang, and D. Yin.
Exploring memorization in fine-tuned language models.

C. Zhang, D. Ippolito, K. Lee, M. Jagielski, F. Tramér, and N. Carlini. Counterfac-
tual memorization in neural language models.

26

Appendix A

Appendix A: Examples of generated
text

Num. Repeated ] LORA | Adapted | Largest Shared Suffix | Temp. ] Canary Perplexity | Output Perplexity | Share of Correctly Generated | Output Tokens
Canaries Rank | Weights N-gram Base Model _| Fine-Tuned Model N-gram Suffix Tokens

16 T | Topror] 2 or BA 25 TO is OBJECTIVE: To report 2 cases

of serotonin syndrome with seri-

ous extrapyramidal movement: dis-
orders occurring when metoclo-
pramide was coadministered with
sertraline or venlafaxine

16 T | Toprol] a 05 Ba aT 02 is, OBJECTIVE: To report 2 cases
of serotonin syndrome w
ous extrapyramidal mo
orders oceurring
with metoclopramide. This report
is based on a review of the litera-

ture and on
16 T | To-prorT o To Ba 2S 2 ih; OBJECTIVE: To report 2 cases
of serotonin syndrome with seri-

ous extrapyramidal movement dis-

order ated with the coadmin-

istration of metoclopramide and se-
lective serotonin reuptake inhib

By T | Toprol] Ww or Ba I TO is, OBJECTIVE: To report 2 cases
of serotonin syndrome with seri-
ous extrapyramidal moven
order

nt dis-

occurring when
administered with

pramide was

sertraline or venlafaxine
By T | Toprol] Ww 05 Ba I TO is, OBJECTIVE:
of serotonin syndrome with seri-

To report 2 canes

ous extrapyramidal movement: dis-
orders occurring when metoclo-
pramide was coadministered with
sertraline or venlafaxine

By T | Toprol] Ww Ta Ba I TO is, OBJECTIVE: To report 2 cases

of serotonin syndrome with seri-
ous extrapyramidal movement: dis-
orders occurring when metoclo-
pramide was coadministered with

sertraline or venlafaxine

Table A.1: Updated Experimental Results for Canary Repetition and Model Performance
for the Canary

PREFIX: ”jsj OBJECTIVE: To report 2 cases of serotonin syndrome with serious ex-
trapyramidal movement disorders”

SUFFIX: ”occurring when metoclopramide was coadministered with sertraline or ven-
lafaxine”

27

Acknowledgments

Use of GitHub Copilot by Microsoft for code, Claude by Anthropic for report editing and
formatting

28

