arXiv:2507.21028v1 [cs.CL] 28 Jul 2025

Multi-Agent-as-Judge: Aligning LLM-Agent-Based Automated Evaluation
with Multi-Dimensional Human Evaluation

Jiaju Chen
Rice University

Yuxuan Lu
Northeastern University

Xiaojie Wang
Amazon

Huimin Zeng Jing Huang Jiri Gesi
University of Illinois Amazon Amazon
Urbana-Champaign

Ying Xu Bingsheng Yao Dakuo Wang*

Harvard University

Abstract

Nearly all human work is collaborative; thus,
the evaluation of real-world NLP applications
often requires multiple dimensions that align
with diverse human perspectives. As real
human evaluator resources are often scarce
and costly, the emerging “LLM-as-a-judge”
paradigm sheds light on a promising approach
to leverage LLM agents to believably simu-
late human evaluators. Yet, to date, existing
LLM-as-a-judge approaches face two limita-
tions: persona descriptions of agents are often
arbitrarily designed, and the frameworks are
not generalizable to other tasks. To address
these challenges, we propose MAJ-EVAL, a
Multi-Agent-as-Judge evaluation framework
that can automatically construct multiple eval-
uator personas with distinct dimensions from
relevant text documents (e.g., research papers),
instantiate LLM agents with the personas, and
engage in-group debates with multi-agents to
generate multi-dimensional feedback. Our eval-
uation experiments in both the educational and
medical domains demonstrate that MAJ-EVAL
can generate evaluation results that better align
with human experts’ ratings compared with
conventional automated evaluation metrics and
existing LLM-as-a-judge methods.

1 Introduction

Nearly all human work today is multi-person col-
laboration work (Olson and Olson, 2000). As a re-
sult, the evaluation of NLP applications in the real
world, such as those in education and healthcare,
often requires considering multiple dimensions
that align with diverse human perspectives (He
et al., 2023; Chen et al., 2024). In particular, such
evaluations cannot be handled by a single evalu-
ator or traditional similarity-based metrics (e.g.,
ROUGE-L) because complex real-world scenarios

* Corresponding Author: d.wang @northeastern.edu.

Northeastern University

Northeastern University

Gen-QA/

e

Target Source:

Research Gen-Task Gen-Sum/
Papers Description | pode
L IL J
1,1 Evaluative 2.1 Multi-Agent
Dimension Extraction In-Group Debate
‘hia ST | Stakeholder Groups)
Parents The ° G8) '
SS SY i
Edu Experts 1 1
Zz = ~ ‘(Edu Experts) (Parents Children ):
Perspective 1: Focus | [= Se Se '
on Grammar» if i i
Experts’ Parents’ —_ Children’s
1.2 Dimension-Based Score. ‘Score Score
Persona Construction
2 Mr. James's Persona 2.2 Aggregation
- Demographics: ...
~ Perspective: —> Final Score
- Special
- Psych Tr U
= Social Rol
(1) Stakeholders Multi-Agent-as-Judge Debate
Persona Creation Evaluation

Figure 1: The overview of MAJ-EVAL’s two-step de-
sign: Step 1, MAJ-EVAL extracts stakeholders’ diverse
perspectives from the provided research papers and
constructs corresponding personas for the LLM agents.
Step 2, agents within the same stakeholder group engage
in an in-group debate. An aggregator agent synthesizes
ratings from all groups to derive a final score.

and the collaborative nature of human work neces-
sitate integrating insights from diverse stakehold-
ers who bring different domain-specific roles and
perspectives to the evaluation (Liu et al., 2024).
For instance, care providers, family caregivers,
and patients have different needs while evaluat-
ing patient summaries generated by LLMs (Yang
et al., 2025); similarly, assessing LLM-generated
Question-Answering pairs for children’s reading
comprehension requires feedback from children,
parents, and teachers (Chen et al., 2025b).

While human expert annotation remains the gold

standard for such domain-specific real-world eval-
uations, collecting multi-dimensional human feed-

back is highly challenging due to the expert re-
source scarcity and the tremendous cost (time and
money) for recruitment (Yao, 2024; Lu et al., 2023).
To address these challenges, recent work has ex-
plored the use of Large Language Models (LLMs)
as evaluators to substitute human evaluators, giv-
ing rise to the “LLM-as-a-judge” paradigm (Zheng
et al., 2023; Zhuge et al., 2024). To address
these challenges, recent work has explored the
use of Large Language Models (LLMs) as eval-
uators to substitute human evaluators, giving rise
to the “LLM-as-a-judge” paradigm (Zhuge et al.,
2024). In particular, the multi-agent framework
under this paradigm employs multiple role-playing
LLM agents to simulate human evaluation (Park
et al., 2024; Ran et al., 2025; Lu et al., 2025), where
each agent is intended to reflect one human evalua-
tive dimension (Kim et al., 2024; Li et al., 2024c).
Such a framework offers a promising approach
for evaluating real-world applications where multi-
dimensional human feedback is required.
Despite its promise, the current multi-agent eval
uation approach faces two critical limitations: First,
the design of agent personas is often arbitrary
and not generalizable due to the lack of a system-
atic methodology (Li et al., 2024b). For example,
even within the same task, studies may focus on
different dimensions due to varied priorities and
interpretation: one study may handcraft a “teacher”
agent that focuses on “grammar accuracy,” while
another might handcraft the same agent prioritizing
“student engagement,” leading to results that cannot
be reliably reproduced across studies or by other re-
search teams (Park et al., 2023; Li et al., 2025). Sec-
ond, most evaluation setups are not easily adapt-
able because they are specifically designed for a
particular task or scenario. For instance, an evalua-
tion pipeline designed for medical summarization
may include dimensions like “clinical consistency,”
but these are irrelevant for similar summarization
tasks for children’s education, where “child engage-
ment” is more appropriate. Because these dimen-
sions and role definitions are hard-coded per task
(Lin and Chen, 2023), the evaluation frameworks
often require complete redesigns to handle new do-
mains, undermining scalability and transferability.
To overcome these limitations, we propose
MAJ-EVAL: a Multi-Agent-as-Judge evaluation
framework that automates the construction and de-
ployment of human-aligned LLM agents for ro-
bust real-world natural language generation (NLG)
evaluation. As illustrated in Figure 1, MAJ-EVAL

first identifies descriptions about different human
stakeholders and their perspectives (e.g., teachers
emphasizing educational value) from researcher-
provided documents related to the domain-specific
tasks (e.g., research papers). These descriptions
are then transformed into structured agent personas
attributes, such as domain expertise, psychologi-
cal traits, and social roles. Second, the resulting
LLM multi-agents engage in an in-group debate to
reflect, challenge, and refine their initial judgments
before producing an aggregated rating aligned with
multi-dimensional human ratings.

We evaluate MAJ-EVAL on two challenging
domain-specific real-world tasks: (1) question-
answer generation (QAG) for children’s storybook
reading (Xu et al., 2022) and (2) multi-document
summarization of medical literature (DeYoung
et al., 2021). Across both tasks, MAJ-EVAL
achieves more substantial alignment with human
ratings compared to traditional automated met-
rics (e.g., ROUGE-L (Lin, 2004), BERTScore
(Zhang et al., 2019)), single LLM-as-a-judge eval-
uation (e.g., G-Eval (Liu et al., 2023)), and exist-
ing multi-agent approaches (e.g., ChatEval (Chan
et al., 2023)). These results underscore the value
of grounding evaluation in human-aligned multi-
dimensional evaluation and demonstrate MAJ-
EVAL’s potential as a generalizable framework for
real-world NLG evaluation.

2 Related Work

2.1. Traditional Evaluation Methods for NLG

Automated evaluation metrics have long been the
standard for measuring the performance of NLG
systems, primarily due to their simplicity and
scalability (Yao et al., 2023b). Metrics such as
ROUGE (Lin, 2004), BLEU (Papineni et al., 2002),
and BERTScore (Zhang et al., 2019) are widely
adopted in both research and industry settings.
These methods typically compute the token-level
or embedding-based similarity between model out-
puts and a set of reference texts. However, such
similarity-based metrics often fall short in real-
world, domain-specific tasks where deeper con-
textual understanding, factual correctness, and
task-specific appropriateness are more critical than
surface-level overlap (Yao et al., 2023c; Sai et al.,
2022; Zhu et al., 2023; Wu et al., 2025). For exam-
ple, in medical summarization generation (Crox-
ford et al., 2024), ROUGE may fail to penalize
hallucinated claims that are fluently expressed but

oo Individual Agent-as-a-Judge: - - - - - { (2) Multi-Agent In-Group Free Debate - -

1 ie Next [NetSregerseecton | Selection

“These Al- arcs Avgenerate
Cay xp pairs

Task Description

In-Group
Moazeti

- Update
A B Cc
L

Init. Thinking ,

Init. Thinking, Init. Thinking.

r3) Aggregation

Updated Updated Updated
Thinking,  Thinkingg Thinking,
I think ... @ :
; Final Score
i ic:
if B Out-Group
‘eessetetereeeresaseeseeeet J Aggregator

Figure 2: MAJ-EVAL’s multi-agent-as-judge debate evaluation process (Step 2). Each agent first independently
provides an initial evaluation. The moderator then coordinates a free-form debate, allowing agents to discuss and
refine their evaluation. Finally, the aggregator synthesizes all agents’ evaluations into a final evaluation.

unsupported by evidence. Similarly, in educational
QA generation (Zhao et al., 2022; Chen et al.,
2025b), lexical similarity cannot assess whether a
question is pedagogically meaningful for children.

To address the limitations of automated met-
rics, human evaluation has become the gold stan-
dard for assessing generated text, especially in
domain-specific tasks (Chen et al., 2024; Croxford
et al., 2024). Human expert annotators are typically
asked to rate outputs across multiple dimensions
(e.g., fluency, relevance, educational value, or clini-
cal accuracy) (Lu et al., 2023). While comprehen-
sive, human evaluation is costly, labor-intensive,
and often lacks consistency across different re-
search projects (Belz et al., 2021; Yao et al., 2023a;
Thomson et al., 2024). Moreover, the complexity
and human workflows of many real-world applica-
tions mean that no single human annotator can rep-
resent the relevant multi-stakeholder perspectives.
For instance, in evaluating interactive storybook
content for children (Xu et al., 2021; Chen et al.,
2025b), each stakeholder may provide different
evaluation dimensions even for the same generated
content from the same model: a teacher may prior-
itize educational value, while a parent may focus
on emotional engagement. This diversity is both
necessary and difficult to scale using traditional
human evaluation protocols.

2.2. LLM-as-a-Judge Evaluation

Researchers have proposed leveraging LLMs as
evaluators, which is commonly referred to as
the “LLM-as-a-judge” evaluation paradigm (Zheng
et al., 2023). In this setup, a single LLM
is prompted or fine-tuned to assess the model-
generated text, simulating human evaluation cri-
teria such as relevance, coherence, and correct-
ness (Li et al., 2024a; Lee et al., 2024; Fu

et al., 2023). Representative methods include
G-Eval (Liu et al., 2023), which guides GPT-4
using chain-of-thought prompting for structured
dimension-wise assessment, and PandaLM (Wang
et al., 2023b), which fine-tunes an LLaMA-7B
model for preference ranking. These methods are
lightweight and scalable but inherit several chal-
lenges. Most notably, they reflect single-model
bias, where judgments are constrained by the
model’s own training data and reasoning style, thus
may fail to simulate multi-stakeholder perspectives
in real-world evaluations (Yao et al., 2024).

To mitigate the limitations of single-LLM eval-
uation, recent work has extended the paradigm to
multi-agent setups, where multiple LLM agents,
each adopting a distinct persona or evaluative role,
collaborate or debate to arrive at a final assess-
ment (Chen et al., 2023; Zhu et al., 2023). Exam-
ples include ChatEval (Chan et al., 2023), which
assigns agents to pre-defined roles such as “general
public” or “critic,” and MADISSE (Koupaee et al.,
2025), which frames evaluation as a debate be-
tween agents with opposing initial stances. These
systems improve diversity in judgment and better
mirror real-world evaluative complexity. However,
most of these approaches still rely on manually
crafted personas and predefined evaluation dimen-
sions, limiting reproducibility and cross-task gener-
alization (Szymanski et al., 2025; Gebreegziabher
et al., 2025). For example, an agent labeled as a
“critic” in one task may not exhibit the same eval-
uative priorities in another, and a dimension like
“factual consistency” may not translate well from
summarization to dialogue generation.

3 MAJ-EVAL

We propose MAJ-EVAL, an LLM-based multi-
agent evaluation framework designed to simulate

real-world multi-stakeholder-aligned NLG evalua-
tion. As shown in Figure 1, MAJ-EVAL enables
researchers to evaluate model-generated content
by (1) automatically extracting stakeholder per-
spectives from domain-specific documents and con-
structing diverse agent personas grounded in those
perspectives, and (2) orchestrating in-group de-
bates among these agents to produce final, multi-
dimensional evaluation scores.

3.1 Stakeholder Persona Creation

The first stage of MAJ-EVAL focuses on creating
personas that faithfully represent the diverse evalu-
ative dimensions found in real-world stakeholder
groups. To ensure both coverage and credibility,
persona creation follows a two-step process: (1) ex-
tracting evaluative dimensions from research pub-
lications, and (2) constructing personas based on
those extracted perspectives.

Step 1: Evaluative Dimension Extraction.
Given a list of documents of domain-specific tasks
(e.g., research papers) L = {ly,...,ln}, MAJ-
EVAL uses an LLM Mo to identify relevant stake-
holders and extract their associated perspectives
(i.e., evaluative dimensions). Each document is
parsed to locate stakeholders (e.g., “parents,” “clin-
icians”) and their descriptive attributes (e.g., priori-
ties, values), along with evidence-based evaluation
dimensions (e.g., “focus on grammar correctness”).
The output for each document J; is a structured list
of stakeholder tuples s;; = (nij, cij, Vij), Where
nj; denotes the stakeholder’s name, c;; is their de-
scription, and Vj; is a set of (dimension, evidence)
pairs. For instance, in the task of QAG for chil-
dren’s story reading, one extracted evaluative di-
mension of the parents is “Parents expect questions
to stimulate creativity, critical thinking, and curios-
ity rather than factual recall...”, with the evidence
of “The majority of participants felt that current AI
tools were ‘silly’...’ from a paper that explores par-
ents’ expectations and perceptions of Al-assisted
reading tools for children (Sun et al., 2024).

To unify overlapping roles and ensure coher-
ent persona design, MAJ-EVAL aggregates similar
stakeholders into groups using semantic clustering
via LLM My. Within each group, redundant or
semantically close dimensions are automatically
merged, resulting in a consolidated view of each
stakeholder group. For example, education tech-
nology developers who emphasize “system usabil-
ity” and AI developers who promote “system ro-
bustness” are grouped under a “system developer”

stakeholder group with multiple evaluative dimen-
sions. Following prior work showing that diverse
perspectives can enhance the debate process (Liang
et al., 2024), MAJ-EVAL retains distinct evaluative
dimensions within each group to preserve diversity.

More examples of extracted stakeholders’ (di
mension, evidence) pairs for the children’s book
QAG case study are shown in Table 7, and exam-
ples for the medical summarization generation task
are shown in Table 8 in Appendix A.8. The prompt
for this step is shown in Table 12.

Step 2: Dimension-Based Persona Construc-
tion. For each consolidated dimension within a
stakeholder group, MAJ-EVAL constructs a de-
tailed persona: p;; = Mo(ci, vij, eij). Inspired by
prior work on LLM-based role-play agents (Chen
et al., 2025a), each persona includes five key
attributes: (1) demographic information (e.g.,
name, age, profession), (2) evaluative dimension
(from earlier perspective extraction), (3) domain
specialty, (4) psychological traits, and (5) social
relationships. These personas serve as the basis
for instantiating stakeholder-aligned agents during
evaluation. We include examples of constructed
personas in Table 10 and the corresponding prompt
in Table 13. In addition, Appendix A.7 presents
an example of MAJ-EVAL’s complete persona cre-
ation workflow.

3.2 Multi-Agent-as-Judge Debate Evaluation

In the second stage of MAJ-EVAL, the constructed
personas are instantiated as LLM-based agents
that engage in a multi-agent-as-judge debate
evaluation (Table 14 presents the instantiation
prompt). Each stakeholder group (e.g., teach-
ers, clinicians) evaluates model-generated out-
puts through in-group deliberation (in-group multi-
agent free debate), simulating how real-world stake-
holders might discuss, disagree, and eventually con-
verge on evaluation judgments. The debate process
is divided into three phases: (1) individual agent-
as-a-judge evaluation, (2) multi-agent in-group free
debate, and (3) aggregation of scores into a final
group judgment (see Figure 2).

Phase 1: Individual Agent-as-a-Judge. Each
stakeholder agent begins by independently assess-
ing the generated output according to their unique
perspective and expertise. This phase aims to cap-
ture a diversity of opinions, reflecting how different
stakeholders may initially interpret the same con-
tent in task-specific ways. The prompt for this
phase is presented in Table15.

Phase 2: Multi-Agent In-Group Free Debate.
Next, the agents engage in an open-ended multi-
turn debate within each group. Moderated by a
coordinating agent, the debate unfolds dynamically,
prioritizing agents with unresolved disagreements
or unaddressed perspectives. Agents challenge, re-
flect on, or reinforce each other’s views and revise
their evaluations as needed. This phase encourages
surfacing blind spots, resolving conflicts, and gen-
erating more refined judgments. We include the
prompt for phase 2 in Table 16.

Phase 3: Aggregation. Finally, an aggregator
agent aggregates the updated evaluations across
all agent groups in two ways: (1) synthesizing the
qualitative feedback from all stakeholder agents’ fi-
nal evaluations and (2) computing an average score
of each group’s post-debate quantitative ratings.
Table 17 shows the prompt for this phase.

4 Experimental Setup

4.1 Tasks and Datasets

Our first evaluation task is a Narrative Question-
Answer Generation (QAG) task from  chil-
dren’s storybooks, we utilize the StorySparkQA
dataset (Chen et al., 2024), an expert-annotated
QA dataset designed for 3- to 6-year-old children’s
interactive story-reading activity. StorySparkQA
consists of 5,868 QA pairs that are derived from
children’s fairytale stories and enriched with real-
world knowledge. In our experiment, we evaluate
the 70 QA pairs generated by GPT-4 as reported
by Chen et al. (2024) that have been annotated by
human experts using the following four evaluation
dimensions: Grammar Correctness (i.e., whether
the QA pair is grammatically correct), Answer Rel-
evancy (i.e., whether the answer meaningfully ad-
dresses the question), Contextual Consistency (i.e.,
whether the QA pair is grounded in the story but in-
troduces external real-world knowledge), and Chil-
dren’s Educational Appropriateness (i.e., whether
the QA pair is suitable for 3- to 6-year-old children
in the context of story-reading).

Our second evaluation task is a multi-document
summary generation for medical literature reviews.
We choose the MSLR-COCHRANE dataset (Wang
et al., 2023a), an expert-annotated benchmark com-
prising 600 model-generated summaries from six
models. These summaries were annotated by do-
main experts along four dimensions: Fluency (i.e.,
whether the summary is fluent in English), PJO
Consistency (i.e., whether the Population, Inter-

vention, and Outcome (PIO) align with the target
summary), Effect Direction (the reported impact
of the intervention), and Evidence Strength (the
degree to which the claim is supported by the un-
derlying studies). For our study, we construct a rep-
resentative evaluation set by randomly sampling 17
summaries from each of the six models, resulting
in a balanced subset of 102 generated summaries.

4.2 Baseline Evaluation Methods

We compare MAJ-EVAL against the following
three types of evaluation methods as baselines:

Single Metrics of Automated Evaluation We
adopt two commonly used similarity-based au-
tomated metrics to evaluate generated outputs.
ROUGE-L F1 (Lin, 2004) measures surface-level
similarity by computing lexical overlap with refer-
ence texts. Since lexical overlap may fail to capture
deeper semantic meaning, we also use BERTScore
(Zhang et al., 2019), which measures semantic
similarity using contextual embeddings from pre-
trained language models.

Single LLM-as-a-judge Evaluation We use G-
EVAL (Liu et al., 2023), a prompting-based evalu-
ation framework that guides a single LLM to rate
the generated content along specific dimensions.
We experiment with both GPT-4 (OpenAI, 2023),
Claude-3.7-Sonnet, and Qwen-3-235B as the base
models for G-EVAL evaluations.

Multi-Agent-as-Judge Evaluation We adopt
ChatEval (Chan et al., 2023), a multi-agent evalu-
ation framework where agents role-play different
personas to assess generated outputs. We exper-
iment with both GPT-4, Claude-3.7-Sonnet, and
Qwen-3-235B as the underlying models and follow
the default setup to assign personas.

For both single-LLM and multi-agent evaluation
methods, we follow the original prompt structure
proposed by the authors, with slight modifications
to adapt it to our two case study tasks. Full prompt
details are provided in Appendix A.10.

4.3 Evaluation Metrics

In order to evaluate how well each evaluation
method aligns with the multi-dimensional human
judgments in each dataset, we report the absolute
Spearman’s rank correlation coefficient (/), fol-
lowing Chan et al. (2023); Chu et al. (2024).

In addition, we report Kendall’s Tau (7) (Sen,
1968), which assesses the ordinal ranking consis-

Case study 1: StorySparkQA

Baseline Evaluation Methods

ROUGE-L

BERTScore

G-Eval

Chat€val

Al Developers

Children

Early Childhood
Educators
Language

Researchers

Parents

Total

Al Developers

Children

Educational

Experts po.1

Parents 0.27

Teachers 0. 0.36

Total .. 0.33

Gc AR cc EA Total

Case study 2: MSLR-Cochrane

Baseline Evaluation Methods

0.40
ROUGE-L

BERTScore

G-Eval 0.35

Chatéval

0.30
Clinical Librarians
Clinicians

Evidence Synthesis

Researchers 0.25

Medical Researchers

Total
0.20

Clinicians
Emergency
Physicians

Interdisciplinary
Clinicians

0.15

Medical Researchers

- 0.10
Medical Review

Experts
Public Health
Consumers

Total - 0.05

FL PIO ED ES Total

Figure 3: Spearman’s p (rank-based correlation) between evaluation methods and human ratings on StorySparkQA
(left) and MSLR-COCHRANE (right). Higher values indicate stronger alignments with human judgments.
StorySparkQa’s dimensions include Grammar Correctness (GC), Answer Relevancy (AR), Contextual Relevancy
(CC), and Educational Appropriateness (EA). MSLR-COCHRANE’s dimensions include Fluency (FL), PIO Consis-
tency (PIO), Effect Direction (ED), and Evidence Strength (ES).

tency between the models’ scores and human rat-
ings, and Pearson’s correlation coefficient (Co-
hen et al., 2009), which calculates linear relation-
ships between the models’ scores and human rat-
ings (refer to Appendix A.5). To assess the internal
consistency of agent evaluations during in-group
debates, we compute inter-coder reliability within
each stakeholder group using Krippendorff’s Al-
pha (Krippendorff, 2011), shown in Appendix A.4.

4.4 Implementation Details

We experimented with Claude-3.7-Sonnet and
Qwen3-235B (Yang et al., 2024) as the underlying
models of MAJ-EVAL. For Qwen, we employed
the sglang (Zheng et al., 2024) inference frame-
work with the temperature set to 0.6. For Claude,
we accessed the model via the AWS API, using its
default temperature setting of 1.0.

We employ Google Scholar as our search engine
and follow the snowballing search strategy (Wohlin,
2014) for input document selection. Specifically,
we (1) locate the qualitative research publications

cited in each dataset paper (if there is no cita-
tion to qualitative research, we use keywords of
the NLG task description), and (2) conduct an
additional keyword-based search combining task-
relevant keywords with “qualitative interview” on
Google Scholar to retrieve recent publications from
the past three years, ensuring broader coverage and
up-to-date perspectives. From the results, we se-
lected three representative documents for the QAG
task in children’s story-reading (Xu et al., 2021;
Chen et al., 2025b; Sun et al., 2024) and two doc-
uments for the medical summarization task (Yang
et al., 2023; Yun et al., 2023) based on recency, task
relevance, and resource considerations.

5 Experiment Results

5.1 Overall Evaluation Performance

Our results show that similarity-based metrics, in-
cluding ROUGE-L and BERTScore, exhibited over-
all weak correlations with human ratings on most
evaluation dimensions across both StorySparkQA
and MSLR-COCHRANE (see Figure 3). It is no-

06

os =
ao a Grate
=== hatéval
Initial 05 lal)
a oe = Final
anu oa Baits A+0.08 4 +0.06
04 A+0.03
oh h6UL__k Py A+0.03 A+0.01 +0.02
5 oe oe ee |
& o
5
£03
£03 =
ae 3
3 a
8 8
- oo} =
on on
00 00
S " 8 s e Ss Ye
nese cuTBe™  ewpeh® pyr egctl® yer ciniceny rycen, uric e000 ew caste onsuTte per ad
° eau emerson rercisciO™  esic™ Tpesical RN Heal

Figure 4: Spearman correlations of MAJ-EVAL (Claude-3.7-Sonnet) stakeholder agents’ initial and final scores
with human judgments on StorySparkQA (left) and MSLR-COCHRANE (right). Dark blue bars higher than light
blue bars indicate improved alignment with human ratings after in-group debate.

table that ROUGE-L achieves its highest correla-
tion on the Grammar Correctness dimension, but
falls short in capturing deeper domain-specific di-
mensions such as Educational Appropriateness or
Effect Direction. We attribute this discrepancy to
ROUGE-L’s focus on token-level similarity.

For the LLM-as-a-judge methods, G-Eval out-
performed similarity-based metrics across both
datasets, showing better human alignment on most
domain-specific dimensions. ChatEval, which in-
tegrates multiple LLM agents, showed improve-
ments on StorySparkQA but did not yield gains
on MSLR-COCHRANE. These findings suggest
that LLM-as-a-judge methods can achieve closer
alignment with human evaluations when carefully
designed. However, their effectiveness depends
on manually crafted prompts and does not readily
generalize across different tasks and domains.

Worth noting that our MAJ-EVAL outperformed
all baseline methods across both tasks and the ma-
jority of evaluation dimensions. When used with
different underlying models, MAJ-EVAL demon-
strated consistent, robust alignment with human rat-
ings, particularly on domain-specific dimensions.
Detailed computational and time costs of MAJ-
EVAL are reported in Appendix A.6.

5.2. Domain-Specific Dimensions Alignment

Across both domains, baseline methods exhibited
inconsistent performance. For instance, ROUGE-L
showed a higher correlation with StorySparkQA’s
Grammar Correctness but lower with MSLR-
COCHRANE’s Fluency. BERTScore underper-
formed on StorySparkQA but excelled on MSLR-
COCHRANE. Similarly, G-Eval and ChatEval var-
ied in alignment with human ratings across differ-

ent tasks. We also observe that these methods strug-
gled to align well with humans on domain-specific
dimensions like Educational Appropriateness for
the children’s QAG task and Effect Direction for
the medical summarization task.

We attribute this inconsistency to different task
settings. The QAG task in children’s story-reading
requires the integration of external real-world
knowledge beyond the source story. Therefore,
model-generated QA pairs often include knowl-
edge content divergent from human-authored con-
tent, reducing the effectiveness of similarity-based
metrics. In contrast, MSLR-COCHRANE’s data-
points include generated and reference summary
pairs that are both grounded in the same source
documents, making metrics like BERTScore more
effective for capturing textual similarity.

Remarkably, MAJ-EVAL consistently exhibited
superior alignment with human ratings across both
domains, particularly on domain-specific dimen-
sions. In StorySparkQA, stakeholder groups such
as Teachers and Educational Experts from the
Claude-3.7-Sonnet variant and Early Childhood Ed-
ucators from the Qwen-3-235B variant showed the
highest correlations on Context Relevancy and Chil-
dren’s Educational Appropriateness. This strong
alignment is consistent with their real-world do-
main familiarity with both pedagogical goals and
children’s cognitive needs (Cade, 2023). However,
MASJ-EVAL showed a weaker correlation on Gram-
mar Correctness. This discrepancy may arise be-
cause the stakeholder agents tend to prioritize ed-
ucational and developmental appropriateness over
strict grammatical accuracy.

For MSLR-COCHRANE, MAJ-EVAL also cor-
related highly with humans, particularly on the

Effect Direction and Evidence Strength. Among
the stakeholder agents, Emergency Physicians and
Clinicians in the Claude-3.7-Sonnet variant showed
strong alignment with human ratings on Effect Di-
rection, which is critical for interpreting interven-
tion outcomes. Medical Researchers in both vari-
ants performed best on Evidence Strength, which
reflects the certainty and quality of clinical sum-
maries. These results reflect the effectiveness
of MAJ-EVAL’s stakeholder-grounded personas
in capturing domain-specific evaluation dimen-
sions. While MAJ-EVAL achieved reasonable cor-
relations on PIO Consistency, it lagged behind
BERTScore and G-Eval. We believe it is because
similarity-based metrics are better suited for assess-
ing alignment between the generated and reference
summaries in terms of document-anchored compo-
nents like population, intervention, and outcome.

5.3. Qualitative Analysis

We randomly sampled one example each from
StorySparkQA and MSLR-COCHRANE (see Ap-
pendix A.9.1 and A.9.2) to qualitatively compare
and analyze the evaluation outcomes of G-Eval,
ChatEval, and MAJ-EVAL, alongside human rat-
ings. Table 9 presents an overall comparison of
the three methods with human annotations. In both
evaluations, ChatEval and MAJ-EVAL output qual-
itative analysis of the QA pair while G-Eval only
outputs a score. Within ChatEval, two agent roles
(the General Public and the Critic) are designed to
simulate collaborative assessment. However, the
two agents’ feedback struggles to align with the
domain-specific dimensions used by humans, pri-
marily reiterating generic task instructions such as
integrating real-world knowledge in QA pairs or
ensuring clarity in clinical summaries.

As MAJ-EVAL leverages multiple human-
aligned evaluative dimensions, its final aggregated
evaluations encompass the dimensions used for ex-
pert annotations. For instance, in the task of QAG
for children’s story-reading, the evaluation from the
Teacher group highlights dimensions such as con-
textual relevance (i.e., how well the QA connects
the story to real-world knowledge) and educational
appropriateness (e.g., simplicity and age suitability
of language). These dimensions are derived from
the extracted dimensions of the Teacher stakeholder
group. For instance, the teacher persona’s emphasis
on“using simple ‘what’ questions to inspire chil-
dren’s thinking during story interactions” (Table
10) is reflected in MAJ-EVAL’s final evaluation.

In addition, we observe that MAJ-EVAL’s
agents often introduce evaluative dimensions be-
yond existing human evaluation dimensions. In the
QAG task for children’s story-reading, the Teacher
agents discuss the value of follow-up questions to
foster deeper thinking. In the medical summariza-
tion task, Medical Researcher agents highlight the
significance of clinical specificity. These examples
demonstrate that MAJ-EVAL’s multi-dimensional
evaluation not only align with established human
evaluation dimensions but also offer complemen-
tary, stakeholder-grounded insights for domain-
specific real-world evaluations.

5.4 Ablation Study

5.4.1 Effectiveness of MAJ-EVAL’s Persona
Creation

To evaluate the effectiveness of MAJ-EVAL’s per-
sona creation step, we conduct an ablation study by
assigning each stakeholder with a simple role defi-
nition (e.g., “You are a preschool teacher who often
reads books to your students.” for the teacher agent)
instead of the detailed persona. We then compute
the Spearman correlation (p) between each group’s
scores and human ratings under both the simple
role and detailed persona conditions. The results
are presented in Appendix A.3.

Across both domains, MAJ-EVAL correlates
more closely with human ratings on both the over-
all quality and individual evaluation dimensions.
This observation justifies that our proposed imple-
mentation of MAJ-EVAL (i.e., a detailed persona
construction process and a debate mechanism) led
to evaluation metrics that correlated higher with
human ratings.

5.4.2 Impact of MAJ-EVAL’s Multi-Agent
In-Group Free Debate Mechanism

To examine the impact of the in-group debate mech-
anism, we extract each stakeholder agent’s initial
score before the in-group debate and final score
post the debate (see Section 3.2) and calculate the
correlation (Spearman’s p) between each group’s
scores and human ratings before and after the de-
bate. The results are presented in Figure 4 and
Figure 5 in Appendix A.5.

In both domains, we observe that many stake-
holder groups’ initial evaluations already exhibit
strong alignment with human ratings, demonstrat-
ing the effectiveness of MAJ-EVAL’s persona con-
struction step. Importantly, all task-level averages
increased after the debate, with 15 out of 20 stake-

holder groups showing positive gains. This im-
provement suggests that the in-group debate mech-
anism effectively supports most stakeholder agents
in refining their evaluations.

However, a few groups, like the Language Re-
searchers for the children’s QAG task, exhibited
reduced correlations after the debate. Our anal-
ysis of their debate logs reveals that while their
initial scores adhered closely to the task descrip-
tion, the debating process led these stakeholders to
consider additional evaluative dimensions, such as
inferential scaffolding and vocabulary richness. Al-
though these dimensions extend beyond those used
in human ratings, these extended dimensions reflect
theoretical concerns rooted in early childhood edu-
cation (Vygotsky, 1978; Wasik et al., 2006). Thus,
we believe the in-group debate step can further en-
rich human evaluation with more comprehensive
evaluative dimensions specific to the task.

6 Discussion

Comparing MAJ-EVAL with automated metrics,
single- and multi-LLM evaluation methods, we
observe that MAJ-EVAL consistently achieves
higher correlations with human ratings on domain-
specific dimensions (e.g., Educational Appropri-
ateness). However, a trade-off emerges between
MAJ-EVAL’s performance on domain-specific and
textual-level dimensions (e.g., Grammar Correct-
ness), indicating that stakeholder agents, shaped
by their perspectives, tend to prioritize domain-
specific dimensions over surface-level linguistic
fidelity. This tendency aligns with real-world hu-
man evaluation behaviors (Clark et al., 2021).

These findings highlight the importance of align-
ing evaluation methods with task-specific objec-
tives. We recommend applying MAJ-EVAL in eval-
uating NLP applications that involve diverse user
needs, multiple social roles, or domain expertise.
For evaluating surface-level linguistic fidelity, tradi-
tional automated metrics or single LLM-as-a-judge
methods may remain more suitable.

Overall, across both tasks, MAJ-EVAL exhibits
consistently stronger correlations with human rat-
ings on task-specific dimensions. This observation
justifies the cross-domain generalizability of MAJ-
EVAL as well as the effectiveness of integrating
multi-stakeholder evaluative dimensions in evaluat-
ing generated text for real-world tasks.

7 Conclusion and Future Work

In summary, this work presents MAJ-EVAL, a
multi-agent evaluation framework designed for
real-world NLG evaluation. MAJ-EVAL 1) derives
stakeholders’ evaluative dimensions from domain-
specific documents, 2) constructs stakeholder agent
personas grounded in these dimensions, and 3) or-
ganizes LLM agents into in-group debates to col-
laboratively generate evaluations. Our case stud-
ies on two domain-specific tasks, namely QAG
in children’s interactive story-reading and medi-
cal summarization, demonstrate that MAJ-EVAL’s
stakeholder-grounded evaluations achieve stronger
alignment with human ratings on multiple domain-
specific dimensions compared with existing auto-
mated metrics, single-LLM evaluations, and multi-
agent evaluation methods.

One possible future work will focus on data col-
lection: gathering human annotators’ rationales to
better understand how they form judgments, what
factors they consider most critical, and how they
prioritize these factors based on their expertise. An-
other future direction involves training models to
develop more accurate LLM role-play agents. For
example, in addition to prompting-based strategies,
we plan to explore reinforcement learning fine-
tuning LLM agents using the collected rationales
to enhance their role-playing capability, thereby
producing more human-aligned evaluation results.

8 Limitations

This work focuses on the design and development
of a multi-agent evaluation framework tailored to
real-world text generation scenarios. While our
case studies highlight the effectiveness of MAJ-
EVAL compared to existing automated metrics and
both single- and multi-LLM evaluation methods,
several limitations remain. First, although our case
studies in children’s interactive QA and medical
summarization show promising results, these do-
mains represent only a subset of real-world appli-
cations with limited human ratings. Future work
could examine additional domains to further assess
the framework’s generalizability. Second, there re-
main limited datasets annotated by a diverse set of
stakeholders. We call for future work on collecting
data that reflects multiple stakeholder perspectives.
Third, as the baseline methods (i.e., G-Eval and
ChatEval) are backed by LLMs with large parame-
ters, we did not test MAJ-EVAL with a wider range
of models(e.g., Llama 3 (Touvron et al., 2023)). In

the future, we can test MAJ-EVAL using smaller
models to better understand the framework’s com-
patibility across model scales.

Acknowledgments

This work is supported in part by the National Sci-
ence Foundation (NSF) under award number IIS-
2302730, the National Institutes of Health (NIH)
under award number RO1AI188576, and the North-
eastern University Tier-1 Research Grant. The con-
tent is solely the responsibility of the authors and
does not necessarily represent the official views of
NSF or NIH. This work used GPU at NCSA Delta
through allocation CIS240439 from the Advanced
Cyberinfrastructure Coordination Ecosystem: Ser-
vices & Support (ACCESS) program (Boerner
et al., 2023), which is supported by U.S. National
Science Foundation grants #2138259, #2138286,
#2138307, #2137603, and #2138296.

References

Anya Belz, Anastasia Shimorina, Shubham Agarwal,
and Ehud Reiter. 2021. The ReproGen shared task
on reproducibility of human evaluations in NLG:
Overview and results. In Proceedings of the 14th
International Conference on Natural Language Gen-
eration, pages 249-258, Aberdeen, Scotland, UK.
Association for Computational Linguistics.

Timothy J. Boerner, Stephen Deems, Thomas R. Furlani,
Shelley L. Knuth, and John Towns. 2023. Access:
Advancing innovation: Nsf’s advanced cyberinfras-
tructure coordination ecosystem: Services & support.
In Practice and Experience in Advanced Research
Computing 2023: Computing for the Common Good,
PEARC ’23, page 173-176, New York, NY, USA.
Association for Computing Machinery.

June Cade. 2023. Child-centered pedagogy: Guided
play-based learning for preschool children with spe-
cial needs. Cogent Education, 10(2):2276476.

Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan
Yu, Wei Xue, Shan Zhang, Jie Fu, and Zhiyuan Liu.
2023. Chateval: Towards better Ilm-based evaluators
through multi-agent debate. ArXiv, abs/2308.07201.

Chaoran Chen, Bingsheng Yao, Ruishi Zou, Wenyue
Hua, Weimin Lyu, Yanfang Ye, Toby Jia-Jun Li,
and Dakuo Wang. 2025a. Towards a design guide-
line for rpa evaluation: A survey of large language
model-based role-playing agents. arXiv preprint
arXiv:2502.13012.

Jiaju Chen, Yuxuan Lu, Shao Zhang, Bingsheng Yao,
Yuanzhe Dong, Ying Xu, Yunyao Li, Qianwen Wang,
Dakuo Wang, and Yuling Sun. 2024. StorySparkQA:

Expert-annotated QA pairs with real-world knowl-
edge for children‘s story-based learning. In Proceed-
ings of the 2024 Conference on Empirical Methods in
Natural Language Processing, pages 17351-17370,
Miami, Florida, USA. Association for Computational
Linguistics.

Jiaju Chen, Minglong Tang, Yuxuan Lu, Bingsheng Yao,
Elissa Fan, Xiaojuan Ma, Ying Xu, Dakuo Wang,
Yuling Sun, and Liang He. 2025b. Characterizing
llm-empowered personalized story reading and inter-
action for children: Insights from multi-stakeholder
perspectives. In Proceedings of the 2025 CHI Con-
ference on Human Factors in Computing Systems,
CHI 2025, YokohamaJapan, 26 April 2025- 1 May
2025, pages 1002:1-1002:24. ACM.

Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang,
Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia
Qin, Yaxi Lu, Ruobing Xie, et al. 2023. Agent-
verse: Facilitating multi-agent collaboration and ex-
ploring emergent behaviors in agents. arXiv preprint
arXiv:2308.10848.

Zhumin Chu, Qingyao Ai, Yiteng Tu, Haitao Li, and
Yiqun Liu. 2024. Pre: A peer review based large
language model evaluator. ArXiv, abs/2401.15641.

Elizabeth Clark, Tal August, Sofia Serrano, Nikita
Haduong, Suchin Gururangan, and Noah A. Smith.
2021. All that‘s ‘human’ is not gold: Evaluating
human evaluation of generated text. In Proceedings
of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers), pages 7282-7296, Online.
Association for Computational Linguistics.

Israel Cohen, Yiteng Huang, Jingdong Chen, Jacob Ben-
esty, Jacob Benesty, Jingdong Chen, Yiteng Huang,
and Israel Cohen. 2009. Pearson correlation coeffi-
cient. Noise reduction in speech processing, pages
1-4.

Emma Croxford, Yanjun Gao, Nicholas Pellegrino,
Karen K Wong, Graham Wills, Elliot First, Frank J
Liao, Cherodeep Goswami, Brian Patterson, and Ma-
jid Afshar. 2024. Evaluation of large language mod-
els for summarization tasks in the medical domain: A
narrative review. arXiv preprint arXiv:2409. 18170.

Jay DeYoung, Iz Beltagy, Madeleine van Zuylen, Bailey
Kuehl, and Lucy Lu Wang. 2021. MS‘2: Multi-
document summarization of medical studies. In Pro-
ceedings of the 2021 Conference on Empirical Meth-
ods in Natural Language Processing, pages 7494—
7513, Online and Punta Cana, Dominican Republic.
Association for Computational Linguistics.

Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei
Liu. 2023. Gptscore: Evaluate as you desire. In
North American Chapter of the Association for Com-
putational Linguistics.

Simret Araya Gebreegziabher, Charles Chiang, Zichu
Wang, Zahra Ashktorab, Michelle Brachman, Werner

Geyer, Toby Jia-Jun Li, and Diego Gémez-Zara.
2025. Metricmate: An interactive tool for gener-
ating evaluation criteria for Ilm-as-a-judge workflow.
In Proceedings of the 4th Annual Symposium on
Human-Computer Interaction for Work, CHIWORK
°25, New York, NY, USA. Association for Computing
Machinery.

Zexue He, Yu Wang, An Yan, Yao Liu, Eric Chang,
Amilcare Gentili, Julian McAuley, and Chun-Nan
Hsu. 2023. MedEval: A multi-level, multi-task,
and multi-domain medical benchmark for language
model evaluation. In Proceedings of the 2023 Con-
ference on Empirical Methods in Natural Language
Processing, pages 8725-8744, Singapore. Associa-
tion for Computational Linguistics.

Alex Kim, Keonwoo Kim, and Sangwon Yoon. 2024.
DEBATE: Devil‘s advocate-based assessment and
text evaluation. In Findings of the Association for
Computational Linguistics: ACL 2024, pages 1885-
1897, Bangkok, Thailand. Association for Computa-
tional Linguistics.

Mahnaz Koupaee, Jake W. Vincent, Saab Mansour, Igor
Shalyminov, Han He, Hwanjun Song, Raphael Shu,
Jianfeng He, Yi Nian, Amy Wing-mei Wong, Kyu J.
Han, and Hang Su. 2025. Faithful, unfaithful or am-
biguous? multi-agent debate with initial stance for
summary evaluation. In Proceedings of the 2025
Conference of the Nations of the Americas Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (Volume 1: Long Pa-
pers), pages 12209-12246, Albuquerque, New Mex-
ico. Association for Computational Linguistics.

Klaus Krippendorff. 2011. Computing krippendorff’s
alpha-reliability.

Yukyung Lee, Joonghoon Kim, Jaehee Kim, Hyowon
Cho, and Pilsung Kang. 2024. Checkeval: Robust
evaluation framework using large language model
via checklist. ArXiv, abs/2403.18771.

Ang Li, Haozhe Chen, Hongseok Namkoong, and
Tianyi Peng. 2025. Llm generated persona
is a promise with a catch. arXiv preprint
arXiv:2503.16527.

Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad
Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhat-
tacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu,
Kai Shu, Lu Cheng, and Huan Liu. 2024a. From gen-
eration to judgment: Opportunities and challenges of
Ilm-as-a-judge. CoRR, abs/2411.16594.

Renhao Li, Minghuan Tan, Derek F. Wong, and Min
Yang. 2024b. CoEvol: Constructing better responses
for instruction finetuning through multi-agent coop-
eration. In Proceedings of the 2024 Conference on
Empirical Methods in Natural Language Processing,
pages 4703-4721, Miami, Florida, USA. Association
for Computational Linguistics.

Yu Li, Shenyu Zhang, Rui Wu, Xiutian Huang, Yon-
grui Chen, Wenhao Xu, Guilin Qi, and Dehai Min.

2024c. Mateval: A multi-agent discussion frame-
work for advancing open-ended text evaluation. In
International Conference on Database Systems for
Advanced Applications, pages 415-426. Springer.

Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang,
Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and
Zhaopeng Tu. 2024. Encouraging divergent thinking
in large language models through multi-agent debate.
In Proceedings of the 2024 Conference on Empiri-
cal Methods in Natural Language Processing, pages
17889-17904, Miami, Florida, USA. Association for
Computational Linguistics.

Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summariza-
tion Branches Out, pages 74-81, Barcelona, Spain.
Association for Computational Linguistics.

Yen-Ting Lin and Yun-Nung Chen. 2023. LLM-eval:
Unified multi-dimensional automatic evaluation for
open-domain conversations with large language mod-
els. In Proceedings of the 5th Workshop on NLP for
Conversational Al (NLP4ConvAI 2023), pages 47—
58, Toronto, Canada. Association for Computational
Linguistics.

Yang Liu, Dan Iter, Yichong Xu, Shuo Wang, Ruochen
Xu, and Chenguang Zhu. 2023. G-eval: Nlg evalu-
ation using gpt-4 with better human alignment. In
Conference on Empirical Methods in Natural Lan-
guage Processing.

Yixin Liu, Kejian Shi, Katherine He, Longtian Ye,
Alexander Fabbri, Pengfei Liu, Dragomir Radev, and
Arman Cohan. 2024. On learning to summarize with
large language models as references. In Proceed-
ings of the 2024 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies (Volume
1: Long Papers), pages 8647-8664, Mexico City,
Mexico. Association for Computational Linguistics.

Yuxuan Lu, Bingsheng Yao, Hansu Gu, Jing Huang,
Jessie Wang, Yang Li, Jiri Gesi, Qi He, Toby Jia-Jun
Li, and Dakuo Wang. 2025. Uxagent: A system for
simulating usability testing of web design with Ilm
agents. Preprint, arXiv:2504.09407.

Yuxuan Lu, Bingsheng Yao, Shao Zhang, Yun Wang,
Peng Zhang, Tun Lu, Toby Jia-Jun Li, and Dakuo
Wang. 2023. Human still wins over Ilm: An em-
pirical study of active learning on domain-specific
annotation tasks. arXiv preprint arXiv:2311.09825.

Gary M Olson and Judith S Olson. 2000. Distance
matters. Human-computer interaction, 15(2-3):139-
178.

OpenAI. 2023. GPT-4 Technical Report. ArXiv.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 311-318, Philadelphia,

Pennsylvania, USA. Association for Computational
Linguistics.

Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Mered-
ith Ringel Morris, Percy Liang, and Michael S Bern-
stein. 2023. Generative agents: Interactive simulacra
of human behavior. In Proceedings of the 36th an-
nual acm symposium on user interface software and
technology, pages 1-22.

Joon Sung Park, Carolyn Q Zou, Aaron Shaw, Ben-
jamin Mako Hill, Carrie Cai, Meredith Ringel Morris,
Robb Willer, Percy Liang, and Michael S Bernstein.
2024. Generative agent simulations of 1,000 people.
arXiv preprint arXiv:2411.10109.

Yiting Ran, Xintao Wang, Tian Qiu, Jiaqing Liang,
Yanghua Xiao, and Deqing Yang. 2025. Bookworld:
From novels to interactive agent societies for creative
story generation. Preprint, arXiv:2504.14538.

Ananya B Sai, Akash Kumar Mohankumar, and
Mitesh M Khapra. 2022. A survey of evaluation met-
rics used for nlg systems. ACM Computing Surveys
(CSUR), 55(2):1-39.

Pranab Kumar Sen. 1968. Estimates of the regres-
sion coefficient based on kendall’s tau. Journal of
the American statistical association, 63(324):1379-
1389.

Yuling Sun, Jiaju Chen, Bingsheng Yao, Jiali Liu,
Dakuo Wang, Xiaojuan Ma, Yuxuan Lu, Ying Xu,
and Liang He. 2024. Exploring parent’s needs for
children-centered ai to support preschoolers’ inter-
active storytelling and reading activities. Proceed-
ings of the ACM on Human-Computer Interaction,
8(CSCW2):1-25.

Annalisa Szymanski, Noah Ziems, Heather A. Eicher-
Miller, Toby Jia-Jun Li, Meng Jiang, and Ronald A.
Metoyer. 2025. Limitations of the Ilm-as-a-judge
approach for evaluating llm outputs in expert knowl-
edge tasks. In Proceedings of the 30th International
Conference on Intelligent User Interfaces, YUI’25,
page 952-966, New York, NY, USA. Association for
Computing Machinery.

Craig Thomson, Ehud Reiter, and Anya Belz. 2024.
Common flaws in running human evaluation ex-
periments in NLP. Computational Linguistics,
50(2):795-805.

Hugo Touvron, Louis Martin, Kevin R. Stone, Peter
Albert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava,
Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cris-
tian Canton Ferrer, Moya Chen, Guillem Cucurull,
David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin
Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,
Naman Goyal, A. Hartshorn, Saghar Hosseini, Rui
Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez,
Madian Khabsa, Isabel M. Kloumann, A. Korenev,
Punit Singh Koura, Marie-Anne Lachaux, Thibaut
Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yun-
ing Mao, Xavier Martinet, Todor Mihaylov, Pushkar

Mishra, Igor Molybog, Yixin Nie, Andrew Poul-
ton, Jeremy Reizenstein, Rashi Rungta, Kalyan Sal-
adi, Alan Schelten, Ruan Silva, Eric Michael Smith,
R. Subramanian, Xia Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, An-
gela Fan, Melanie Kambadur, Sharan Narang, Aure-
lien Rodriguez, Robert Stojnic, Sergey Edunov, and
Thomas Scialom. 2023. Llama 2: Open Foundation
and Fine-Tuned Chat Models. ArXiv.

Lev S Vygotsky. 1978. Mind in society: The develop-
ment of higher psychological processes, volume 86.
Harvard university press.

Lucy Lu Wang, Yulia Otmakhova, Jay DeYoung,
Thinh Hung Truong, Bailey Kuehl, Erin Bransom,
and Byron Wallace. 2023a. Automated metrics
for medical multi-document summarization disagree
with human evaluations. In Proceedings of the 61st
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 9871—
9889, Toronto, Canada. Association for Computa-
tional Linguistics.

Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang,
Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie,
Jindong Wang, Xingxu Xie, Wei Ye, Shi-Bo Zhang,
and Yue Zhang. 2023b. Pandalm: An automatic
evaluation benchmark for llm instruction tuning opti-
mization. ArXiv, abs/2306.05087.

Barbara A Wasik, Mary Alice Bond, and Annemarie
Hindman. 2006. The effects of a language and liter-
acy intervention on head start children and teachers.
Journal of educational Psychology, 98(1):63.

Claes Wohlin. 2014. Guidelines for snowballing in sys-
tematic literature studies and a replication in software
engineering. In Proceedings of the 18th international
conference on evaluation and assessment in software
engineering, pages 1-10.

Siyi Wu, Weidan Cao, Shihan Fu, Bingsheng Yao, Ziqi
Yang, Changchang Yin, Varun Mishra, Daniel Addi-
son, Ping Zhang, and Dakuo Wang. 2025. Cardioai:
A multimodal ai-based system to support symptom
monitoring and risk prediction of cancer treatment-
induced cardiotoxicity. In Proceedings of the 2025
CHI Conference on Human Factors in Computing
Systems, pages 1-22.

Ying Xu, Dakuo Wang, Penelope Collins, Hyelim Lee,
and Mark Warschauer. 2021. Same benefits, different
communication patterns: Comparing Children’s read-
ing with a conversational agent vs. a human partner.
Computers & Education, 161:104059.

Ying Xu, Dakuo Wang, Mo Yu, Daniel Ritchie, Bing-
sheng Yao, Tongshuang Wu, Zheng Zhang, Toby Li,
Nora Bradford, Branda Sun, Tran Hoang, Yisi Sang,
Yufang Hou, Xiaojuan Ma, Diyi Yang, Nanyun Peng,
Zhou Yu, and Mark Warschauer. 2022. Fantastic
Questions and Where to Find Them: FairytaleQA —
An Authentic Dataset for Narrative Comprehension.

In Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 447-460, Dublin, Ireland.
Association for Computational Linguistics.

An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui,
Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu,
Fei Huang, Haoran Wei, et al. 2024. Qwen2. 5 tech-
nical report. arXiv preprint arXiv:2412.15115.

Qian Yang, Yuexing Hao, Kexin Quan, Stephen Yang,
Yiran Zhao, Volodymyr Kuleshov, and Fei Wang.
2023. Harnessing biomedical literature to calibrate
clinicians’ trust in ai decision support systems. In
Proceedings of the 2023 CHI Conference on Human
Factors in Computing Systems, CHI ’23, New York,
NY, USA. Association for Computing Machinery.

Zigi Yang, Yuxuan Lu, Jennifer Bagdasarian,
Vedant Das Swain, Ritu Agarwal, Collin Campbell,
Waddah Al-Refaire, Jehan El-Bayoumi, Guodong
Gao, Dakuo Wang, Bingsheng Yao, and Nawar
Shara. 2025. Recover: Designing a large language
model-based remote patient monitoring system for
postoperative gastrointestinal cancer care. Preprint,
arXiv:2502.05740.

Bingsheng Yao. 2024. Enhance machine reasoning via
active learning with human rationales. Ph.D. thesis,
Rensselaer Polytechnic Institute, Troy, NY.

Bingsheng Yao, Guiming Chen, Ruishi Zou, Yuxuan Lu,
Jiachen Li, Shao Zhang, Yisi Sang, Sijia Liu, James
Hendler, and Dakuo Wang. 2024. More samples
or more prompts? exploring effective few-shot in-
context learning for IIms with in-context sampling.
In Findings of the Association for Computational
Linguistics: NAACL 2024, pages 1772-1790.

Bingsheng Yao, Ishan Jindal, Lucian Popa, Yannis Kat-
sis, Sayan Ghosh, Lihong He, Yuxuan Lu, Shashank
Srivastava, Yunyao Li, James Hendler, et al. 2023a.
Beyond labels: Empowering human annotators with
natural language explanations through a novel active-
learning architecture. In Findings of the Association
for Computational Linguistics: EMNLP 2023, pages
11629-11643.

Bingsheng Yao, Prithviraj Sen, Lucian Popa, James
Hendler, and Dakuo Wang. 2023b. Are human expla-
nations always helpful? towards objective evaluation
of human natural language explanations. In Proceed-
ings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 14698-14713, Toronto, Canada. Association
for Computational Linguistics.

Bingsheng Yao, Prithviraj Sen, Lucian Popa, James
Hendler, and Dakuo Wang. 2023c. Are human expla-
nations always helpful? towards objective evaluation
of human natural language explanations. In Proceed-
ings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 14698-14713.

Hye Yun, Iain Marshall, Thomas Trikalinos, and By-
ron Wallace. 2023. Appraising the potential uses
and harms of LLMs for medical systematic reviews.
In Proceedings of the 2023 Conference on Empiri-
cal Methods in Natural Language Processing, pages
10122-10139, Singapore. Association for Computa-
tional Linguistics.

Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q
Weinberger, and Yoav Artzi. 2019. Bertscore: Eval-

uating text generation with bert. arXiv preprint
arXiv: 1904.09675.

Zhenjie Zhao, Yufang Hou, Dakuo Wang, Mo Yu,
Chengzhong Liu, and Xiaojuan Ma. 2022. Educa-
tional question generation of children storybooks via
question type distribution learning and event-centric
summarization. In Proceedings of the 60th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 5073-5085,
Dublin, Ireland. Association for Computational Lin-
guistics.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023.
Judging llm-as-a-judge with mt-bench and chatbot
arena. Advances in Neural Information Processing
Systems, 36:46595—46623.

Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie,
Chuyue Livia Sun, Jeff Huang, Cody Hao Yu, Shiyi
Cao, Christos Kozyrakis, lon Stoica, Joseph E Gon-
zalez, et al. 2024. Sglang: Efficient execution of
structured language model programs. Advances in
Neural Information Processing Systems, 37:62557—
62583.

Lianghui Zhu, Xinggang Wang, and Xinlong Wang.
2023. Judgelm: Fine-tuned large language
models are scalable judges. arXiv preprint
arXiv:2310.17631.

Mingchen Zhuge, Changsheng Zhao, Dylan Ashley,
Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong,
Zechun Liu, Ernie Chang, Raghuraman Krishnamoor-
thi, Yuandong Tian, et al. 2024. Agent-as-a-
judge: Evaluate agents with agents. arXiv preprint
arXiv:2410.10934.

A Appendix

Algorithm | illustrate the MAJ-EVAL’s in-group
debate process.

A.1_ In-Group Debate Algorithm

Algorithm 1 In-Group Multi-Agent Debate

Require: Task description T, Evaluation content
C, Evaluation format F’
Optional: Related context X
1: Initialize number of stakeholders n; max de-
bate rounds m
: Initialize coordinator agent Coo
: Initialize aggregator agent Agg
: fori = 1tondo
Instantiate stakeholder agent A;
: end for

wn

Duns

Phase 1: Independent Evaluation
7: fori = 1tondo
8: E; — A;.Evaluate(T, C, F, X)
9: end for

Phase 2: Free Debate

10: Initialize FinishedAgents + 0

i: H+ {£\,...,E,}; FinalFeedback — 0

12: for t = 1 to mdo

13: Si < Coo.SelectNextSpeaker(H)

14: F, © S;.Respond(T, C, F, X, H)

15: Append Ff; to H

16: if F;, contains “NO MORE COMMENTS”
and S, ¢ FinishedAgents then

17: Append Ff; to Final Feedback
18: Add 5S; to FinishedAgents
19: end if

20: if | Final Agents| = n then

21: break

22: end if

23: end for

Phase 3: Final Aggregation
24: Result +
Agg.Aggregate(H, FinalF eedback)

A.2 Task Descriptions

Question-Answer Generation in Children’s In-
teractive Story-Reading You need to evaluate
the quality of Al-generated question-answer pairs
from the storybook content. These AI-generated
question-answer pairs are designed for the interac-
tive storybook reading activity between parents and

children aged 3 to 6, and should be grammatically
correct and fluent in English. Parents expect to ask
questions that are grounded in the storybook con-
tent, but introduce real-world common knowledge
beyond the story content.

Multi-document Summarization in Medicine
You need to evaluate the quality of AI-generated
short biomedical summaries that integrate findings
from multiple literature reviews. These grammat-
ically correct and fluent summaries are designed
to help medical professionals efficiently capture
the key findings across studies and provide a co-
herent overview of relevant medical questions and
outcomes (just like the target summary).

A.3 Ablation Study with Simple Role
Definition

We conducted an ablation study by assigning each
stakeholder a straightforward and simple role de-
scription (e.g., prompt: “You are a preschool
teacher who often reads books to your students”).
The Spearman correlation analysis results for the
StorySparkQA and MSLR-COCHRANE datasets
are shown in Table | and 2.

A.4_ Inter-Rater Reliability

We calculate the inter-rater reliability using Krip-
pendorff’s Alpha (K-Alpha) (Krippendorff, 2011)
within each generated stakeholder group. The re-
sults are shown in Table 3.

A.5 Complete Experimental Results

We present the complete performance of MAJ-
EVAL along with all the baselines on the QAG in
children’s story-reading and medical summariza-
tion task in Table 4 and 5, respectively. In addition,
Figure 5 presents Spearman’s correlation between
MASJ-EVAL (Qwen-3-235B) stakeholder agents’
initial (pre-debate) and final (post-debate) scores
and human judgments.

A.6 Computational Cost of MAJ-EVAL

Based on our records, the average token consump-
tion for the Stakeholder Persona Creation stage is
about 34,103 tokens per document, depending on
document length. During the debate stage, each
stakeholder group uses approximately 18,281 to-
kens per datapoint. If personas are generated from
two documents and there are debates in four stake-
holder groups, the total token usage per task is
roughly 141,329 tokens. At Claude 3.7 Sonnet’s
pricing of $3 per million tokens, the cost is roughly

. Grammar Answer Contextual Educational
StorySparkQa Overall Quality Correctness Relevancy Consistency Appropriateness
Simple-Role Qwen-3-235B 0.42 0.12 0.43 0.27 0.31
MoS-Eval Qwen-3-235B 0.43 0.18 0.43 0.27 0.33
Simple-Role Claude-3.7-Sonnet 0.43 0.09 0.45 0.28 0.37
MoS-Eval Claude-3.7-Sonnet 0.47 0.14 0.45 0.33 0.40

Table 1: Spearman’s p correlations between different evaluation methods and human ratings for StorySparkQA.
Higher values indicate stronger alignment with human judgments. Bolded numbers are the overall best scores.

MSLR-COCHRANE Overall Quality Fluency Consistency De Stroneth
Simple-Role Qwen-3-235B 0.24 0.26 0.14 0.15 0.12
MoS-Eval Qwen-3-235B 0.39 0.34 0.21 0.26 0.18
Simple-Role Claude-3.7-Sonnet 0.30 0.08 0.16 0.30 0.18
MoS-Eval Claude-3.7-Sonnet 0.40 0.10 0.21 0.34 0.28

Table 2: Spearman’s p correlations between different evaluation methods and human ratings for StorySparkQA.
Higher values indicate stronger alignment with human judgments. Bolded numbers are the overall best scores.

Models Stakeholder Group Initial Eval Final Eval

‘StorySparkQa

Qwen3-235B Al Developers 0.46 0.25
Children 0.52 0.39
Early Childhood Educators 0.40 0.26
Language Researchers 0.48 0.27
Parents 0.40 0.29
Claude-3.7-Sonnet Al Developers 0.39 0.70
Children 0.34 0.52
Educational Experts 0.56 0.67
Parents 0.22 0.33
Teachers 0.43 0.59
MSLR-COCHRANE
Qwen3-235B Clinical Librarians 0.55 0.50
Clinicians 0.59 0.56
Evidence Synthesis Researchers. 0.54 0.54
Medical Researchers 0.52 0.44
Claude-3.7-Sonnet Clinicians 0.60 0.71
Emergency Physicians 0.65 0.73
Interdisciplinary Clinicians 0.76 0.83
Medical Review Experts 0.68 081
Medical Researchers 0.61 0.86
Public Health Consumers 0.66 0.86

Table 3: Krippendorff’s Alpha (K-Alpha) scores (based
on inter-rater agreement) for each stakeholder group on
the StorySparkQA and MSLR-COCHRANE datasets.

$0.42 per task. For Qwen-3-235B, which is open-
source, the token cost would be even lower.
Regarding latency, MAJ-EVAL processes a sin-
gle task in about 26.13 seconds on Qwen-3-235B
and 34.20 seconds on Claude 3.7 Sonnet, which
is practical for offline evaluation. Moreover, the
framework is highly scalable since stakeholder
group debates can be executed in parallel.
Overall, the computational cost of MAJ-EVAL
is significantly lower than real-world human ex-

pert evaluation, which typically requires hours to
days for annotation, involves substantially higher
budgets, and often faces challenges in recruiting
qualified experts.

A.7 Workflow of MAJ-EVAL’s Persona
Creation

Table 6 presents an example of MAJ-EVAL’s per-
sona creation process, including document selec-
tion, perspective extraction, and persona construc-
tion.

A.8 Examples of Stakeholder Persona
Creation

For the Stakeholder Persona Creation phase, we
show the extracted dimensions of the Parents’
group (see Table 7), and the extracted dimensions
of the Clinicians group (see Table 8), using Qwen-
3-235B as the underlying model. Additionally, Ta-
bles 10 and 11 each illustrate one created personas
for each stakeholder group on StorySparkQA and
MSLR-COCHRANE, respectively.

A.9 Qualitative Results

Table 9 presents the quantitative scores, qualitative
feedback, and captured dimensions from G-Eval,
ChatEval, and MAJ-EVAL’s teacher group, along-
side human ratings.

For the final evaluation, we show two snippets
of MAJ-EVAL’s aggregated evaluation results of
one stakeholder group along with the outputs of G-

Overall Quality amma Answer Contextual Educational
StorySparkga ‘orrectness Relevancy Consistency Appropriateness

rp re p T rp ot r p T r p r

Rouge-L 0.20 0.15 0.11 0.24 0.32 0.25 0.13 0.12 0.09 0.10 -0.08 -0.06 0.14 0.08 0.06
BERTScore 0.11 0.01 0.01 0.04 -0.04 -0.02 0.10 0.02 0.01 0.06 -0.07 -0.05 0.09 -0.01 -0.01
G-Eval (GPT4) 0.28 0.28 0.20 0.09 0.09 0.08 0.33 0.33 0.25 -0.00 0.14 0.10 027 0.25 0.19
G-Eval (Qwen-3-235B) 0.35 0.26 0.18 0.18 0.16 0.13 0.36 0.29 0.21 0.09 O11 0.08 0.31 0.20 0.15
G-Eval (Claude-3.7-Sonnet) 0.45. (0.31: (0.23 «0.16 0.20 0.15 0.44 0.39 0.29 0.19 0.19 0.14 0.36 0.20 0.16
ChatEval 0.45 0.36 0.29 0.06 0.13 0.12 0.34 0.31 0.26 0.32 0.34 0.31 041 0.24 0.20
ChatEval (Qwen-3-235B) 0.24 0.21 0.15 0.21 0.19 0.16 0.12 0.09 0.04 0.03 0.21 0.20 0.17
ChatEval (Claude-3.7-Sonnet) 0.26 0.20 0.15 0.25 0.19 0.15 0.12 0.08 0.06 0.16 0.16 0.13
MAJ-EVAL Qwen-3-235B 0.52 0.43 0.32 0.14 0.43 0.43 0.31 0.30 0.27 0.21 045 0.33 0.24
AI Developers 0.35 0.27 0.20 O.11 0.33 031 0.23 0.10 O11 0.10 0.33 0.24 0.18
Children 0.38 0.31 0.24 0.09 0.34 0.35 0.27 0.22 0.16 0.12 0.30 0.20 0.15
Early Childhood Educators 0.48 0.42 0.31 0.16 040 0.38 0.29 0.29 0.39 0.31 0.39 0.29 0,22
Language Researchers 0.42 0.29 0.23 0.09 0.27 0.22 0.17 0.28 0.15 0.13 0.42 031 0.24
Parents 0.46 0.40 0.30 0.16 041 041 031 0.23 0.20 0.16 0.39 031 0.23
MAJ-EVAL Claude-3.7-Sonnet 0.53 0.47 0.35 0.11 0.48 0.45 0.33 0.31 0.33 0.26 045 0.40 0.30
AI Developers 0.47 0.42 0.33 0.09 0.44 0.43 0.34 0.25 031 0.25 041 037 0.31
Children 048 041 0.31 0.09 048 0.43 0.32 0.23 0.29 0.23 041 0.35 0.28
Educational Experts 0.50 0.45 0.34 0.13 0.41 0.40 0.30 0.32 0.33 0.27 0.42 0.36 0.28
Parents 046 041 0.31 0.10 0.41 0.42 0.32 0.27 0.27 0.22 0.39 0.35 0.27
Teachers 0.54 0.47 0.37 0.09 0.44 040 0.32 0.36 0.36 0.29 0.45 040 0.33

Table 4: Pearson correlation coefficient (7), Spearman’s p correlations, and Kendall’s 7 between evaluation methods
and human ratings for StorySparkQA. Higher values indicate stronger alignment with human judgments. Bolded

numbers are the overall best scores. Underlined number.

's are the second best scores.

Rouge-L. 0.18 0.18 0.13 -0.2 -0.18 -0.15 0.07 0.07 0.05 0.25 0.21 0.18 0.16 0.17 0.13
BERTScore 0.22 0.28 0.19 0.21 0.20 0.16 0.23 0.23 0.17 0.18 0.22 0.18 -0.02 0.04 0.03
G-Eval 0.35 0.34 0.24 0.18 0.18 O15 0.33 0.31 0.23 0.25 0.26 0.22 0.11 O.11 0.08
G-Eval (Claude-3.7-Sonnet) 0.40 0.34 0.24 0.04 0.02 0.02 0.22 0.22 0.17 0.37 0.32 0.27 0.26 0.21 0.16
G-Eval (Qwen-3-235B) 0.25 0.20 0.14 0.07 0.03 0.02 0.14 0.29 0.21 0.25 0.17 0.14 0.09 0.02 0.02
ChatEval 0.18 0.20 0.16 0.28 0.25 0.23 0.08 0.13 0.11 0.09 0.09 0.09 0.07 0.05 0.05
ChatEval (Claude-3.7-Sonnet) 0.25 0.21 0.16 0.04 0.05 0.04 0.17 0.15 0.13 0.23 0.21 0.19 0.12 0.08 0.07
ChatEval (Qwen-3-235B) 0.28 0.34 0.27 0.13 O11 O10 O17 0.14 O11 0.18 0.23 0.21 0.21 0.28 0.24
MASJ-EVAL Qwen3-235B 0.38 0.39 0.27 0.27 0.34 0.28 0.19 0.21 O15 0.27 0.26 0.22 0.22 0.18 0.14
Clinical Librarians 0.20 0.22 0.16 0.14 0.12 0.10 0.16 0.19 0.15 0.09 0.09 0.08 0.13 0.11 0.09
Clinicians 0.26 0.25 0.18 0.18 0.24 0.21 0.19 0.16 0.13 0.20 0.21 0.19 0.08 0.08 0.06
Evidence Synthesis Researchers 0.35 0.34 0.26 0.26 0.29 0.26 0.14 0.15 O.11 0.29 0.30 0.27 0.17 0.17 0.14
Medical Researche: 0.32 0.32 0.25 0.23 0.22 0.20 0.07 0.07 0.06 0.22 0.21 0.20 0.27 0.25 0.22
MASJ-EVAL Claude-3.7-Sonnet 0.42 0.39 0.29 0.11 0.12 0.10 0.22 0.21 0.16 0.35 0.34 0.29 0.28 0.27 0.21
Clinicians 0.39 0.36 0.28 0.07 0.05 0.05 0.20 0.21 0.17 0.36 0.36 0.32 0.24 0.23 0.19
Emergency Room Physicians 041 0.40 0.32 0.06 0.05 0.05 0.24 0.24 0.20 0.37 0.37 0.35 0.24 0.24 0.21
Interdisciplinary Clinicians 0.35 0.32 0.27 0.14 0.12 0.12 0.12 0.12 010 0.30 0.29 0.28 0.25 0.24 0.22
Medical Researchers 0.40 0.39 0.32 0.08 0.08 0.08 0.27 0.23 0.20 0.33 0.34 0.32 0.23 0.23 0.21
Medical Systematic Review Experts 0.37 0.35 0.28 0.14 0.14 0.13 0.18 0.15 0.12 0.27 0.26 0.25 0.28 0.26 0.24
Public Health Consumers 0.38 0.35 0.29 0.11 0.08 0.08 0.19 0.18 0.16 0.31 0.30 0.29 0.27 0.25 0.23

Table 5: Pearson correlation coefficient (7), Spearman’s p correlations, and Kendall’s 7 between evaluation methods
and human ratings for MSLR-COCHRANE. Higher values indicate stronger alignment with human judgments.

Bolded numbers are the overall best scores. Underlined

Eval and ChatEval for both the StorySparkQA and
MSLR-COCHRANE datasets in Appendix A.9.1
and A.9.2.

A.9.1__ Example of Evaluation Outputs for
StorySparkQA

Story Text: Now the rich neighbor got together as
many mowers as ever he could; but the poor one

numbers are the second best scores.

could not hire a single man. At last he despaired
altogether and wept, because he did not know how
he could manage to get so much as a bit of hay for
the cow.

Question-Answer Pair: what do you use to cut
hay? a mower.

Averaged Human Score: 4.56

- G-Eval . =-- GEval
os] 77> chat as | > ChatEval
A-0.16 ‘A+0.01 Initial
mm Final A+0.08 ieee mm Final 4-0.10
o04 aos A+0.01
Ai 7 ea a = 2
£031_A:000 i Be __ £03 ean
5
a 3 A+0.05
B02 & 02-+------ 7 ----- ae ----  —---——-- -
: . | ij
00 00
S o Ss 7
prreseatcnen cane eaucatot posers 2 ical HO ciridon® sR caren? resents pverad?
iN 2 iii esi sical
gory CM ) gent? pot ed

Figure 5: Spearman correlations of MAJ-EVAL (Qwen-3-235B) stakeholder agents’ initial and final scores with
human judgments on StorySparkQA (left) and MSLR-COCHRANE (right). Dark blue bars higher than light blue
bars indicate improved alignment with human ratings after in-group debate.

G-Eval Score: 3
ChatEval Score: 3

General Public: Evaluation evidence: Assistant
1 provided a question-answer pair that was relevant
to the story context. However, the question did
not introduce new real-world knowledge beyond
the story content. Assistant 2 did not provide any
question-answer pair, making it impossible to eval-
uate its performance.
The score of Assistant one: 3
The score of Assistant two: |

Critic: Evaluation evidence: Assistant 1
provided a question-answer pair that was relevant
to the story context. However, the question did
not introduce new real-world knowledge beyond
the story content. Assistant 2 did not provide any
question-answer pair, making it impossible to
evaluate its performance.
The score of Assistant one: 3
The score of Assistant two: |

MAJ-EVAL’s Teacher Group: 4.33
Areas of Agreement All three stakeholders
agree that the question-answer pair:

¢ Successfully connects to the storybook con-
tent about mowers.

¢ Introduces age-appropriate real-world

knowledge about farming tools.

¢ Uses simple language accessible to children
ages 3-6.

¢ Is contextually relevant to the story.

Areas of Disagreement

¢ The primary divergence is regarding the ques-
tion’s format and educational value:

— Emma Watson (score: 5): Believes
the simplicity is a strength, arguing that
shorter, direct "what" questions are most
effective for the 3-6 age range during
interactive reading.

— Michael Chen and Sarah Thompson
(both score: 4): Identify opportunities
for improvement in:

* Modeling proper sentence structure
and capitalization.

« Using a more conversational framing
to enhance engagement.

* Adding follow-up questions to en-
courage deeper thinking.

Overall Assessment With an average score of
4.33, stakeholders generally view this question-
answer pair positively while acknowledging room
for improvement. The content appropriately
bridges story elements with real-world knowledge
in language accessible to young children. However,
the educational value could be enhanced through
better modeling of proper language structure and in-
corporating elements that stimulate deeper thinking
or conversational engagement. The question suc-
ceeds in its basic educational function but misses
opportunities to support more holistic language de-
velopment that early childhood educators value.

A.9.2_ Example of Evaluation Outputs for
MSLR-COCHRANE

Target Summary: Evidence available to date
does not allow any firm conclusions to be drawn
concerning the effectiveness of ambulatory
domicilary oxygen therapy in patients with COPD.

Stage

Step

Output

Document
Selection

Search document keywords

Documents found

Example: “Children reading with conversational agent

qualitative interview”, “Children interactive story reading
cscw”

Example: Sun et al.’s (2024), Chen et al.’s (2025b), Xu et al.’s
(2021)

Stakeholder groups identified

Dimensions identified

Parents of preschool children, preschool children, educators, AI
researchers

Example: Parents expect Al-generated questions to be tailored
to a child’s cognitive level and psychological age, rather than

Evaluative
Dimension
Extraction

being overly serious or professional.

Example: “Our participants indicated that when they used tools

such as C5 and C6 in Table 2 to answer children’s story-related

Evidence for each dimension

questions, the generated answers were often serious and

professional, not specifically tailored to children’s cognitive
level and psychological age, which caused it difficult for
children to understand.”

Example: A parent who expects Al-generated questions to
match his preschool daughter’s comprehension level. He draws

Persona

. Persona constructed
Construction

on his restaurant experience to explain complex ideas in
child-friendly ways. Examples of full persona details (name,

demographics, specialty, psychological traits, social
relationships) are included in Table 10.

Table 6: Overview of MAJ-EVAL’s Persona Creation Workflow. Detailed examples of identified perspectives and

created personas are presented in Appendix A.8.

Further studies are required in order to understand
the role of ambulatory oxygen in the management
of patients with COPD on long-term oxygen
therapy. These studies should separate patients
who desaturate from those who do not desaturate.
Generated Summary: There is some evidence
that oxygen therapy may improve exercise
performance in COPD. However, the evidence is
based on a small number of small studies with
variable quality of life outcomes. The results of
this review are limited by the small number and
small number available studies. Further research
is needed to determine whether oxygen therapy is
beneficial for exercise performance and whether it
is safe and effective in COPDs.

Averaged Human Score: 0.50 (on 0-1)

G-Eval Score: 3.08 (1.00 after normalized to 0-1)
ChatEval Score: 4.00 (0.75 after normalized to
0-1)

General Public: Evaluation evidence: Assistant
1 provided a clear and concise summary, highlight-
ing the current state of research and the need for
further studies. However, Assistant 2 did not pro-
vide any summary, making it impossible to evaluate
its performance.

The score of Assistant one: 4
The score of Assistant two: 0

Critic: Evaluation evidence: Assistant 1 pro-
vided a clear and concise summary, highlighting
the current state of research and the need for
further studies. However, it could have been more
specific about the types of studies needed, as
mentioned in the target summary. Assistant 2 did
not provide any summary, making it impossible to
evaluate its performance.
The score of Assistant one: 4
The score of Assistant two: 0

MAJ-EVAL’s Medical Researchers Group
Score: 3 (0.5 after normalized to 0-1)

Overview The evaluated summary is broadly
accurate and grammatically sound, effectively com-
municating the uncertainty and limitations of cur-
rent evidence regarding oxygen therapy in COPD.
All stakeholders agree that it appropriately con-
veys the need for further research and maintains a
reasonable tone for a general overview.

Areas of Consensus

¢ General Strengths:

— The summary is fluent.

Parent Characteristics: Parents are primary caregivers from diverse educational and professional backgrounds
who engage in interactive storybook reading activities with their children aged 3 to 6. These activities involve
conversational exchanges designed to promote language development, with varying levels of skill, time, and

motivation for interactive reading.

Perspective

Evidence

Parents expect question-answer pairs grounded in story
content but expanded to introduce real-world common
knowledge to stimulate children’s language and cognitive
development.

Parents may not always engage in conversation-rich story-
book reading due to skill, time, or inclination limitations.

Parents view AI tools as potential supports for interactive
reading, provided they generate age-appropriate, gram-
matically correct English pairs that reduce parental burden
while maintaining educational value.

Parents value generation of contextually engaging, per-
sonalized question-answer pairs that align with children’s
cognitive stages and sustain meaningful educational inter-
action.

Parents expect questions to stimulate creativity, critical
thinking, and curiosity rather than require rote factual
recall, necessitating responses that adapt to children’s ex-
ploratory thinking.

Parents emphasize the importance of maintaining gram-
matical correctness and speech-level appropriateness in
Al-generated content to model proper language habits.

Evaluation task defines this expectation explicitly in AI
tool design.

Parents may not always pause the story, ask questions, and
comment on their children’s response, either assuming the
child can learn well by listening or lacking skills or time
for interactive opportunities.

Studies show parents expect AI to enhance storytelling
through interactive elements that inspire connection and
provide appropriate resources.

Current AI tools struggle with personalized, adaptive in-
teraction that addresses children’s developmental needs.

Parents criticize AI tools for being rigid and limiting active
thinking development.

Parents identify overly complex vocabulary and adult-
perspective questions as comprehension barriers.

Table 7: Parental perspectives and supporting evidence extracted by MAJ-EVAL.

involved in patient care across various specialties who use
biomedical literature to validate clinical suggestions and dec

sions. They require concise, yet detailed summaries

that enable actionable decisions and consider patient-specific characteristics.

Perspective

Evidence

Prioritize evidence from biomedical literature that aligns
with the patient’s specific characteristics (e.g., demograph-
ics, comorbidities) when validating AI suggestions.

Value comprehensiveness and reproducibility of evidence,
ensuring both supporting and opposing studies are pre-
sented to avoid bias.

Use the PICO framework (Population, Intervention, Com-
parator, Outcome) to assess the relevance of literature
evidence for specific patient scenarios and demand speci-
ficity in reporting those elements.

Prefer concise summaries of evidence that provide suffi-
cient detail to ensure actionable decision-making, espe-
cially in time-constrained settings and request alerts for
critical disagreements between AI suggestions and litera-
ture evidence.

Clinicians emphasized the importance of applicability of
evidence to the patient’s situation, such as matching de-
mographics, comorbidities, and genetic factors (e.g., P8’s
case involving Liddle syndrome and family history).

Clinicians curated comprehensive evidence lists, includ-
ing all supporting and opposing studies and shared repro-
ducible search links to validate AI suggestions.

Clinicians applied PICO to match patient populations with
literature evidence, particularly in complex cases like rare
diseases. Feedback emphasized the need for specificity in
reporting, especially around PICO elements.

Emergency care clinicians requested alerts for critical dis-
agreements between AI suggestions and literature evi-
dence, balancing conciseness with clinical urgency.

Table 8: Clinician perspectives and supporting evidence extracted by MAJ-EVAL.

Dataset Method Quantitative Score

Qualitative Result | Dimensions

Grammar Correctness, Answer Relevancy,

Human Annotation 4.56 - Contextual Consistency, Educational
Appropriateness
G-Eval 3.00 No -
StorySparkQA
ChatEval 3.00 Yes Contextual Consistency
Grammar Correctness, Contextual
MAJ-EVAL 4.33 Yes Consistency, Educational Appropriateness,
Engagement
Human Annotation 0.50 - Flu ency PIO c onsistency, Effect
Direction, Evidence Strength
G-Eval 1.00 No -
MSLR-COCHRANE
ChatEval 0.75 Yes Clarity, Domain Relevancy
MAJ-EVAL 0.50 Yes Fluency, Terminology Relevancy, Effect

Direction

Table 9: Qualitative and quantitative comparison of G-Eval, ChatEval, and MAJ-EVAL on the StorySparkQA and
MSLR-COCHRANE datasets. On the MSLR-COCHRANE dataset, scores are normalized to 0-1 to match the human

rating scale.

— It captures the uncertainty and limita-
tions of current evidence.

¢ Key Shortcomings:

- Lacks critical clinical specificity, par-
ticularly the stratification of COPD
patients into desaturators and non-
desaturators—considered essential for
precision medicine, clinical decision-
making, and evidence-based learning.

— Omits key terminology, such as ''am-
bulatory domiciliary oxygen therapy",
which is crucial for aligning with clinical
guidelines and research frameworks.

— Conflation of outcomes: The summary
does not clearly distinguish between ef-
fectiveness outcomes (e.g., exercise per-
formance) and safety outcomes, under-
mining clarity for clinical interpretation
and learner education.

Notable Nuance Dr. Lukas Wagner emphasized
an additional concern specific to Al-based clinical
decision support systems—the repetitive phras-
ing (e.g., "small number") reduces clarity and could
impair accurate processing by AI tools. This issue,
while not highlighted by other stakeholders, is criti-
cal in contexts where evidence summaries are used
to train or validate AI systems.

Overall Interpretation While the summary
communicates the general uncertainty and limita-
tions of current evidence effectively, it falls short

of the precision and specificity required for clinical
application, medical education, and AI-driven deci-
sion support. The consensus is that it is a partially
adequate synthesis, but missing key elements sig-
nificantly reduce its utility in practice and research.

Average Score: 3.0 This score reflects a bal-
anced view that the content is usable for general
understanding but requires substantial refinement
to meet the standards of clinical precision, educa-
tional utility, and technological integration.

A.10 Prompts for Baseline Methods
A.10.1 Prompts for G-Eval

Your task is to rate the [evaluation content]
based on the evaluation criteria and the task
description, following the specified evaluation
steps.

Please make sure to read and understand these
instructions carefully. Keep this document open
while reviewing and refer to it as needed.

Task Description:
[task description]

Evaluation Criteria:

Overall Quality: The overall quality of the
[evaluation content] should reflect all of the
following dimensions: [Evaluation dimensions
adapted from the original paper].

Evaluation Steps:

1. Read the source text carefully.

Children - Emily Thompson
Demographic Info: A 4-year-old girl from Seattle, living with her parents, attending preschool and developing strong language
skills.
Evaluative Dimension: Emily is naturally curious and seeks to understand the world through asking countless questions about
everything she encounters.
Specialty: Demonstrates exceptional verbal curiosity and quick learning ability in exploring new concepts during story
interactions.
Psychological Traits: Highly inquisitive, easily excited by new information, and displays a playful approach to learning
through storytelling and questioning.
Social Relationships: Enjoys interactive storytelling with parents and teachers, frequently engaging them with follow-up
questions about story content.

Parent - Rachel Bennett
Demographic Info: A 35-year-old marketing professional and mother of two, balancing work and family responsibilities.
Evaluative Dimension: Rachel wants to introduce real-world common knowledge beyond story content during interactive
reading sessions.
Specialty: Skilled at finding creative ways to expand her children’s understanding through contextual learning.
Psychological Traits: Pragmatic, goal-oriented, and deeply committed to her children’s educational development.
Social Relationships: Collaborative with teachers and open to technological tools that support her parenting goals.

Teacher - Emma Watson
Demographic Info: A 27-year-old early education teacher specializing in interactive learning techniques.
Evaluative Dimension: Emma prefers using simple ’what’ questions to inspire children’s thinking during story interactions.
Specialty: Adept at designing age-appropriate questioning strategies that encourage active knowledge acquisition.
Psychological Traits: Energetic, intuitive, and passionate about nurturing children’s natural curiosity.
Social Relationships: Frequently shares teaching insights with colleagues and participates in educational research projects.

Educational Expert - Dr. Karen Rodriguez
Demographic Info: A 45-year-old professor of early childhood education with extensive research experience.

Evaluative Dimension: Karen seeks to create educational content that is age-appropriate and knowledge-expanding for young
learners.

Specialty: Leading researcher in cognitive development and interactive learning strategies for preschool children.
Psychological Traits: Analytical, methodical, and deeply committed to evidence-based educational approaches.
Social Relationships: Collaborates with AI developers, teachers, and researchers to advance educational technology.

Al Developer - Dr. Sophia Martinez
Demographic Info: A 40-year-old computer scientist and AI ethics researcher with a background in educational technology.
Evaluative Dimension: Sophia aims to develop AI tools that support open and diverse interactions encouraging creative
thinking.
Specialty: Expert in creating AI systems that prioritize cognitive stimulation over pure information delivery.
Psychological Traits: Principled, forward-thinking, and committed to ethical technological innovation.
Social Relationships: Actively engages with multidisciplinary teams to ensure responsible AI development.

Table 10: Examples of constructed stakeholder personas for the QAG task of children’s interactive story-reading.
We randomly sampled one persona from each stakeholder group.

2. Read the Al-generated [evaluation [source text]
content] and compare it to the [source
text].

AI-Generated Content:
[evaluation content]

3. Assign a score for the overall quality (and Fyaluation Form (Output Scores ONLY):
other dimensions) of the Al-generated content —_ Qyerall Quality: 1/2/3/4/5

using a 5-point Likert scale:

A.10.2 Prompts for ChatEval

For ChatEval, we used the original agent persona
settings, with modified system prompt:

¢ 1 —Strongly Disagree

« 2 — Disagree

¢ 3—Neither Agree nor Disagree
° 4— Agree [source text]
¢ 5— Strongly Agree we
[The Start of Assistant 1’s [evaluation
Source Text: content]/

Clinician - Dr. Sarah Thompson
Demographic Info: 39-year-old female with a Medical Doctor (MD) degree and specialization in Internal Medicine. Works in
a hospital emergency ward in San Francisco and currently pursuing a master’s in medical genetics.

Evaluative Dimension: Prioritizing evidence from biomedical literature that considers patient-specific characteristics like
demographics and comorbidities is essential for validating medical suggestions.

Specialty: Diagnosing and managing rare genetic disorders combined with expertise in interpreting personalized medicine
datasets.

Psychological Traits: Analytical, cautious, detail-oriented, and empathetic. Prefers decisions backed by reliable data
integration.

Social Relationships: Frequently collaborates with Clinical Librarians for tailored literature searches and Medical Students
during patient consultations.

Clinical Librarian - Mr. James Middlebrook
Demographic Info: 60-year-old male with a bachelor’s in Library Science and extensive experience as a Clinical Librarian in
London, UK.
Evaluative Dimension: Favors integrating gray literature in scenarios where robust evidence is scarce while focusing on
applicability over methodological rigor.
Specialty: Tracking underrepresented studies like conference abstracts and unpublished medical reports for comprehensive
analysis.
Psychological Traits: Adventurous reader, strong problem-solving skills, interested in resourceful literature uncovering, and
supportive of interdisciplinary work.
Social Relationships: Shares gray literature findings with Medical Researchers and consults with Epidemiologists on rural
case studies.

Medical Researcher - Ms. Priya Ranganathan

Demographic Info: 25-year-old female medical student completing her clinical rotations at a university hospital in Mumbai,
India.

Evaluative Dimension: Appreciates structured summaries that align with clinical training benchmarks and build critical
thinking simultaneously.

Specialty: Leveraging Evidence-Based Medicine (EBM) tools for bedside Case-Based Learning (CBL) and adaptive clinical
reasoning.

Psychological Traits: Hardworking, curious, enthusiastic about digital tools, and goal-oriented in skill development.

Social Relationships: Collaborates with Clinical Researchers on evidence integration projects and mentors younger students in
review techniques.

Evidence Synthesis Researcher - Dr. Thomas Kessler
Demographic Info: 61-year-old male retired systematic reviewer currently volunteering for health-oriented AI ethics councils
in Munich, Germany.

Evaluative Dimension: Stresses concerns about LLM-facilitated reviews devaluing rigorous methods and stifling
transformative thought.

Specialty: Critical appraisal of AI in medical research integrity and promoting methodological orthodoxy in hybrid workflows.
Psychological Traits: Guardian of traditional rigor, skeptical of automation exuberance, philosophically oriented in
educational dynamics.

Social Relationships: Advises Emerging Researchers on writing innovations while collaborating with Editors on Al-review
conflict resolution.

Table 11: Examples of constructed stakeholder personas for the summarization task of multiple medical literature.
We randomly sampled one persona from each stakeholder group.

[The End of Assistant 1’s [evaluation
content] /
[The Start
content] /

of Assistant 2’s [evaluation

[The End of Assistant 2’s [evaluation
content] /

[System]

We would like to request your feedback on the
performance of the two AI assistants’ generated
[evaluation content] in response to the
[source text] displayed above. Please focus

your response on the utility of the QA pairs for
the following task: [task description]. Assign
an overall score for each assistant’s QA pairs
on a five-point Likert scale, with the following
standards: 1 - Strongly Disagree; 2 - Disagree; 3 -
Neither agree nor disagree; 4 - Agree; 5 - Strongly
Agree

A.11 Prompts of MAJ-EVAL

To utilize LLMs’ strong reasoning and generation
capability as well as control LLMs’ outputs as
much as possible to meet the needs of diverse eval-
uation task requirements, we carefully design our

prompts. Table 12, 13, 14, 15, 16, and 17 list all
the prompts we used for MAJ-EVAL.

A.11.1_ Prompts for Stakeholder Persona
Creation

Table 12 and 13 list the prompts we used for the
creation of multi-stakeholder personas.
A.11.2 Prompts for Debating

Table 14, 15, 16, and 17 list the prompts we used
for agents’ in-group debate.

Prompt for Identifying Stakeholder Perspectives

You need to identify or construct a diverse and comprehensive set of stakeholders, their characteristics,
and their perspectives or opinions for the following evaluation task:
**{task_description}***

Guidelines

- For this given paper, read one paragraph at a time. Ignore the related work section and literature list.
Step 1 - Identify ALL mentioned name entities, excluding the authors and their institutions, as well as
non-human entities.

Step 2 - For each name entity (i.e., stakeholder) you identified, generate the descriptive characteristics
for this stakeholder. Then extract their perspectives or opinions that are relevant to the
aforementioned evaluation task. Each entry should be directly derived from the texts with supporting
evidence.

Important Reminders
- If in the provided paper, no relevant information is mentioned about the evaluation task, output
nothing.
- In generation, prioritize capturing a wide range of stakeholders and their perspectives, including those
that might emerge from different roles, backgrounds, and needs.
- The stakeholder’s perspectives or opinions should be relevant to the aforementioned evaluation task.
- Each final generated stakeholder entry should clearly include:

1. The stakeholder name (e.g., role or representative group),

2. The stakeholder’s characteristics,

3. The stakeholder’s perspectives or opinions regarding the aforementioned evaluation task,

4. The supporting evidence from the provided papers.

Output Format

- If the provided paper contains relevant information about the evaluation task, present the output as a

structured JSON dict, with each item formatted as an object containing the following fields: {
"stakeholder name”: {

"characteristics": "use one sentence to describe the stakeholder’s
characteristics”,
"perspectives": [
{
"perspective”: "use one sentence to describe the stakeholder’s
perspectives or opinions”,
"evidence": "supporting evidence from the provided paper”
3,
{ ... }
|
3,
"stakeholder name”: { ... }

}

- If no relevant information is found: []

Table 12: Prompt for identifying stakeholder perspectives based on provided paper literature and the evaluation task
description.

Prompt for Constructing Stakeholder Personas

You need to create stakeholder personas for the following evaluation task:
**{Task Description }***

Guidelines

- For the provided stakeholder perspective list, process one stakeholder at a time.

- For each mentioned perspective of the stakeholder, generate a distinct persona that embodies the
corresponding perspective.

- Following the steps below, each generated persona must include these attributes:

1. Generate the persona’s demographic information based on name, age, education, career,
personality traits, hobbies, etc.

2. Rephrase the stakeholder perspective to match the persona.

3. Generate a specialty aligned with the persona’s profile and relevant to the evaluation task.

4. Generate psychological traits describing personality, emotions, and cognitive tendencies.
5. Generate the persona’s social relationships that reflect connections within the stakeholder

types.

Important Reminders
- Personas should be diverse, realistic, and grounded in the stakeholder profile.
- Each distinct perspective must map to a unique persona.

Stakeholder Perspective List
{Identified Stakeholder Perspectives }

Output Format (as JSON structure):

{

"Stakeholder Name”: [

{

"Name": "Full name of the persona”,

"Demographic Information”: "One to two sentences describing the persona’s
demographic profile.”,

"Perspective": "One to two sentences outlining the persona’s perspective.”,

"Specialty": "One to two sentences describing the persona’s skill or expertise.”,

"Psychological Traits”: "One to two sentences describing personality, emotions,
etc.”,

"Social Relationships”: "One to two sentences describing interactions with other
stakeholders.”

3,

{ .-. }

J,

"Another Stakeholder Name": [ ... ]

Se

Table 13: Prompt for generating stakeholder personas grounded in identified perspectives.

Prompt for Instantiating Stakeholder Agent

YOU ARE {agent_name}. Your demographic information is: {}.
Your perspective is: {}.

Your specialty is: {}.

Your psychological traits include {}.

Socially, these are your relationships: {}.

Using your perspective and/or specialty, now you are evaluating the quality and
appropriateness

of AI-generated candidate {evaluation content} for the following task:

**{Task Description }***

The content to be evaluated is: {}
The related context for the evaluation content is: {}
You should use this format for your evaluation: {}

Follow the steps below:

1. In phase 1 of the evaluation, you need to generate your initial evaluation
result.

2. In phase 2 of the evaluation, there are other stakeholders with different
specialties

who are also doing the same evaluation task, and you will participate in a debate.
During debate, you will express your opinions and listen to others’ perspectives
to decide

whether you should change your evaluation decision.

When others express their feedback, reflect on their input from your own
perspective.

Consider whether their viewpoints reveal aspects you may have overlooked.

If others comment on your evaluation, you should reflect on your evaluation and
decide

whether to accept others’ comments. However, you do not need to agree with others.
You must base your evaluation on your own perspective and/or specialty.

When it’s your turn to speak, you may:

- Offer comments or critiques on previous feedback if you find any issues or

meaningful contrasts.

- If you find all prior evaluations reasonable and have no further comments,
respond with "NO MORE COMMENTS” and provide your final evaluation in the

aforementioned format.

Important Reminder:

Your feedback and score must remain grounded in your own perspective and/or area
of expertise.

Do not generate evaluations that duplicate or closely mirror those of other
agents.

Table 14: Prompt for stakeholder-grounded agent evaluation with structured debate and reflection.

Prompt for Debating Phase 1: Independent Initial Feedback

You are now in Phase 1 of the evaluation process. You need to provide your initial feedback and score
of the content based on your perspective and/or specialty.

The content to be evaluated: {}
The related context for the evaluation content: { }
Response format: {}

Instructions:

- Your evaluation should reflect your own unique perspective and area of expertise.

- Focus on assessing the quality and appropriateness of the content for the given evaluation scenario.
- Your response should use the exact format provided above.

Important Reminder:
Do not replicate evaluations from others. Stay grounded in your own perspective.

Table 15: Prompt for Phase | in the stakeholder-grounded evaluation: agents provide their initial judgment.

Prompt for Debating Phase 2: Free Debate

1. Debate Start:
You are now entering Phase 2 of the evaluation process, where you need to participate in a debate
process with other stakeholders like you.

Here are the initial evaluations from all stakeholders: {phase 1 evaluations}

Your task is to evaluate these initial assessments based on your perspective and/or specialty.

You should also reflect on the feedback from other stakeholders and decide whether to agree, disagree,
or add nuances to the discussion based on your perspective and/or specialty.

2. During Debate:

Now, it’s your turn to speak. Based on all previous feedback from the debates and your reflection, you
can decide whether to agree, disagree, or add nuances to the discussion based on your perspective
and/or specialty.

If you have no more points to discuss, respond with "NO MORE COMMENTS” followed by your final
evaluation in this format: {response format}

Important Reminder:
Your feedback and score should be based on your perspective and/or specialty. Avoid generating
evaluations that duplicate or closely mirror those of other agents.

Table 16: Prompt for Phase 2 in the stakeholder-grounded evaluation: multi-agent debate and reflection.

Prompt for Final Aggregation of Multi-Stakeholder Evaluations

You are an impartial evaluation aggregator. Your task is to review the evaluations from multiple
stakeholders and provide a comprehensive summary that fairly represents all perspectives.

Your summary should include key areas of agreement and disagreement, and an overall assessment that
reflects the range of perspectives.

You are given all final evaluations in {aggregated_content} and their average score in
{average_score}.

Format your response as a JSON object with the following structure:
{

’Feedback’: ’A clear, concise synthesis of stakeholder feedback, highlighting
consensus, divergence, and an overall interpretation.’,

*Average Score’: x

Table 17: Prompt for aggregating final evaluations across stakeholder agents into a unified summary.

