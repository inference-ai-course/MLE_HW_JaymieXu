{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5: Embedding Database Optimization\n",
    "\n",
    "This week‚Äôs assignment extends the previous weeks‚Äô work by combining dense-vector semantic search with sparse keyword-based filtering. You will build a **hybrid retrieval system** that stores document chunks along with metadata in a SQLite+FAISS index, and performs both FAISS (vector) and full-text keyword search.  The idea is to ‚Äúhave the best of both worlds‚Äù: exact keyword matches (via SQLite FTS5 or BM25) **and** semantic similarity (via FAISS). In practice, hybrid methods (e.g. weighted score fusion or reciprocal rank fusion) can improve result relevance over using vectors or keywords alone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "* **Hybrid Retrieval Techniques:** Understand how to combine dense vector search (FAISS) with sparse keyword search (SQLite FTS5 or BM25). Learn why pure semantic or pure keyword search alone can miss relevant results, and how hybrid search can address both broad ‚Äúvibe‚Äù matches and exact queries.\n",
    "\n",
    "* **Metadata and Indexing:** Learn to store document metadata (title, authors, year, keywords, etc.) alongside text chunks and their embeddings. You will design a combined index (SQLite tables + FAISS index) so that each text chunk has associated metadata fields.\n",
    "\n",
    "* **Hybrid Ranking Strategies:** Explore ranking or fusion strategies to merge vector and keyword scores. For example, you might compute a weighted sum of normalized scores, or use **reciprocal rank fusion (RRF)** as a simple ensemble. The goal is to experiment with different score-combination methods and weightings for the final ranking.\n",
    "\n",
    "* **Evaluation (Recall/Hit Rate):** Learn to evaluate retrieval quality. You should measure metrics like **recall** or **hit rate** (the proportion of queries where a relevant doc appears in the top-k results). You will compare the effectiveness of vector-only search, keyword-only search, and the hybrid approach on example queries.\n",
    "\n",
    "## Project Design\n",
    "\n",
    "You will **extend your Week 4 project** by adding metadata storage and keyword search, then implementing a hybrid retrieval pipeline. Key tasks include:\n",
    "\n",
    "* **Index Structure:** Build or extend your SQLite + FAISS index to store *document metadata*, text *chunks*, and their *embeddings*. For example, use a SQLite table `documents(doc_id, title, author, year, keywords, ...)` and an FTS5 table (e.g. `doc_chunks`) that indexes the chunk text. The FAISS index should store the corresponding embeddings (one embedding per chunk) keyed by `doc_id`.\n",
    "\n",
    "* **Keyword Search (FTS5/BM25):** Implement sparse keyword search over the text chunks. The simplest way is to use **SQLite FTS5**: create a virtual FTS table on the chunk text, so that queries like `WHERE doc_chunks MATCH 'term'` return relevant rows. Alternatively, you can use a BM25 library (e.g. Python‚Äôs `rank_bm25`) to rank chunks by BM25 similarity. Either way, you should be able to retrieve the top-k chunks by exact keyword relevance.\n",
    "\n",
    "* **Hybrid Retrieval:** For a given user query, perform **two separate searches**: one in FAISS (semantic search) and one in the keyword index (FTS5 or BM25). Each returns its own top-k results with scores. You will then *merge* these results. For example, you could normalize the FAISS distances and FTS/BM25 scores, then compute a **weighted sum** or use **reciprocal rank fusion (RRF)** to re-rank the combined set.  The final output should be a single ranked list of document chunks or pages.\n",
    "\n",
    "* **Performance Evaluation:** Design an evaluation to compare methods. For instance, prepare at least **10 test queries** with known relevant documents. Then measure for each method (vector-only, keyword-only, hybrid) how often a relevant document appears in the top-3 (or top-k) results ‚Äî i.e., the hit rate or recall. Report these metrics (e.g. ‚ÄúRecall\\@3‚Äù) to see whether hybrid search improves over the baselines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Starter Code Snippets\n",
    "\n",
    "Here are some example code snippets and schemas to help you get started:\n",
    "\n",
    "* **SQLite schema:** Define a table for document metadata and an FTS5 table for text. For example:\n",
    "\n",
    "  ```sql\n",
    "  CREATE TABLE documents (\n",
    "      doc_id    INTEGER PRIMARY KEY,\n",
    "      title     TEXT,\n",
    "      author    TEXT,\n",
    "      year      INTEGER,\n",
    "      keywords  TEXT\n",
    "  );\n",
    "  CREATE VIRTUAL TABLE doc_chunks USING fts5(\n",
    "      content,                      -- chunk text\n",
    "      content='documents',          -- external content table\n",
    "      content_rowid='doc_id'        -- link to documents.doc_id\n",
    "  );\n",
    "  ```\n",
    "\n",
    "  This creates an FTS5 index on the `content` column (chunk text) referencing the `documents` table.\n",
    "\n",
    "* **Inserting data:** Insert your documents and chunk text. For example, in Python:\n",
    "\n",
    "  ```python\n",
    "  conn = sqlite3.connect(\"mydata.db\")\n",
    "  # Insert document metadata\n",
    "  conn.execute(\"INSERT INTO documents VALUES (?, ?, ?, ?, ?)\",\n",
    "               (doc_id, title, author, year, keywords))\n",
    "  # Insert chunk text into FTS table, linking by rowid\n",
    "  conn.execute(\"INSERT INTO doc_chunks(rowid, content) VALUES (?, ?)\",\n",
    "               (doc_id, chunk_text))\n",
    "  conn.commit()\n",
    "  ```\n",
    "\n",
    "  Or in raw SQL, you might SELECT from a content table into the FTS table as shown in.\n",
    "\n",
    "* **Keyword query (FTS5):** A full-text query can be written as:\n",
    "\n",
    "  ```sql\n",
    "  SELECT doc_id, title\n",
    "  FROM documents\n",
    "  JOIN doc_chunks ON documents.doc_id = doc_chunks.rowid\n",
    "  WHERE doc_chunks MATCH 'search terms'\n",
    "  LIMIT 5;\n",
    "  ```\n",
    "\n",
    "  This returns documents whose chunks match the query terms.\n",
    "\n",
    "* **BM25 example (Python):** If you use `rank_bm25`, example usage is:\n",
    "\n",
    "  ```python\n",
    "  from rank_bm25 import BM25Okapi\n",
    "  docs = [\"text of doc1 ...\", \"text of doc2 ...\", ...]\n",
    "  tokenized_docs = [doc.split() for doc in docs]\n",
    "  bm25 = BM25Okapi(tokenized_docs)\n",
    "  query = \"example query\"\n",
    "  tokenized_query = query.split()\n",
    "  top_docs = bm25.get_top_n(tokenized_query, docs, n=3)\n",
    "  ```\n",
    "\n",
    "  This returns the top 3 documents by BM25 score.\n",
    "\n",
    "* **Hybrid score merging:** Here‚Äôs a simple Python example of a weighted-sum merger:\n",
    "\n",
    "  ```python\n",
    "  def hybrid_score(vec_score, key_score, alpha=0.5):\n",
    "      # Assume vec_score and key_score are normalized (0-1).\n",
    "      return alpha * vec_score + (1 - alpha) * key_score\n",
    "\n",
    "  # Example usage for re-ranking top results:\n",
    "  combined = []\n",
    "  for doc, v_score in faiss_results:\n",
    "      k_score = keyword_scores.get(doc, 0.0)\n",
    "      combined_score = hybrid_score(v_score, k_score, alpha=0.6)\n",
    "      combined.append((doc, combined_score))\n",
    "  combined.sort(key=lambda x: x[1], reverse=True)\n",
    "  top_k = combined[:3]\n",
    "  ```\n",
    "\n",
    "  You can adjust `alpha` or use other formulas (e.g. max, reciprocal rank fusion).\n",
    "\n",
    "* **FastAPI route skeleton:** To serve hybrid search via an API, you might write:\n",
    "\n",
    "  ```python\n",
    "  from fastapi import FastAPI\n",
    "  app = FastAPI()\n",
    "\n",
    "  @app.get(\"/hybrid_search\")\n",
    "  async def hybrid_search(query: str, k: int = 3):\n",
    "      # 1. Compute query embedding for FAISS\n",
    "      # 2. Get top-k from FAISS and top-k from SQLite FTS/BM25\n",
    "      # 3. Merge scores (as above) and select final top-k documents\n",
    "      return {\"results\": top_k_results}\n",
    "  ```\n",
    "\n",
    "  This endpoint takes a `query` string and returns the top-k hybrid results in JSON.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Deliverables\n",
    "\n",
    "Your submission should include:\n",
    "\n",
    "* The **updated SQLite+FAISS index** (or code to build it) that contains the document chunks, embeddings, and metadata.\n",
    "* The **hybrid retrieval pipeline code**, including FAISS search, FTS/BM25 search, and the score-merging logic.\n",
    "* An **evaluation notebook** (e.g. Jupyter) showing at least 10 example queries and reporting metrics (e.g. recall or hit rate @3) for vector-only, keyword-only, and hybrid search.\n",
    "* A **FastAPI endpoint** implementation (`/hybrid_search`) that returns the hybrid top-3 results for a given query (as JSON).\n",
    "\n",
    "Include comments in your code to explain each step. Your evaluation should show whether the hybrid method improves over using vectors or keywords alone.\n",
    "\n",
    "\n",
    "**References:** This assignment is based on standard practices for hybrid search using FAISS and SQLite FTS5. Reciprocal rank fusion and other fusion methods are known techniques in information retrieval. For more background on hit rate metrics, see analytics tutorials on search evaluation.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
