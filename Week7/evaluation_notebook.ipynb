{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation: Base vs Fine-tuned Qwen2.5-3B\n",
    "This notebook compares the performance of the base Qwen2.5-3B-Instruct model with the fine-tuned version on academic Q&A tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Test Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gc\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load test questions\n",
    "with open('test_evaluation.json', 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(test_data)} test questions\")\n",
    "for i, item in enumerate(test_data[:3], 1):\n",
    "    print(f\"Q{i}: {item['question'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Base Model (Qwen2.5-3B-Instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "base_model_name = \"unsloth/Qwen2.5-3B-Instruct\"\n",
    "print(f\"Loading base model: {base_model_name}\")\n",
    "\n",
    "base_model, base_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    base_model_name,\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(base_model)\n",
    "\n",
    "print(\"Base model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test base model on all questions\n",
    "system_prompt = \"You are a helpful academic Q&A assistant specialized in scholarly content.\"\n",
    "base_answers = []\n",
    "\n",
    "print(\"Testing base model...\")\n",
    "for i, item in enumerate(test_data, 1):\n",
    "    question = item['question']\n",
    "    \n",
    "    # Format prompt\n",
    "    prompt = f\"<|system|>{system_prompt}<|user|>{question}<|assistant|>\"\n",
    "    \n",
    "    # Tokenize and generate\n",
    "    inputs = base_tokenizer(prompt, return_tensors=\"pt\").to(base_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = base_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            do_sample=False,  # Deterministic for fair comparison\n",
    "            temperature=0.1,\n",
    "            pad_token_id=base_tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode answer\n",
    "    full_response = base_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer = full_response.split('<|assistant|>')[-1].strip()\n",
    "    \n",
    "    base_answers.append(answer)\n",
    "    print(f\"Q{i} completed\")\n",
    "\n",
    "print(f\"Base model testing completed. Generated {len(base_answers)} answers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display base model answers\n",
    "print(\"=== BASE MODEL ANSWERS ===\")\n",
    "for i, (item, answer) in enumerate(zip(test_data, base_answers), 1):\n",
    "    print(f\"\\nQ{i}: {item['question']}\")\n",
    "    print(f\"Base Answer: {answer}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up base model from memory\n",
    "del base_model\n",
    "del base_tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"Base model cleared from memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Fine-tuned Model (Qwen2.5-3B-qlora-finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned model\n",
    "ft_model_path = \"Qwen2.5-3B-qlora-finetuned\"\n",
    "print(f\"Loading fine-tuned model: {ft_model_path}\")\n",
    "\n",
    "try:\n",
    "    ft_model, ft_tokenizer = FastLanguageModel.from_pretrained(\n",
    "        ft_model_path,\n",
    "        max_seq_length=2048,\n",
    "        dtype=None,\n",
    "        load_in_4bit=True\n",
    "    )\n",
    "    \n",
    "    # Enable inference mode\n",
    "    FastLanguageModel.for_inference(ft_model)\n",
    "    \n",
    "    print(\"Fine-tuned model loaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading fine-tuned model: {e}\")\n",
    "    print(\"Make sure the fine-tuned model directory exists and contains the model files.\")\n",
    "    ft_model = None\n",
    "    ft_tokenizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test fine-tuned model on all questions\n",
    "ft_answers = []\n",
    "\n",
    "if ft_model is not None:\n",
    "    print(\"Testing fine-tuned model...\")\n",
    "    for i, item in enumerate(test_data, 1):\n",
    "        question = item['question']\n",
    "        \n",
    "        # Format prompt (same as base model)\n",
    "        prompt = f\"<|system|>{system_prompt}<|user|>{question}<|assistant|>\"\n",
    "        \n",
    "        # Tokenize and generate\n",
    "        inputs = ft_tokenizer(prompt, return_tensors=\"pt\").to(ft_model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = ft_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=150,\n",
    "                do_sample=False,  # Deterministic for fair comparison\n",
    "                temperature=0.1,\n",
    "                pad_token_id=ft_tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode answer\n",
    "        full_response = ft_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        answer = full_response.split('<|assistant|>')[-1].strip()\n",
    "        \n",
    "        ft_answers.append(answer)\n",
    "        print(f\"Q{i} completed\")\n",
    "    \n",
    "    print(f\"Fine-tuned model testing completed. Generated {len(ft_answers)} answers.\")\n",
    "else:\n",
    "    print(\"Skipping fine-tuned model testing due to loading error.\")\n",
    "    ft_answers = [\"Model not available\"] * len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display fine-tuned model answers\n",
    "if ft_model is not None:\n",
    "    print(\"=== FINE-TUNED MODEL ANSWERS ===\")\n",
    "    for i, (item, answer) in enumerate(zip(test_data, ft_answers), 1):\n",
    "        print(f\"\\nQ{i}: {item['question']}\")\n",
    "        print(f\"Fine-tuned Answer: {answer}\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up fine-tuned model from memory\n",
    "if ft_model is not None:\n",
    "    del ft_model\n",
    "    del ft_tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"Fine-tuned model cleared from memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare answers and analyze results\n",
    "print(\"=== MODEL COMPARISON ANALYSIS ===\")\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for i, (item, base_ans, ft_ans) in enumerate(zip(test_data, base_answers, ft_answers), 1):\n",
    "    question = item['question']\n",
    "    expected = item['answer']\n",
    "    \n",
    "    print(f\"\\nüîç QUESTION {i}:\")\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"\\nüìö EXPECTED ANSWER:\")\n",
    "    print(f\"   {expected}\")\n",
    "    print(f\"\\nü§ñ BASE MODEL:\")\n",
    "    print(f\"   {base_ans}\")\n",
    "    print(f\"\\nüéØ FINE-TUNED MODEL:\")\n",
    "    print(f\"   {ft_ans}\")\n",
    "    \n",
    "    # Simple evaluation (you can make this more sophisticated)\n",
    "    base_length = len(base_ans.split())\n",
    "    ft_length = len(ft_ans.split())\n",
    "    \n",
    "    # Check if answers contain key terms from expected answer\n",
    "    expected_words = set(expected.lower().split())\n",
    "    base_words = set(base_ans.lower().split())\n",
    "    ft_words = set(ft_ans.lower().split())\n",
    "    \n",
    "    base_overlap = len(expected_words.intersection(base_words)) / max(len(expected_words), 1)\n",
    "    ft_overlap = len(expected_words.intersection(ft_words)) / max(len(expected_words), 1)\n",
    "    \n",
    "    # Determine which is better\n",
    "    if ft_ans == \"Model not available\":\n",
    "        better_model = \"N/A - Fine-tuned model not loaded\"\n",
    "        improvement = \"N/A\"\n",
    "    elif ft_overlap > base_overlap:\n",
    "        better_model = \"Fine-tuned\"\n",
    "        improvement = \"‚úÖ Better keyword overlap with expected answer\"\n",
    "    elif base_overlap > ft_overlap:\n",
    "        better_model = \"Base\"\n",
    "        improvement = \"‚ùå Base model had better keyword overlap\"\n",
    "    else:\n",
    "        better_model = \"Similar\"\n",
    "        improvement = \"‚ûñ Similar performance\"\n",
    "    \n",
    "    print(f\"\\nüìä ANALYSIS:\")\n",
    "    print(f\"   Base overlap: {base_overlap:.2f}, Fine-tuned overlap: {ft_overlap:.2f}\")\n",
    "    print(f\"   Better model: {better_model}\")\n",
    "    print(f\"   Assessment: {improvement}\")\n",
    "    \n",
    "    # Store results\n",
    "    comparison_results.append({\n",
    "        \"question_id\": i,\n",
    "        \"question\": question,\n",
    "        \"expected_answer\": expected,\n",
    "        \"base_answer\": base_ans,\n",
    "        \"finetuned_answer\": ft_ans,\n",
    "        \"base_word_overlap\": round(base_overlap, 3),\n",
    "        \"finetuned_word_overlap\": round(ft_overlap, 3),\n",
    "        \"better_model\": better_model,\n",
    "        \"improvement_note\": improvement\n",
    "    })\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 100)\n",
    "\n",
    "print(f\"\\nüìà SUMMARY:\")\n",
    "if ft_answers[0] != \"Model not available\":\n",
    "    ft_better = sum(1 for r in comparison_results if r['better_model'] == 'Fine-tuned')\n",
    "    base_better = sum(1 for r in comparison_results if r['better_model'] == 'Base')\n",
    "    similar = sum(1 for r in comparison_results if r['better_model'] == 'Similar')\n",
    "    \n",
    "    print(f\"Fine-tuned better: {ft_better}/{len(comparison_results)}\")\n",
    "    print(f\"Base better: {base_better}/{len(comparison_results)}\")\n",
    "    print(f\"Similar: {similar}/{len(comparison_results)}\")\n",
    "else:\n",
    "    print(\"Fine-tuned model was not available for comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Results to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation results to JSON file\n",
    "eval_results = {\n",
    "    \"evaluation_metadata\": {\n",
    "        \"base_model\": \"unsloth/Qwen2.5-3B-Instruct\",\n",
    "        \"finetuned_model\": \"Qwen2.5-3B-qlora-finetuned\",\n",
    "        \"num_questions\": len(test_data),\n",
    "        \"evaluation_date\": \"2024\"\n",
    "    },\n",
    "    \"results\": comparison_results\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "with open('eval_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(eval_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úÖ Evaluation results saved to 'eval_results.json'\")\n",
    "print(f\"üìÅ Results contain {len(comparison_results)} question comparisons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook compared the base Qwen2.5-3B-Instruct model with the fine-tuned version on academic Q&A tasks. The evaluation focused on:\n",
    "\n",
    "1. **Loading and testing both models** on 10 academic questions\n",
    "2. **Memory management** by properly unloading models after testing\n",
    "3. **Comparative analysis** using keyword overlap with expected answers\n",
    "4. **Structured output** saved to JSON for further analysis\n",
    "\n",
    "The fine-tuning process aimed to improve the model's ability to provide more accurate and relevant answers to academic questions by training on domain-specific Q&A pairs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}