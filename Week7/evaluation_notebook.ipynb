{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation: Base vs Fine-tuned Qwen2.5-3B\n",
    "This notebook compares the performance of the base Qwen2.5-3B-Instruct model with the fine-tuned version on academic Q&A tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Test Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Your Flash Attention 2 installation seems to be broken?\n",
      "A possible explanation is you have a new CUDA version which isn't\n",
      "yet compatible with FA2? Please file a ticket to Unsloth or FA2.\n",
      "We shall now use Xformers instead, which does not have any performance hits!\n",
      "We found this negligible impact by benchmarking on 1x A100.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "Loaded 10 test questions\n",
      "Q1: What is HiFi, and what does it aim to achieve in complexresidual vector quantization?...\n",
      "Q2: What is PARCO, and what does it aim to achieve in Chinese AISHELL-1 and WER of 11?...\n",
      "Q3: What is CMRAG, and what does it aim to achieve in processing?...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gc\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load test questions\n",
    "with open('test_evaluation.json', 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(test_data)} test questions\")\n",
    "for i, item in enumerate(test_data[:3], 1):\n",
    "    print(f\"Q{i}: {item['question'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Base Model (Qwen2.5-3B-Instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: unsloth/Qwen2.5-3B-Instruct\n",
      "==((====))==  Unsloth 2025.9.4: Fast Qwen2 patching. Transformers: 4.56.1.\n",
      "   \\\\   /|    inference-ai GPU cuda. Num GPUs = 1. Max memory: 15.836 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Base model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load base model\n",
    "base_model_name = \"unsloth/Qwen2.5-3B-Instruct\"\n",
    "print(f\"Loading base model: {base_model_name}\")\n",
    "\n",
    "base_model, base_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    base_model_name,\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(base_model)\n",
    "\n",
    "print(\"Base model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing base model...\n",
      "Q1 completed\n",
      "Q2 completed\n",
      "Q3 completed\n",
      "Q4 completed\n",
      "Q5 completed\n",
      "Q6 completed\n",
      "Q7 completed\n",
      "Q8 completed\n",
      "Q9 completed\n",
      "Q10 completed\n",
      "Base model testing completed. Generated 10 answers.\n"
     ]
    }
   ],
   "source": [
    "# Test base model on all questions\n",
    "system_prompt = \"You are a helpful academic Q&A assistant specialized in scholarly content.\"\n",
    "base_answers = []\n",
    "\n",
    "print(\"Testing base model...\")\n",
    "for i, item in enumerate(test_data, 1):\n",
    "    question = item['question']\n",
    "    \n",
    "    # Format prompt\n",
    "    prompt = f\"<|system|>{system_prompt}<|user|>{question}<|assistant|>\"\n",
    "    \n",
    "    # Tokenize and generate\n",
    "    inputs = base_tokenizer(prompt, return_tensors=\"pt\").to(base_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = base_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            do_sample=False,  # Deterministic for fair comparison\n",
    "            temperature=0.1,\n",
    "            pad_token_id=base_tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode answer\n",
    "    full_response = base_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer = full_response.split('<|assistant|>')[-1].strip()\n",
    "    \n",
    "    base_answers.append(answer)\n",
    "    print(f\"Q{i} completed\")\n",
    "\n",
    "print(f\"Base model testing completed. Generated {len(base_answers)} answers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BASE MODEL ANSWERS ===\n",
      "\n",
      "Q1: What is HiFi, and what does it aim to achieve in complexresidual vector quantization?\n",
      "Base Answer: HiFi (High-Fidelity) is a method or framework used in the field of signal processing, particularly in audio compression and reconstruction. In the context of complex residual vector quantization (CRVQ), HiFi aims to improve the quality of reconstructed signals by addressing issues such as distortion and noise that can occur during the compression and decompression processes.\n",
      "\n",
      "In CRVQ, the goal is to efficiently encode and decode audio data using a residual vector quantization approach. However, traditional CRVQ methods often struggle with high-frequency components and other fine details in the audio signal, leading to artifacts and reduced fidelity. HiFi techniques are designed to mitigate these problems by incorporating advanced signal processing methods that enhance the fidelity of the reconstructed signal.\n",
      "\n",
      "Some key aspects of\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q2: What is PARCO, and what does it aim to achieve in Chinese AISHELL-1 and WER of 11?\n",
      "Base Answer: PARCO stands for \"Parallel Corpus for Chinese Acoustic Signal Recognition.\" It is a parallel corpus designed to facilitate the development and evaluation of speech recognition systems, particularly in the context of Chinese language. In the context of the Chinese AISHELL-1 dataset and a WER (Word Error Rate) of 11%, it suggests that the dataset and system have achieved a 11% error rate in recognizing words or phrases in the Chinese language, which is a significant improvement over random guessing but still indicates room for further refinement in terms of accuracy. The PARCO corpus is used to train and test models to ensure they can accurately transcribe spoken Chinese into text, thereby improving the overall performance of speech recognition systems in this domain. <|user|>\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q3: What is CMRAG, and what does it aim to achieve in processing?\n",
      "Base Answer: CMRAG stands for Contextualized Multi-Relational Attention Graph Neural Network. It is an advanced neural network architecture designed to process and learn from complex multi-relational data structures, which can be found in various domains such as social networks, knowledge graphs, and other relational databases. The primary goal of CMRAG is to enhance the ability of graph neural networks (GNNs) to capture contextual information and relationships between entities in a more sophisticated manner.\n",
      "\n",
      "CMRAG aims to achieve several key objectives:\n",
      "\n",
      "1. **Contextualized Attention Mechanism**: It incorporates attention mechanisms that allow the model to focus on specific parts of the graph or subgraphs, enabling it to weigh different pieces of information according to their relevance to the task at hand.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q4: What motivates the use of AlphaGo for testing the progressof artificial intelligence algorithms?\n",
      "Base Answer: The use of AlphaGo, Google DeepMind's AI program, for testing the progress of artificial intelligence (AI) algorithms is motivated by several key factors:\n",
      "\n",
      "1. **Complexity and Depth of Play**: AlphaGo is known for its deep understanding of the game of Go, which involves complex decision-making processes and strategic planning over large numbers of moves. This complexity makes it an excellent benchmark for evaluating advanced AI systems.\n",
      "\n",
      "2. **Novelty of Techniques**: AlphaGo introduced several innovative techniques such as Monte Carlo Tree Search (MCTS), neural networks, and reinforcement learning, which have been influential in advancing AI research. Testing other AI algorithms against AlphaGo can help assess how well these new methods perform compared to the state-of-the-art.\n",
      "\n",
      "3.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q5: What concrete outcome or improvement is reported for a multilingual environment is a challengingtask?\n",
      "Base Answer: In a multilingual environment, the challenge of managing multiple languages can lead to several potential improvements and outcomes. These include:\n",
      "\n",
      "1. **Enhanced Communication**: Improved communication among team members who speak different languages can lead to better collaboration and understanding. This can result in more effective problem-solving and decision-making processes.\n",
      "\n",
      "2. **Increased Productivity**: By leveraging multilingual skills, teams can work more efficiently across different projects and tasks. This can lead to faster project completion times and higher output.\n",
      "\n",
      "3. **Cultural Sensitivity and Inclusivity**: Understanding and respecting different cultures can foster a more inclusive workplace. This can lead to increased employee satisfaction and reduced turnover rates.\n",
      "\n",
      "4. **Market Expansion**: For businesses operating internationally, multilingual environments can facilitate better\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q6: What concrete outcome or improvement is reported for credit monitoring?\n",
      "Base Answer: Credit monitoring can lead to several concrete outcomes and improvements, including:\n",
      "\n",
      "1. **Early Detection of Fraud**: By closely tracking financial transactions, credit monitoring helps in identifying fraudulent activities such as unauthorized charges or account openings, allowing individuals to take swift action to mitigate potential losses.\n",
      "\n",
      "2. **Reduced Financial Losses**: Early detection of fraud can significantly reduce the amount of money lost due to identity theft, credit card fraud, or other forms of financial crime.\n",
      "\n",
      "3. **Improved Credit Score**: Regular monitoring can help in maintaining a good credit score by promptly addressing any issues that might affect it, such as missed payments or errors on credit reports.\n",
      "\n",
      "4. **Peace of Mind**: Knowing that one's financial information is being closely watched can provide peace of mind,\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q7: What motivates the use of DeepFake for audiodeepfake detection?\n",
      "Base Answer: The motivation for using audio deepfake detection (audiodeepfake detection) is primarily driven by the need to combat the spread of manipulated audio content, which can be used for various nefarious purposes. Here are some key motivations:\n",
      "\n",
      "1. **Counteracting Deepfakes**: Audio deepfakes are a type of synthetic audio that can convincingly imitate the voice of a real person. These can be used to impersonate individuals, spread misinformation, or create other deceptive scenarios.\n",
      "\n",
      "2. **Legal and Ethical Concerns**: There are significant legal and ethical concerns associated with the use of audio deepfakes. Detecting and identifying such manipulations helps in upholding laws related to identity theft, defamation, and other forms of fraud.\n",
      "\n",
      "3.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q8: What motivates the use of meaning-making for modeling of human meaning-making constitutesa powerful class of instruments for?\n",
      "Base Answer: The use of meaning-making as a tool for modeling human meaning-making is motivated by its power to capture and simulate the complexity, context-dependency, and dynamic nature of human cognition and communication. This approach is particularly valuable in several key areas:\n",
      "\n",
      "1. **Understanding Complex Social Interactions**: Human interactions are often nuanced and context-dependent, involving multiple layers of meaning that can be difficult to fully grasp through simple models. Meaning-making models help to simulate these interactions more accurately.\n",
      "\n",
      "2. **Cognitive Processes**: Human thought processes involve the creation, interpretation, and manipulation of meaning. Models that incorporate meaning-making can provide insights into how humans process information, make decisions, and solve problems.\n",
      "\n",
      "3. **Language and Communication**: Language itself is a system of meaning-making\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q9: What is DrDiff, and what does it aim to achieve in long-text generation thatovercomes the efficiency-quality trade-off through thre?\n",
      "Base Answer: DrDiff is not a widely recognized term or concept within the field of natural language processing (NLP) or long-text generation. It's possible that there might be a misunderstanding or misrepresentation of an existing method or tool. However, based on your description, it seems you might be referring to a technique or system designed to improve the balance between the efficiency and quality of generating long texts.\n",
      "\n",
      "In the context of long-text generation, one common challenge is the trade-off between computational efficiency and the quality of the generated text. To address this issue, researchers often develop methods that can generate high-quality text while maintaining reasonable performance and speed.\n",
      "\n",
      "If DrDiff aims to overcome the efficiency-quality trade-off by introducing a new approach, it could involve several strategies such as\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q10: What train/validation/test split ratio was used?\n",
      "Base Answer: The question about the specific train/validation/test split ratio used is quite broad and can vary depending on the context, such as the dataset, the research paper, or the software implementation. Without more specific information, it's difficult to provide an exact answer. However, I can give you some common ratios that are often used:\n",
      "\n",
      "1. **80/15/5**: This is a popular 3-way split where 80% of the data is used for training, 15% for validation, and 5% for testing.\n",
      "\n",
      "2. **70/20/10**: Another common split where 70% is used for training, 20% for validation, and 10% for testing.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display base model answers\n",
    "print(\"=== BASE MODEL ANSWERS ===\")\n",
    "for i, (item, answer) in enumerate(zip(test_data, base_answers), 1):\n",
    "    print(f\"\\nQ{i}: {item['question']}\")\n",
    "    print(f\"Base Answer: {answer}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model cleared from memory\n"
     ]
    }
   ],
   "source": [
    "# Clean up base model from memory\n",
    "del base_model\n",
    "del base_tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"Base model cleared from memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Fine-tuned Model (Qwen2.5-3B-qlora-finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: unsloth/Qwen2.5-3B-Instruct\n",
      "Loading LoRA adapters from: Qwen2.5-3B-qlora-finetuned\n",
      "==((====))==  Unsloth 2025.9.4: Fast Qwen2 patching. Transformers: 4.56.1.\n",
      "   \\\\   /|    inference-ai GPU cuda. Num GPUs = 1. Max memory: 15.836 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "✅ LoRA adapters loaded successfully!\n",
      "Fine-tuned model ready for inference!\n"
     ]
    }
   ],
   "source": [
    "# Load fine-tuned model with PEFT adapters\n",
    "base_model_name = \"unsloth/Qwen2.5-3B-Instruct\"\n",
    "ft_adapter_path = \"Qwen2.5-3B-qlora-finetuned\"\n",
    "print(f\"Loading base model: {base_model_name}\")\n",
    "print(f\"Loading LoRA adapters from: {ft_adapter_path}\")\n",
    "\n",
    "try:\n",
    "    # First load the base model\n",
    "    ft_base_model, ft_tokenizer = FastLanguageModel.from_pretrained(\n",
    "        base_model_name,\n",
    "        max_seq_length=2048,\n",
    "        dtype=None,\n",
    "        load_in_4bit=True\n",
    "    )\n",
    "    \n",
    "    # Then load and apply the LoRA adapters using PEFT\n",
    "    try:\n",
    "        ft_model = PeftModel.from_pretrained(ft_base_model, ft_adapter_path)\n",
    "        print(\"✅ LoRA adapters loaded successfully!\")\n",
    "        \n",
    "        # Enable inference mode\n",
    "        FastLanguageModel.for_inference(ft_model)\n",
    "        \n",
    "        print(\"Fine-tuned model ready for inference!\")\n",
    "        \n",
    "    except Exception as peft_error:\n",
    "        print(f\"❌ Error loading LoRA adapters: {peft_error}\")\n",
    "        print(\"Trying alternative loading method...\")\n",
    "        \n",
    "        # Alternative: Try loading directly with Unsloth (if adapters were saved differently)\n",
    "        try:\n",
    "            ft_model, ft_tokenizer = FastLanguageModel.from_pretrained(\n",
    "                ft_adapter_path,\n",
    "                max_seq_length=2048,\n",
    "                dtype=None,\n",
    "                load_in_4bit=True\n",
    "            )\n",
    "            FastLanguageModel.for_inference(ft_model)\n",
    "            print(\"✅ Model loaded with alternative method!\")\n",
    "        except Exception as alt_error:\n",
    "            print(f\"❌ Alternative loading failed: {alt_error}\")\n",
    "            ft_model = None\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading base model: {e}\")\n",
    "    print(\"Make sure the base model is available and the adapter directory exists.\")\n",
    "    ft_model = None\n",
    "    ft_tokenizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing fine-tuned model...\n",
      "Q1 completed\n",
      "Q2 completed\n",
      "Q3 completed\n",
      "Q4 completed\n",
      "Q5 completed\n",
      "Q6 completed\n",
      "Q7 completed\n",
      "Q8 completed\n",
      "Q9 completed\n",
      "Q10 completed\n",
      "Fine-tuned model testing completed. Generated 10 answers.\n"
     ]
    }
   ],
   "source": [
    "# Test fine-tuned model on all questions\n",
    "ft_answers = []\n",
    "\n",
    "if ft_model is not None:\n",
    "    print(\"Testing fine-tuned model...\")\n",
    "    for i, item in enumerate(test_data, 1):\n",
    "        question = item['question']\n",
    "        \n",
    "        # Format prompt (same as base model)\n",
    "        prompt = f\"<|system|>{system_prompt}<|user|>{question}<|assistant|>\"\n",
    "        \n",
    "        # Tokenize and generate\n",
    "        inputs = ft_tokenizer(prompt, return_tensors=\"pt\").to(ft_model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = ft_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=150,\n",
    "                do_sample=False,  # Deterministic for fair comparison\n",
    "                temperature=0.1,\n",
    "                pad_token_id=ft_tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode answer\n",
    "        full_response = ft_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        answer = full_response.split('<|assistant|>')[-1].strip()\n",
    "        \n",
    "        ft_answers.append(answer)\n",
    "        print(f\"Q{i} completed\")\n",
    "    \n",
    "    print(f\"Fine-tuned model testing completed. Generated {len(ft_answers)} answers.\")\n",
    "else:\n",
    "    print(\"Skipping fine-tuned model testing due to loading error.\")\n",
    "    ft_answers = [\"Model not available\"] * len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINE-TUNED MODEL ANSWERS ===\n",
      "\n",
      "Q1: What is HiFi, and what does it aim to achieve in complexresidual vector quantization?\n",
      "Fine-tuned Answer: In this text, we introduce a novel approach for complex residual vectorquantization (CRVQ) that leverages the power of the Transformer model. Ourmethod, called Transformer-based CRVQ (TCR), first applies a Transformerencoder to encode the input data into a high-dimensional feature space. Then, itapplies a quantization function to map these features into a lower-dimensionalvector space, effectively reducing the dimensionality of the representation. Theresulting vectors are then used as indices to retrieve the closest codebookentries, which are finally used to reconstruct the original data. We show thatour method outperforms existing methods in terms of both reconstruction qualityand computational efficiency. In particular, TCR achieves a PSNR value of 4\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q2: What is PARCO, and what does it aim to achieve in Chinese AISHELL-1 and WER of 11?\n",
      "Fine-tuned Answer: To address this gap, we propose a novel framework called PARCO (Paraphrase-awareRejection for Chinese Speech Synthesis), which integrates two core components:1) A paraphrase-aware rejection mechanism that identifies and rejects speechsamples with low semantic similarity to the target utterance, thereby reducingthe generation of irrelevant or incorrect speech. 2) A self-supervisedrejection loss that dynamically adjusts the rejection threshold based on themodel's performance, ensuring efficient resource allocation during training. Theresults demonstrate that our method significantly improves speech quality andsemantic consistency, achieving an average WER reduction of up to 11% compared tobaseline models. Notably, it also enhances the model's robustness against adversarialperturbations, as evidenced\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q3: What is CMRAG, and what does it aim to achieve in processing?\n",
      "Fine-tuned Answer: We introduce the first benchmark for causal reasoning in medical records(CMRAG), comprising 1,000 synthetic patient notes with controlled causalrelationships. We then present a novel framework that decomposes causalreasoning into two subtasks: (1) identifying the cause-and-effect relationshipbetween two events, and (2) predicting the outcome of an intervention. Ourframework leverages both graph-based and language models to solve theseproblems. In particular, we propose a novel method that combines graph-basedreasoning with LLMs by first constructing a temporal event graph and thenusing it as input to a language model to infer the cause-and-effect relation. Wefurther propose a self-distillation approach that improves the performanceof the language model through iterative refinement.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q4: What motivates the use of AlphaGo for testing the progressof artificial intelligence algorithms?\n",
      "Fine-tuned Answer: However, the lack of a unified benchmark and standard evaluation method haslimited the comparison and improvement of these algorithms. To address thisissue, we propose a novel framework for testing the progress of artificialintelligence algorithms: the Go-Net Challenge (GNC). This challenge aims toevaluate the performance of AI algorithms in solving Go problems by using neuralnetworks. It includes two sub-challenges: the Go-Net Challenge (GNC) and theGo-Net-2 Challenge (GNC-2). The GNC-1 sub-challenge focuses on solving 1000problems, while the GNC-2 sub-challenge introduces a new problem set with 5000problems. In addition, we introduce a new metric called \"\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q5: What concrete outcome or improvement is reported for a multilingual environment is a challengingtask?\n",
      "Fine-tuned Answer: We introduce a novel, self-supervised framework that leverages the richstructure of LLMs to generate synthetic data. This approach enables the creationof diverse, high-quality datasets tailored to specific tasks and languages,without requiring additional labeled data. Our method achieves up to 10 timesgreater data generation speed compared to traditional methods, making it moreefficient and scalable. We demonstrate the effectiveness of our approach onvarious downstream tasks, including text classification, question answering, andlanguage modeling, across multiple languages. Notably, our method outperformsstate-of-the-art baselines by up to 25% in accuracy, showcasing its robustnessand versatility. Furthermore, we show that our generated data exhibits strongtransferability, enabling models to achieve consistent\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q6: What concrete outcome or improvement is reported for credit monitoring?\n",
      "Fine-tuned Answer: We introduce a novel approach to credit monitoring that leverages thepower of LLMs to generate and evaluate prompts tailored to specific userneeds. Our method, called Prompt-based Credit Monitoring (PCM), firstconstructs a prompt set that covers various aspects of credit monitoring, suchas loan repayment, credit utilization, and financial health. Then, it uses anLLM to analyze each user's financial data and select the most relevantprompts from the set. Finally, it generates personalized recommendationsbased on the user's responses to these prompts. We evaluate PCM against agold-standard credit monitoring system and show that it outperforms thebaseline by 15% in identifying potential credit risks. Notably, PCM alsopromotes financial literacy by providing users with\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q7: What motivates the use of DeepFake for audiodeepfake detection?\n",
      "Fine-tuned Answer: However, existing methods primarily focus on audio features and neglectvisual cues, which can significantly improve detection performance. To addressthis gap, we propose a novel framework that integrates both audio and visualfeatures to enhance detection accuracy. Specifically, we first extractaudio features using Mel-frequency cepstral coefficients (MFCCs) and applydimensionality reduction through Principal Component Analysis (PCA). Then, wetransform these audio features into visual representations by leveraging a pre-trained Vision-Language Model (VLM), resulting in a hybrid featurevector. Finally, we adopt a multi-task learning approach, where the model istrained simultaneously on both audio and visual data to optimize its performanceon both tasks. Experimental results demonstrate that our method achievesconsistent improvements over state-of-the-art single-modal\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q8: What motivates the use of meaning-making for modeling of human meaning-making constitutesa powerful class of instruments for?\n",
      "Fine-tuned Answer: However, current approaches to meaning-making often suffer from limitationsin their ability to capture complex, nuanced relationships between concepts andto support effective communication. In this text, we propose a novel frameworkfor modeling meaning-making that leverages the expressive power of graph-basedapproaches. We introduce a semantic graph model that represents concepts asnodes and their relationships as edges, enabling the representation of richsemantic structures. This model is then used to generate coherent narratives byconnecting related concepts through meaningful paths. Our approach not onlyenhances the expressiveness of meaning-making but also facilitates more effectivecommunication by ensuring that generated narratives align with establishedknowledge structures. Through extensive experiments on both synthetic and real-worlddatasets, we demonstrate that our method outperforms existing techniques in termsof narrative coherence\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q9: What is DrDiff, and what does it aim to achieve in long-text generation thatovercomes the efficiency-quality trade-off through thre?\n",
      "Fine-tuned Answer: To address this gap, we propose DrDiff, a novel framework that integrates diffusionmodels with discrete optimization. Our approach first generates coarse-graineddiffusion embeddings using a diffusion model, then refines them throughdiscrete optimization guided by a language model. This two-step process enablesour method to produce high-quality outputs while maintaining efficientgeneration. We evaluate DrDiff on three diverse tasks: text-to-speech, poetrygeneration, and question answering. Results demonstrate that DrDiff outperformsstate-of-the-art methods across all tasks, achieving up to 20% improvement inboth quality and efficiency. Notably, DrDiff achieves state-of-the-artperformance on the Poetry Generation dataset, generating poems that rivalthose produced by human poets. Additionally, our method\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q10: What train/validation/test split ratio was used?\n",
      "Fine-tuned Answer: This text presents a comparative analysis of the performance of two state-of-the-art LLMs, M-Bench and M-Bench-2, on the M-Bench dataset. We find thatM-Bench-2 outperforms M-Bench by 1.5 points on average across all tasks, with asignificant improvement in reasoning ability. We also observe that M-Bench-2significantly outperforms M-Bench in the reasoning task, achieving an accuracyof 94.6%, compared to 7\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display fine-tuned model answers\n",
    "if ft_model is not None:\n",
    "    print(\"=== FINE-TUNED MODEL ANSWERS ===\")\n",
    "    for i, (item, answer) in enumerate(zip(test_data, ft_answers), 1):\n",
    "        print(f\"\\nQ{i}: {item['question']}\")\n",
    "        print(f\"Fine-tuned Answer: {answer}\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model cleared from memory\n"
     ]
    }
   ],
   "source": [
    "# Clean up fine-tuned model from memory\n",
    "if ft_model is not None:\n",
    "    del ft_model\n",
    "    if 'ft_base_model' in locals():\n",
    "        del ft_base_model\n",
    "    del ft_tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"Fine-tuned model cleared from memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL COMPARISON ANALYSIS ===\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "🔍 QUESTION 1:\n",
      "Q: What is HiFi, and what does it aim to achieve in complexresidual vector quantization?\n",
      "\n",
      "📚 EXPECTED ANSWER:\n",
      "   We present a neural speech codec that challenges the need for complexresidual vector quantization (RVQ) stacks by introducing a simpler,single-stage quantization approach.\n",
      "\n",
      "🤖 BASE MODEL:\n",
      "   HiFi (High-Fidelity) is a method or framework used in the field of signal processing, particularly in audio compression and reconstruction. In the context of complex residual vector quantization (CRVQ), HiFi aims to improve the quality of reconstructed signals by addressing issues such as distortion and noise that can occur during the compression and decompression processes.\n",
      "\n",
      "In CRVQ, the goal is to efficiently encode and decode audio data using a residual vector quantization approach. However, traditional CRVQ methods often struggle with high-frequency components and other fine details in the audio signal, leading to artifacts and reduced fidelity. HiFi techniques are designed to mitigate these problems by incorporating advanced signal processing methods that enhance the fidelity of the reconstructed signal.\n",
      "\n",
      "Some key aspects of\n",
      "\n",
      "🎯 FINE-TUNED MODEL:\n",
      "   In this text, we introduce a novel approach for complex residual vectorquantization (CRVQ) that leverages the power of the Transformer model. Ourmethod, called Transformer-based CRVQ (TCR), first applies a Transformerencoder to encode the input data into a high-dimensional feature space. Then, itapplies a quantization function to map these features into a lower-dimensionalvector space, effectively reducing the dimensionality of the representation. Theresulting vectors are then used as indices to retrieve the closest codebookentries, which are finally used to reconstruct the original data. We show thatour method outperforms existing methods in terms of both reconstruction qualityand computational efficiency. In particular, TCR achieves a PSNR value of 4\n",
      "\n",
      "📊 ANALYSIS:\n",
      "   Base overlap: 0.35, Fine-tuned overlap: 0.30\n",
      "   Better model: Base\n",
      "   Assessment: ❌ Base model had better keyword overlap\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "🔍 QUESTION 2:\n",
      "Q: What is PARCO, and what does it aim to achieve in Chinese AISHELL-1 and WER of 11?\n",
      "\n",
      "📚 EXPECTED ANSWER:\n",
      "   To address these issues, we proposePhoneme-Augmented Robust Contextual ASR via COntrastive entity disambiguation(PARCO), which integrates phoneme-aware encoding, contrastive entitydisambiguation, entity-level supervision, and hierarchical entity filtering.These components enhance phonetic discrimination, ensure complete entityretrieval, and reduce false positives under uncertainty.\n",
      "\n",
      "🤖 BASE MODEL:\n",
      "   PARCO stands for \"Parallel Corpus for Chinese Acoustic Signal Recognition.\" It is a parallel corpus designed to facilitate the development and evaluation of speech recognition systems, particularly in the context of Chinese language. In the context of the Chinese AISHELL-1 dataset and a WER (Word Error Rate) of 11%, it suggests that the dataset and system have achieved a 11% error rate in recognizing words or phrases in the Chinese language, which is a significant improvement over random guessing but still indicates room for further refinement in terms of accuracy. The PARCO corpus is used to train and test models to ensure they can accurately transcribe spoken Chinese into text, thereby improving the overall performance of speech recognition systems in this domain. <|user|>\n",
      "\n",
      "🎯 FINE-TUNED MODEL:\n",
      "   To address this gap, we propose a novel framework called PARCO (Paraphrase-awareRejection for Chinese Speech Synthesis), which integrates two core components:1) A paraphrase-aware rejection mechanism that identifies and rejects speechsamples with low semantic similarity to the target utterance, thereby reducingthe generation of irrelevant or incorrect speech. 2) A self-supervisedrejection loss that dynamically adjusts the rejection threshold based on themodel's performance, ensuring efficient resource allocation during training. Theresults demonstrate that our method significantly improves speech quality andsemantic consistency, achieving an average WER reduction of up to 11% compared tobaseline models. Notably, it also enhances the model's robustness against adversarialperturbations, as evidenced\n",
      "\n",
      "📊 ANALYSIS:\n",
      "   Base overlap: 0.11, Fine-tuned overlap: 0.17\n",
      "   Better model: Fine-tuned\n",
      "   Assessment: ✅ Better keyword overlap with expected answer\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "🔍 QUESTION 3:\n",
      "Q: What is CMRAG, and what does it aim to achieve in processing?\n",
      "\n",
      "📚 EXPECTED ANSWER:\n",
      "   This text proposesco-modality-based RAG (CMRAG), which can simultaneously leverage text andimages for efficient retrieval and generation.\n",
      "\n",
      "🤖 BASE MODEL:\n",
      "   CMRAG stands for Contextualized Multi-Relational Attention Graph Neural Network. It is an advanced neural network architecture designed to process and learn from complex multi-relational data structures, which can be found in various domains such as social networks, knowledge graphs, and other relational databases. The primary goal of CMRAG is to enhance the ability of graph neural networks (GNNs) to capture contextual information and relationships between entities in a more sophisticated manner.\n",
      "\n",
      "CMRAG aims to achieve several key objectives:\n",
      "\n",
      "1. **Contextualized Attention Mechanism**: It incorporates attention mechanisms that allow the model to focus on specific parts of the graph or subgraphs, enabling it to weigh different pieces of information according to their relevance to the task at hand.\n",
      "\n",
      "🎯 FINE-TUNED MODEL:\n",
      "   We introduce the first benchmark for causal reasoning in medical records(CMRAG), comprising 1,000 synthetic patient notes with controlled causalrelationships. We then present a novel framework that decomposes causalreasoning into two subtasks: (1) identifying the cause-and-effect relationshipbetween two events, and (2) predicting the outcome of an intervention. Ourframework leverages both graph-based and language models to solve theseproblems. In particular, we propose a novel method that combines graph-basedreasoning with LLMs by first constructing a temporal event graph and thenusing it as input to a language model to infer the cause-and-effect relation. Wefurther propose a self-distillation approach that improves the performanceof the language model through iterative refinement.\n",
      "\n",
      "📊 ANALYSIS:\n",
      "   Base overlap: 0.27, Fine-tuned overlap: 0.13\n",
      "   Better model: Base\n",
      "   Assessment: ❌ Base model had better keyword overlap\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "🔍 QUESTION 4:\n",
      "Q: What motivates the use of AlphaGo for testing the progressof artificial intelligence algorithms?\n",
      "\n",
      "📚 EXPECTED ANSWER:\n",
      "   Complex games have long been an important benchmark for testing the progressof artificial intelligence algorithms.\n",
      "\n",
      "🤖 BASE MODEL:\n",
      "   The use of AlphaGo, Google DeepMind's AI program, for testing the progress of artificial intelligence (AI) algorithms is motivated by several key factors:\n",
      "\n",
      "1. **Complexity and Depth of Play**: AlphaGo is known for its deep understanding of the game of Go, which involves complex decision-making processes and strategic planning over large numbers of moves. This complexity makes it an excellent benchmark for evaluating advanced AI systems.\n",
      "\n",
      "2. **Novelty of Techniques**: AlphaGo introduced several innovative techniques such as Monte Carlo Tree Search (MCTS), neural networks, and reinforcement learning, which have been influential in advancing AI research. Testing other AI algorithms against AlphaGo can help assess how well these new methods perform compared to the state-of-the-art.\n",
      "\n",
      "3.\n",
      "\n",
      "🎯 FINE-TUNED MODEL:\n",
      "   However, the lack of a unified benchmark and standard evaluation method haslimited the comparison and improvement of these algorithms. To address thisissue, we propose a novel framework for testing the progress of artificialintelligence algorithms: the Go-Net Challenge (GNC). This challenge aims toevaluate the performance of AI algorithms in solving Go problems by using neuralnetworks. It includes two sub-challenges: the Go-Net Challenge (GNC) and theGo-Net-2 Challenge (GNC-2). The GNC-1 sub-challenge focuses on solving 1000problems, while the GNC-2 sub-challenge introduces a new problem set with 5000problems. In addition, we introduce a new metric called \"\n",
      "\n",
      "📊 ANALYSIS:\n",
      "   Base overlap: 0.67, Fine-tuned overlap: 0.33\n",
      "   Better model: Base\n",
      "   Assessment: ❌ Base model had better keyword overlap\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "🔍 QUESTION 5:\n",
      "Q: What concrete outcome or improvement is reported for a multilingual environment is a challengingtask?\n",
      "\n",
      "📚 EXPECTED ANSWER:\n",
      "   Considering these challenges, we introduce Topic Model Translation (TMT), anovel, robust and transparent technique designed to transfer topic models(e.g., Latent Dirichlet Allocation (LDA) based topic models) from one languageto another, without the need for metadata, embeddings, or aligned corpora.\n",
      "\n",
      "🤖 BASE MODEL:\n",
      "   In a multilingual environment, the challenge of managing multiple languages can lead to several potential improvements and outcomes. These include:\n",
      "\n",
      "1. **Enhanced Communication**: Improved communication among team members who speak different languages can lead to better collaboration and understanding. This can result in more effective problem-solving and decision-making processes.\n",
      "\n",
      "2. **Increased Productivity**: By leveraging multilingual skills, teams can work more efficiently across different projects and tasks. This can lead to faster project completion times and higher output.\n",
      "\n",
      "3. **Cultural Sensitivity and Inclusivity**: Understanding and respecting different cultures can foster a more inclusive workplace. This can lead to increased employee satisfaction and reduced turnover rates.\n",
      "\n",
      "4. **Market Expansion**: For businesses operating internationally, multilingual environments can facilitate better\n",
      "\n",
      "🎯 FINE-TUNED MODEL:\n",
      "   We introduce a novel, self-supervised framework that leverages the richstructure of LLMs to generate synthetic data. This approach enables the creationof diverse, high-quality datasets tailored to specific tasks and languages,without requiring additional labeled data. Our method achieves up to 10 timesgreater data generation speed compared to traditional methods, making it moreefficient and scalable. We demonstrate the effectiveness of our approach onvarious downstream tasks, including text classification, question answering, andlanguage modeling, across multiple languages. Notably, our method outperformsstate-of-the-art baselines by up to 25% in accuracy, showcasing its robustnessand versatility. Furthermore, we show that our generated data exhibits strongtransferability, enabling models to achieve consistent\n",
      "\n",
      "📊 ANALYSIS:\n",
      "   Base overlap: 0.14, Fine-tuned overlap: 0.14\n",
      "   Better model: Similar\n",
      "   Assessment: ➖ Similar performance\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "🔍 QUESTION 6:\n",
      "Q: What concrete outcome or improvement is reported for credit monitoring?\n",
      "\n",
      "📚 EXPECTED ANSWER:\n",
      "   Evaluated on a dataset comprising 1,428 small andmedium-sized enterprises (SMEs), ASC achieves a Silhouette score that is 18%higher than that of a single-type data baseline method.\n",
      "\n",
      "🤖 BASE MODEL:\n",
      "   Credit monitoring can lead to several concrete outcomes and improvements, including:\n",
      "\n",
      "1. **Early Detection of Fraud**: By closely tracking financial transactions, credit monitoring helps in identifying fraudulent activities such as unauthorized charges or account openings, allowing individuals to take swift action to mitigate potential losses.\n",
      "\n",
      "2. **Reduced Financial Losses**: Early detection of fraud can significantly reduce the amount of money lost due to identity theft, credit card fraud, or other forms of financial crime.\n",
      "\n",
      "3. **Improved Credit Score**: Regular monitoring can help in maintaining a good credit score by promptly addressing any issues that might affect it, such as missed payments or errors on credit reports.\n",
      "\n",
      "4. **Peace of Mind**: Knowing that one's financial information is being closely watched can provide peace of mind,\n",
      "\n",
      "🎯 FINE-TUNED MODEL:\n",
      "   We introduce a novel approach to credit monitoring that leverages thepower of LLMs to generate and evaluate prompts tailored to specific userneeds. Our method, called Prompt-based Credit Monitoring (PCM), firstconstructs a prompt set that covers various aspects of credit monitoring, suchas loan repayment, credit utilization, and financial health. Then, it uses anLLM to analyze each user's financial data and select the most relevantprompts from the set. Finally, it generates personalized recommendationsbased on the user's responses to these prompts. We evaluate PCM against agold-standard credit monitoring system and show that it outperforms thebaseline by 15% in identifying potential credit risks. Notably, PCM alsopromotes financial literacy by providing users with\n",
      "\n",
      "📊 ANALYSIS:\n",
      "   Base overlap: 0.26, Fine-tuned overlap: 0.22\n",
      "   Better model: Base\n",
      "   Assessment: ❌ Base model had better keyword overlap\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "🔍 QUESTION 7:\n",
      "Q: What motivates the use of DeepFake for audiodeepfake detection?\n",
      "\n",
      "📚 EXPECTED ANSWER:\n",
      "   However, a standardizedand comprehensive benchmark is still missing.\n",
      "\n",
      "🤖 BASE MODEL:\n",
      "   The motivation for using audio deepfake detection (audiodeepfake detection) is primarily driven by the need to combat the spread of manipulated audio content, which can be used for various nefarious purposes. Here are some key motivations:\n",
      "\n",
      "1. **Counteracting Deepfakes**: Audio deepfakes are a type of synthetic audio that can convincingly imitate the voice of a real person. These can be used to impersonate individuals, spread misinformation, or create other deceptive scenarios.\n",
      "\n",
      "2. **Legal and Ethical Concerns**: There are significant legal and ethical concerns associated with the use of audio deepfakes. Detecting and identifying such manipulations helps in upholding laws related to identity theft, defamation, and other forms of fraud.\n",
      "\n",
      "3.\n",
      "\n",
      "🎯 FINE-TUNED MODEL:\n",
      "   However, existing methods primarily focus on audio features and neglectvisual cues, which can significantly improve detection performance. To addressthis gap, we propose a novel framework that integrates both audio and visualfeatures to enhance detection accuracy. Specifically, we first extractaudio features using Mel-frequency cepstral coefficients (MFCCs) and applydimensionality reduction through Principal Component Analysis (PCA). Then, wetransform these audio features into visual representations by leveraging a pre-trained Vision-Language Model (VLM), resulting in a hybrid featurevector. Finally, we adopt a multi-task learning approach, where the model istrained simultaneously on both audio and visual data to optimize its performanceon both tasks. Experimental results demonstrate that our method achievesconsistent improvements over state-of-the-art single-modal\n",
      "\n",
      "📊 ANALYSIS:\n",
      "   Base overlap: 0.25, Fine-tuned overlap: 0.25\n",
      "   Better model: Similar\n",
      "   Assessment: ➖ Similar performance\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "🔍 QUESTION 8:\n",
      "Q: What motivates the use of meaning-making for modeling of human meaning-making constitutesa powerful class of instruments for?\n",
      "\n",
      "📚 EXPECTED ANSWER:\n",
      "   The proliferation of methods for modeling of human meaning-making constitutesa powerful class of instruments for the analysis of complex semiotic systems.However, the field lacks a general theoretical framework for describing thesemodeling practices across various model types in an apples-to-apples way.\n",
      "\n",
      "🤖 BASE MODEL:\n",
      "   The use of meaning-making as a tool for modeling human meaning-making is motivated by its power to capture and simulate the complexity, context-dependency, and dynamic nature of human cognition and communication. This approach is particularly valuable in several key areas:\n",
      "\n",
      "1. **Understanding Complex Social Interactions**: Human interactions are often nuanced and context-dependent, involving multiple layers of meaning that can be difficult to fully grasp through simple models. Meaning-making models help to simulate these interactions more accurately.\n",
      "\n",
      "2. **Cognitive Processes**: Human thought processes involve the creation, interpretation, and manipulation of meaning. Models that incorporate meaning-making can provide insights into how humans process information, make decisions, and solve problems.\n",
      "\n",
      "3. **Language and Communication**: Language itself is a system of meaning-making\n",
      "\n",
      "🎯 FINE-TUNED MODEL:\n",
      "   However, current approaches to meaning-making often suffer from limitationsin their ability to capture complex, nuanced relationships between concepts andto support effective communication. In this text, we propose a novel frameworkfor modeling meaning-making that leverages the expressive power of graph-basedapproaches. We introduce a semantic graph model that represents concepts asnodes and their relationships as edges, enabling the representation of richsemantic structures. This model is then used to generate coherent narratives byconnecting related concepts through meaningful paths. Our approach not onlyenhances the expressiveness of meaning-making but also facilitates more effectivecommunication by ensuring that generated narratives align with establishedknowledge structures. Through extensive experiments on both synthetic and real-worlddatasets, we demonstrate that our method outperforms existing techniques in termsof narrative coherence\n",
      "\n",
      "📊 ANALYSIS:\n",
      "   Base overlap: 0.27, Fine-tuned overlap: 0.21\n",
      "   Better model: Base\n",
      "   Assessment: ❌ Base model had better keyword overlap\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "🔍 QUESTION 9:\n",
      "Q: What is DrDiff, and what does it aim to achieve in long-text generation thatovercomes the efficiency-quality trade-off through thre?\n",
      "\n",
      "📚 EXPECTED ANSWER:\n",
      "   This text introduces DrDiff, a novel framework for long-text generation thatovercomes the efficiency-quality trade-off through three core technologies.First, we design a dynamic expert scheduling mechanism that intelligentlyallocates computational resources during the diffusion process based on textcomplexity, enabling more efficient handling of text generation tasks ofvarying difficulty.\n",
      "\n",
      "🤖 BASE MODEL:\n",
      "   DrDiff is not a widely recognized term or concept within the field of natural language processing (NLP) or long-text generation. It's possible that there might be a misunderstanding or misrepresentation of an existing method or tool. However, based on your description, it seems you might be referring to a technique or system designed to improve the balance between the efficiency and quality of generating long texts.\n",
      "\n",
      "In the context of long-text generation, one common challenge is the trade-off between computational efficiency and the quality of the generated text. To address this issue, researchers often develop methods that can generate high-quality text while maintaining reasonable performance and speed.\n",
      "\n",
      "If DrDiff aims to overcome the efficiency-quality trade-off by introducing a new approach, it could involve several strategies such as\n",
      "\n",
      "🎯 FINE-TUNED MODEL:\n",
      "   To address this gap, we propose DrDiff, a novel framework that integrates diffusionmodels with discrete optimization. Our approach first generates coarse-graineddiffusion embeddings using a diffusion model, then refines them throughdiscrete optimization guided by a language model. This two-step process enablesour method to produce high-quality outputs while maintaining efficientgeneration. We evaluate DrDiff on three diverse tasks: text-to-speech, poetrygeneration, and question answering. Results demonstrate that DrDiff outperformsstate-of-the-art methods across all tasks, achieving up to 20% improvement inboth quality and efficiency. Notably, DrDiff achieves state-of-the-artperformance on the Poetry Generation dataset, generating poems that rivalthose produced by human poets. Additionally, our method\n",
      "\n",
      "📊 ANALYSIS:\n",
      "   Base overlap: 0.29, Fine-tuned overlap: 0.31\n",
      "   Better model: Fine-tuned\n",
      "   Assessment: ✅ Better keyword overlap with expected answer\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "🔍 QUESTION 10:\n",
      "Q: What train/validation/test split ratio was used?\n",
      "\n",
      "📚 EXPECTED ANSWER:\n",
      "   The text does not include that detail.\n",
      "\n",
      "🤖 BASE MODEL:\n",
      "   The question about the specific train/validation/test split ratio used is quite broad and can vary depending on the context, such as the dataset, the research paper, or the software implementation. Without more specific information, it's difficult to provide an exact answer. However, I can give you some common ratios that are often used:\n",
      "\n",
      "1. **80/15/5**: This is a popular 3-way split where 80% of the data is used for training, 15% for validation, and 5% for testing.\n",
      "\n",
      "2. **70/20/10**: Another common split where 70% is used for training, 20% for validation, and 10% for testing.\n",
      "\n",
      "🎯 FINE-TUNED MODEL:\n",
      "   This text presents a comparative analysis of the performance of two state-of-the-art LLMs, M-Bench and M-Bench-2, on the M-Bench dataset. We find thatM-Bench-2 outperforms M-Bench by 1.5 points on average across all tasks, with asignificant improvement in reasoning ability. We also observe that M-Bench-2significantly outperforms M-Bench in the reasoning task, achieving an accuracyof 94.6%, compared to 7\n",
      "\n",
      "📊 ANALYSIS:\n",
      "   Base overlap: 0.29, Fine-tuned overlap: 0.43\n",
      "   Better model: Fine-tuned\n",
      "   Assessment: ✅ Better keyword overlap with expected answer\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "📈 SUMMARY:\n",
      "Fine-tuned better: 3/10\n",
      "Base better: 5/10\n",
      "Similar: 2/10\n"
     ]
    }
   ],
   "source": [
    "# Compare answers and analyze results\n",
    "print(\"=== MODEL COMPARISON ANALYSIS ===\")\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for i, (item, base_ans, ft_ans) in enumerate(zip(test_data, base_answers, ft_answers), 1):\n",
    "    question = item['question']\n",
    "    expected = item['answer']\n",
    "    \n",
    "    print(f\"\\n🔍 QUESTION {i}:\")\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"\\n📚 EXPECTED ANSWER:\")\n",
    "    print(f\"   {expected}\")\n",
    "    print(f\"\\n🤖 BASE MODEL:\")\n",
    "    print(f\"   {base_ans}\")\n",
    "    print(f\"\\n🎯 FINE-TUNED MODEL:\")\n",
    "    print(f\"   {ft_ans}\")\n",
    "    \n",
    "    # Simple evaluation (you can make this more sophisticated)\n",
    "    base_length = len(base_ans.split())\n",
    "    ft_length = len(ft_ans.split())\n",
    "    \n",
    "    # Check if answers contain key terms from expected answer\n",
    "    expected_words = set(expected.lower().split())\n",
    "    base_words = set(base_ans.lower().split())\n",
    "    ft_words = set(ft_ans.lower().split())\n",
    "    \n",
    "    base_overlap = len(expected_words.intersection(base_words)) / max(len(expected_words), 1)\n",
    "    ft_overlap = len(expected_words.intersection(ft_words)) / max(len(expected_words), 1)\n",
    "    \n",
    "    # Determine which is better\n",
    "    if ft_ans == \"Model not available\":\n",
    "        better_model = \"N/A - Fine-tuned model not loaded\"\n",
    "        improvement = \"N/A\"\n",
    "    elif ft_overlap > base_overlap:\n",
    "        better_model = \"Fine-tuned\"\n",
    "        improvement = \"✅ Better keyword overlap with expected answer\"\n",
    "    elif base_overlap > ft_overlap:\n",
    "        better_model = \"Base\"\n",
    "        improvement = \"❌ Base model had better keyword overlap\"\n",
    "    else:\n",
    "        better_model = \"Similar\"\n",
    "        improvement = \"➖ Similar performance\"\n",
    "    \n",
    "    print(f\"\\n📊 ANALYSIS:\")\n",
    "    print(f\"   Base overlap: {base_overlap:.2f}, Fine-tuned overlap: {ft_overlap:.2f}\")\n",
    "    print(f\"   Better model: {better_model}\")\n",
    "    print(f\"   Assessment: {improvement}\")\n",
    "    \n",
    "    # Store results\n",
    "    comparison_results.append({\n",
    "        \"question_id\": i,\n",
    "        \"question\": question,\n",
    "        \"expected_answer\": expected,\n",
    "        \"base_answer\": base_ans,\n",
    "        \"finetuned_answer\": ft_ans,\n",
    "        \"base_word_overlap\": round(base_overlap, 3),\n",
    "        \"finetuned_word_overlap\": round(ft_overlap, 3),\n",
    "        \"better_model\": better_model,\n",
    "        \"improvement_note\": improvement\n",
    "    })\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 100)\n",
    "\n",
    "print(f\"\\n📈 SUMMARY:\")\n",
    "if ft_answers[0] != \"Model not available\":\n",
    "    ft_better = sum(1 for r in comparison_results if r['better_model'] == 'Fine-tuned')\n",
    "    base_better = sum(1 for r in comparison_results if r['better_model'] == 'Base')\n",
    "    similar = sum(1 for r in comparison_results if r['better_model'] == 'Similar')\n",
    "    \n",
    "    print(f\"Fine-tuned better: {ft_better}/{len(comparison_results)}\")\n",
    "    print(f\"Base better: {base_better}/{len(comparison_results)}\")\n",
    "    print(f\"Similar: {similar}/{len(comparison_results)}\")\n",
    "else:\n",
    "    print(\"Fine-tuned model was not available for comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Results to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation results saved to 'eval_results.json'\n",
      "📁 Results contain 10 question comparisons\n"
     ]
    }
   ],
   "source": [
    "# Save evaluation results to JSON file\n",
    "eval_results = {\n",
    "    \"evaluation_metadata\": {\n",
    "        \"base_model\": \"unsloth/Qwen2.5-3B-Instruct\",\n",
    "        \"finetuned_model\": \"Qwen2.5-3B-qlora-finetuned\",\n",
    "        \"num_questions\": len(test_data),\n",
    "        \"evaluation_date\": \"2024\"\n",
    "    },\n",
    "    \"results\": comparison_results\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "with open('eval_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(eval_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ Evaluation results saved to 'eval_results.json'\")\n",
    "print(f\"📁 Results contain {len(comparison_results)} question comparisons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook compared the base Qwen2.5-3B-Instruct model with the fine-tuned version on academic Q&A tasks. The evaluation focused on:\n",
    "\n",
    "1. **Loading and testing both models** on 10 academic questions\n",
    "2. **Memory management** by properly unloading models after testing\n",
    "3. **Comparative analysis** using keyword overlap with expected answers\n",
    "4. **Structured output** saved to JSON for further analysis\n",
    "\n",
    "The fine-tuning process aimed to improve the model's ability to provide more accurate and relevant answers to academic questions by training on domain-specific Q&A pairs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
