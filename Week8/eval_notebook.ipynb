{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 Homework - Reward Model Evaluation\n",
    "\n",
    "This notebook evaluates our trained reward model using ROUGE and BERTScore metrics on the evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaymi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "W0915 17:10:10.964000 32560 Lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from evaluate import load\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 10 examples [00:00, 1666.13 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 evaluation examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load evaluation data\n",
    "script_dir = os.path.dirname(os.path.abspath('__file__')) if '__file__' in globals() else os.getcwd()\n",
    "eval_file = os.path.join(script_dir, \"eval_data.jsonl\")\n",
    "\n",
    "# Load dataset\n",
    "eval_dataset = load_dataset(\"json\", data_files=eval_file, split=\"train\")\n",
    "print(f\"Loaded {len(eval_dataset)} evaluation examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chosen summaries: 10\n",
      "Number of rejected summaries: 10\n",
      "\n",
      "First chosen summary:\n",
      "### Summary\n",
      "\n",
      "**Problem Addressed:**\n",
      "The paper addresses the evolving role of pragmatics in the context of large language models (LLMs), particularly their communicative capabilities. It challenges the...\n",
      "\n",
      "First rejected summary:\n",
      "### Summary\n",
      "\n",
      "**Problem Addressed:**\n",
      "The paper addresses how the advent of large language models (LLMs) necessitates a reconsideration of pragmatic theory, particularly in terms of its traditional posi...\n"
     ]
    }
   ],
   "source": [
    "# Extract summaries for evaluation\n",
    "chosen_summaries = eval_dataset[\"chosen\"]\n",
    "rejected_summaries = eval_dataset[\"rejected\"]\n",
    "\n",
    "print(f\"Number of chosen summaries: {len(chosen_summaries)}\")\n",
    "print(f\"Number of rejected summaries: {len(rejected_summaries)}\")\n",
    "\n",
    "# Show first example\n",
    "print(\"\\nFirst chosen summary:\")\n",
    "print(chosen_summaries[0][:200] + \"...\")\n",
    "print(\"\\nFirst rejected summary:\")\n",
    "print(rejected_summaries[0][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 7.95kB [00:00, ?B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ROUGE and BERTScore metrics\n"
     ]
    }
   ],
   "source": [
    "# Load ROUGE and BERTScore metrics (following professor's code)\n",
    "rouge = load(\"rouge\")\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "print(\"Loaded ROUGE and BERTScore metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing ROUGE scores...\n",
      "ROUGE Results:\n",
      "rouge1: 0.6258\n",
      "rouge2: 0.3906\n",
      "rougeL: 0.5007\n",
      "rougeLsum: 0.5701\n"
     ]
    }
   ],
   "source": [
    "# Compute ROUGE scores comparing chosen vs rejected summaries\n",
    "print(\"Computing ROUGE scores...\")\n",
    "results_rouge = rouge.compute(predictions=chosen_summaries, references=rejected_summaries)\n",
    "\n",
    "print(\"ROUGE Results:\")\n",
    "for key, value in results_rouge.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing BERTScore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaymi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jaymi\\.cache\\huggingface\\hub\\models--roberta-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore Results:\n",
      "Precision: 0.9203\n",
      "Recall: 0.9205\n",
      "F1: 0.9204\n"
     ]
    }
   ],
   "source": [
    "# Compute BERTScore comparing chosen vs rejected summaries\n",
    "print(\"Computing BERTScore...\")\n",
    "results_bertscore = bertscore.compute(predictions=chosen_summaries, references=rejected_summaries, lang=\"en\")\n",
    "\n",
    "print(\"BERTScore Results:\")\n",
    "print(f\"Precision: {sum(results_bertscore['precision'])/len(results_bertscore['precision']):.4f}\")\n",
    "print(f\"Recall: {sum(results_bertscore['recall'])/len(results_bertscore['recall']):.4f}\")\n",
    "print(f\"F1: {sum(results_bertscore['f1'])/len(results_bertscore['f1']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained reward model...\n",
      "Reward model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load our trained reward model\n",
    "print(\"Loading trained reward model...\")\n",
    "model_path = \"./reward_model\"\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "reward_model.eval()\n",
    "print(\"Reward model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get reward scores\n",
    "def get_reward_score(text):\n",
    "    inputs = reward_tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = reward_model(**inputs)\n",
    "        reward_score = outputs.logits.item()\n",
    "    \n",
    "    return reward_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing reward model scores...\n",
      "Processing example 1/10\n",
      "Processing example 6/10\n",
      "Reward scoring completed\n"
     ]
    }
   ],
   "source": [
    "# Compute reward scores for chosen and rejected summaries\n",
    "print(\"Computing reward model scores...\")\n",
    "\n",
    "chosen_scores = []\n",
    "rejected_scores = []\n",
    "\n",
    "for i, (chosen, rejected) in enumerate(zip(chosen_summaries, rejected_summaries)):\n",
    "    if i % 5 == 0:  # Progress indicator\n",
    "        print(f\"Processing example {i+1}/{len(chosen_summaries)}\")\n",
    "    \n",
    "    chosen_score = get_reward_score(chosen)\n",
    "    rejected_score = get_reward_score(rejected)\n",
    "    \n",
    "    chosen_scores.append(chosen_score)\n",
    "    rejected_scores.append(rejected_score)\n",
    "\n",
    "print(\"Reward scoring completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Model Results:\n",
      "Average score for chosen summaries: 0.1990\n",
      "Average score for rejected summaries: 0.1747\n",
      "Score difference (chosen - rejected): 0.0243\n",
      "Reward model accuracy (chosen > rejected): 60.00%\n"
     ]
    }
   ],
   "source": [
    "# Analyze reward model results\n",
    "import numpy as np\n",
    "\n",
    "chosen_mean = np.mean(chosen_scores)\n",
    "rejected_mean = np.mean(rejected_scores)\n",
    "\n",
    "print(\"Reward Model Results:\")\n",
    "print(f\"Average score for chosen summaries: {chosen_mean:.4f}\")\n",
    "print(f\"Average score for rejected summaries: {rejected_mean:.4f}\")\n",
    "print(f\"Score difference (chosen - rejected): {chosen_mean - rejected_mean:.4f}\")\n",
    "\n",
    "# Count how many times chosen scored higher than rejected\n",
    "correct_preferences = sum(1 for c, r in zip(chosen_scores, rejected_scores) if c > r)\n",
    "accuracy = correct_preferences / len(chosen_scores)\n",
    "print(f\"Reward model accuracy (chosen > rejected): {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results Summary:\n",
      "             Metric  Score\n",
      "            ROUGE-1 0.6258\n",
      "            ROUGE-2 0.3906\n",
      "            ROUGE-L 0.5007\n",
      "BERTScore-Precision 0.9203\n",
      "   BERTScore-Recall 0.9205\n",
      "       BERTScore-F1 0.9204\n",
      "      Reward-Chosen 0.1990\n",
      "    Reward-Rejected 0.1747\n",
      "    Reward-Accuracy 0.6000\n"
     ]
    }
   ],
   "source": [
    "# Create summary table\n",
    "results_summary = {\n",
    "    \"Metric\": [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"BERTScore-Precision\", \"BERTScore-Recall\", \"BERTScore-F1\", \n",
    "               \"Reward-Chosen\", \"Reward-Rejected\", \"Reward-Accuracy\"],\n",
    "    \"Score\": [\n",
    "        results_rouge[\"rouge1\"],\n",
    "        results_rouge[\"rouge2\"], \n",
    "        results_rouge[\"rougeL\"],\n",
    "        sum(results_bertscore['precision'])/len(results_bertscore['precision']),\n",
    "        sum(results_bertscore['recall'])/len(results_bertscore['recall']),\n",
    "        sum(results_bertscore['f1'])/len(results_bertscore['f1']),\n",
    "        chosen_mean,\n",
    "        rejected_mean,\n",
    "        accuracy\n",
    "    ]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results_summary)\n",
    "print(\"\\nEvaluation Results Summary:\")\n",
    "print(results_df.to_string(index=False, float_format='%.4f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example Comparisons:\n",
      "================================================================================\n",
      "\n",
      "Example 1:\n",
      "Chosen Summary (Score: 0.1656):\n",
      "### Summary\n",
      "\n",
      "**Problem Addressed:**\n",
      "The paper addresses the evolving role of pragmatics in the context of large language models (LLMs), particularly their communicative capabilities. It challenges the...\n",
      "\n",
      "Rejected Summary (Score: 0.0711):\n",
      "### Summary\n",
      "\n",
      "**Problem Addressed:**\n",
      "The paper addresses how the advent of large language models (LLMs) necessitates a reconsideration of pragmatic theory, particularly in terms of its traditional posi...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "Chosen Summary (Score: 0.2702):\n",
      "### Summary\n",
      "\n",
      "**Problem Addressed:** Robust content moderation requires systems that can adapt to evolving policies without costly retraining.\n",
      "\n",
      "**Core Method:** The paper introduces the **Contextual Po...\n",
      "\n",
      "Rejected Summary (Score: 0.0722):\n",
      "**Summary:**\n",
      "\n",
      "**Problem Addressed:** Robust content moderation requires systems that can adapt to evolving policies without costly retraining.\n",
      "\n",
      "**Core Method:** The paper introduces **Classification U...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "Chosen Summary (Score: 0.0821):\n",
      "### Summary\n",
      "\n",
      "**Problem Addressed:** The paper addresses the growing threat of scam calls facilitated by large language models (LLMs), which can generate highly realistic scam dialogues and bypass curr...\n",
      "\n",
      "Rejected Summary (Score: 0.1781):\n",
      "**Summary:**\n",
      "\n",
      "**Problem Addressed:** The paper addresses the growing threat of autonomous large language model (LLM) agents that can be used to orchestrate highly persuasive scam calls, a significant ...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Show some example comparisons\n",
    "print(\"\\nExample Comparisons:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(min(3, len(chosen_summaries))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Chosen Summary (Score: {chosen_scores[i]:.4f}):\")\n",
    "    print(chosen_summaries[i][:200] + \"...\")\n",
    "    print(f\"\\nRejected Summary (Score: {rejected_scores[i]:.4f}):\")\n",
    "    print(rejected_summaries[i][:200] + \"...\")\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
