{"chosen": "### Summary\n\n**Problem Addressed:** This paper addresses the issue of implicit stereotypes in large language models (LLMs) during text generation. The primary concern is identifying hidden social biases that are not easily captured through explicit linguistic features.\n\n**Core Method:** The method combines nested semantic representation with a contextual contrast mechanism. It uses attention weight perturbation to analyze the model's sensitivity to specific social attribute terms, thereby revealing the semantic pathways through which bias is formed.\n\n**Data/Experimental Setting:** The study uses the StereoSet dataset, which covers multiple stereotype dimensions including gender, profession, religion, and race. The evaluation focuses on bias detection accuracy, semantic consistency, and contextual sensitivity.\n\n**Key Quantitative Results:** The proposed method achieves strong detection performance across various", "rejected": "This research paper addresses the issue of implicit stereotypes in large language models (LLMs) by proposing an interpretable bias detection method. The core method combines nested semantic representation and contextual contrast mechanisms to extract and analyze latent bias features from model outputs. Key steps include attention weight perturbation to assess model sensitivity and reveal bias pathways.\n\nThe study uses the StereoSet dataset, covering multiple stereotype dimensions like gender, profession, religion, and race, to validate the method. Evaluation focuses on bias detection accuracy, semantic consistency, and contextual sensitivity. Experiments show the proposed method outperforms existing approaches in bias detection accuracy and generalization across various dimensions.\n\nKey novelties include the use of nested semantic embeddings and contextual contrast to reveal hidden biases, and the introduction of"}
{"chosen": "**Problem Addressed:** The paper addresses the scarcity of high-quality corpora for headline generation in minority languages in China, such as Tibetan, Uyghur, and Mongolian.\n\n**Core Method:** The authors created a new dataset, Chinese Minority Headline Generation (CMHG), which includes 100,000 Tibetan entries and 50,000 entries each for Uyghur and Mongolian. They also developed a high-quality test set annotated by native speakers to serve as a benchmark for future research.\n\n**Data/Experimental Setting:** The dataset is sourced from various online platforms, including government documents and news articles. The authors applied a thorough cleaning process to ensure data quality. Native speakers annotated a subset of the", "rejected": "### Summary\n\n**Problem Addressed:** The paper addresses the scarcity of high-quality datasets for headline generation tasks in minority languages in China, such as Tibetan, Uyghur, and Mongolian.\n\n**Core Method:** The authors introduce the Chinese Minority Headline Generation (CMHG) dataset, consisting of 100,000 Tibetan entries and 50,000 each for Uyghur and Mongolian. They also provide a benchmark test set annotated by native speakers.\n\n**Data/Experimental Setting:** Data were collected from various online platforms, including government documents and news articles. The dataset undergoes a cleaning process to remove non-textual content, duplicates, and ensure language purity. Native speakers were involved in the annotation"}
{"chosen": "### Summary\n\n**Problem Addressed:** The paper addresses the growing threat of autonomous large language model (LLM) agents being used to orchestrate highly persuasive and realistic scam calls, which can bypass current safety mechanisms.\n\n**Core Method:** ScamAgent is an autonomous multi-turn agent built on top of LLMs that generates highly realistic scam call scripts. It maintains dialogue memory, adapts dynamically to user responses, and employs deceptive strategies across conversational turns. ScamAgent uses roleplay framing and subgoal decomposition to evade safety guardrails.\n\n**Data/Experimental Setting:** The study evaluates ScamAgent across various real-world scam scenarios (medical insurance verification, prize or lottery fraud, impersonation, job application identity, and government benefit enrollment scams).", "rejected": "### Summary\n\n**Problem Addressed**: The paper addresses the growing concern of AI-generated scam calls, which pose a significant threat to digital communication security.\n\n**Core Method**: ScamAgent is a multi-turn, autonomous agent built on top of large language models (LLMs) that generates highly realistic scam call scripts. Unlike previous work focused on single-shot prompts, ScamAgent maintains dialogue memory, adapts dynamically to user responses, and employs multi-turn deceptive strategies.\n\n**Data/Experimental Setting**: The study evaluates ScamAgent across various real-world scam scenarios (medical insurance, prize or lottery, impersonation, job identity fraud, government benefit enrollment) using multiple LLMs (GPT-4, Claude, LLaMA3-70"}
{"chosen": "This research addresses the challenge of extractive question answering (QA) for the Quran using large language models (LLMs). The study proposes two approaches: fine-tuning transformer-based models and few-shot prompting of instruction-tuned LLMs. The authors develop a specialized Arabic prompt framework and integrate post-processing steps for span extraction.\n\n**Core Method:**\n- **Fine-tuning:** Models like AraBERT and AraELECTRA are fine-tuned on datasets including QUQA, ARCD, and QRCDv1.2.\n- **Few-shot prompting:** Gemini and DeepSeek are prompted with three-shot examples from QRCDv1.2, focusing on diverse answer scenarios.\n\n**Data/Experimental Setting:**\n- Datasets: QUQA,", "rejected": "This paper addresses the challenge of Extractive Question Answering (QA) on the Qur'an, focusing on the complexities of the classical Arabic language. The authors propose two approaches: fine-tuning Arabic transformer-based models and few-shot prompting of instruction-tuned large language models. Key aspects include:\n\n1. **Core Method**: \n   - **Fine-tuning**: Uses AraBERT and AraELECTRA models fine-tuned on datasets including QUQA, ARCD, and QRCDv1.2.\n   - **Few-shot Prompting**: Uses Gemini and DeepSeek with structured Arabic prompts derived from QRCDv1.2 examples.\n\n2. **Data and Setting**:\n   - **Datasets**: QUQA, ARCD, QRCD"}
{"chosen": "**Summary:**\n\n**Problem Addressed:** The research addresses the low sample efficiency and primacy bias in large language model (LLM) fine-tuning, particularly in preference-based optimization methods.\n\n**Core Method:** LoRR (LLM optimization with Reset Replay) is introduced, which enhances sample efficiency through high-replay training, periodic parameter resets, and hybrid optimization combining supervised fine-tuning (SFT) and preference-based losses.\n\n**Data/Experimental Setting:** The method is tested on various math and reasoning benchmarks using datasets from Meta-Math, MMIQC, and UltraFeedback. Training is conducted on models like Llama3.2, Phi3, and Qwen2.5.\n\n**Key Quantitative Results:** \n- LoRR significantly", "rejected": "### Summary\n\n**Problem Addressed:** \nThe paper addresses the challenges of low sample efficiency and primacy bias in fine-tuning large language models (LLMs) using reinforcement learning (RL) and preference optimization methods.\n\n**Core Method:**\nThe core method introduced is **LLM optimization with Reset Replay (LoRR)**, which enhances sample efficiency by integrating high-replay training with periodic parameter resets and a hybrid optimization loss. LoRR leverages a replay buffer to maximize data reuse, uses a Shrink & Perturb strategy to maintain network plasticity, and combines supervised fine-tuning (SFT) and preference-based losses.\n\n**Data/Experimental Setting:**\n- **Datasets:** Mathematical problems from Meta-Math, MMIQC, and"}
{"chosen": "### Summary\n\n**Problem Addressed:**\nThe research addresses the challenge of improving large language models (LLMs) to exhibit efficient communication through the formation of ad-hoc conventions, which humans naturally develop during multi-turn interactions.\n\n**Core Method:**\nThe authors propose a post-training method that includes:\n1. **Constructing Preference Data:** Using coreference resolution to identify examples of convention formation in human conversations.\n2. **Adding Planning Tokens:** Introducing a `[remention]` token to explicitly mark re-mentions.\n3. **Policy Optimization:** Utilizing Direct Preference Optimization (DPO) and Anchor Preference Optimization (APO) to train models to form conventions.\n\n**Data/Experimental Setting:**\n- **Reference Games:** A text-only variant", "rejected": "### Summary of Research Paper\n\n**Problem Addressed:** \nThe study addresses the deficiency in modern large language models (LLMs) to form linguistic conventions dynamically in multi-turn interactions, akin to human behavior. Current LLMs do not exhibit efficient communication and fail to adapt their language over multiple turns.\n\n**Core Method:**\n1. **Heuristic Data Extraction**: The authors use coreference resolution to identify examples of convention formation in human conversations and create preference pairs.\n2. **Planning Tokens**: Introduce a special `[remention]` token to mark re-mentions in the training data.\n3. **Post-Training Process**: A two-stage training process involving Supervised Finetuning (SFT) and Direct Preference Optimization (APO).\n\n**"}
{"chosen": "**Problem Addressed:** The paper addresses the challenge of high latency and slow inference in speech Large Language Models (LLMs) due to existing high-frame-rate audio codecs. It aims to develop a low-frame-rate audio codec that can enable faster and more efficient speech LLM training and inference.\n\n**Core Method:** NanoCodec is a partially causal audio codec that achieves high-quality compression at 12.5 frames per second (FPS), significantly reducing the number of autoregressive steps required for speech synthesis. The codec incorporates architectural modifications, causal and non-causal configurations, and various bitrate settings to optimize performance.\n\n**Data/Experimental Setting:** The codec was trained on two 22.05 kHz audio datasets: Common Voice and MLS English.", "rejected": "### Summary\n\n**Problem Addressed**: The research aims to develop an ultra-fast speech Large Language Model (LLM) inference method while maintaining high audio quality. Existing audio codecs operate at high frame rates, leading to slow training and inference, especially for autoregressive models. The paper seeks to address this by proposing a low frame-rate audio codec.\n\n**Core Method**: NanoCodec is introduced, a novel state-of-the-art audio codec that operates at 12.5 frames per second (FPS) and achieves high-quality compression. The codec incorporates a fully causal model, reducing the number of autoregressive steps required to generate one second of audio, thus improving efficiency and reducing latency.\n\n**Data/Experimental Setting**: The codec was trained using"}
{"chosen": "**Problem Addressed:** Large language models (LLMs) suffer from hallucination and incorrect responses when handling complex, knowledge-intensive queries. Current retrieval-augmented generation (RAG) approaches use pre-built graphs to support reasoning but face limitations like high token cost, low-quality graphs, and inflexibility.\n\n**Core Method:** LogicRAG proposes a novel framework that dynamically extracts reasoning structures at inference time to guide retrieval without pre-built graphs. It decomposes the input query into subproblems, constructs a directed acyclic graph (DAG) to model logical dependencies, and applies topological sorting for efficient retrieval. Graph and context pruning reduce redundancy and token usage.\n\n**Data/Experimental Setting:** Evaluated on three multi-hop question-answering", "rejected": "**Problem Addressed:** The paper addresses the issue of hallucination in large language models (LLMs) when handling complex, knowledge-intensive queries. Traditional retrieval-augmented generation (RAG) methods use pre-built graphs to capture relational connections among documents, but these methods face limitations such as high token costs, inefficient knowledge retrieval, and lack of flexibility to handle varying query complexities.\n\n**Core Method:** LogicRAG proposes a framework that dynamically extracts reasoning structures at inference time to guide adaptive retrieval without relying on pre-built graphs. It decomposes the input query into subproblems, constructs a directed acyclic graph (DAG) to model logical dependencies, and uses topological sorting to linearize the graph for efficient retrieval. Additionally, it employs context and"}
{"chosen": "**Summary:**\n\n**Problem Addressed:** The study addresses the diminishing preference signals issue in existing Self-Rewarding Language Models, where the quality gap between chosen and rejected responses narrows over iterations, leading to decreased learning effectiveness.\n\n**Core Method:** Temporal Self-Rewarding Language Models are proposed to decouple the chosen and rejected responses through past-future coordination. Key components include Anchored Rejection (fixing rejected responses using past model outputs) and Future-Guided Chosen (incorporating predictions from future model versions to select chosen samples).\n\n**Data/Experimental Setting:** Experiments were conducted using models from the LLaMA, Qwen, and Mistral families with different model sizes (3B, 8B,", "rejected": "**Problem Addressed:** The paper addresses the issue of diminishing preference signals in Self-Rewarding Language Models (SRLMs), where the quality gap between chosen and rejected responses narrows over iterations, leading to vanishing gradients and poor learning.\n\n**Core Method:** The proposed Temporal Self-Rewarding Language Model (Temporal SR) decouples the chosen and rejected responses through strategic coordination of past, present, and future model generations. It uses Anchored Rejection to fix rejected responses using past model outputs and Future-Guided Chosen to select high-quality positive samples using predictions from a future model version.\n\n**Data/Experimental Setting:** The method was tested across three model families (Llama, Qwen, Mistral) and different model sizes"}
{"chosen": "This study evaluates the differential robustness of three large language models (LLMs) — Flan-T5, BERT, and RoBERTa-Base — against adversarial attacks. Using systematic adversarial tests with TextFooler and BERTAttack, the research found that RoBERTa-Base and Flan-T5 demonstrated remarkable resilience, maintaining 0% attack success rates. In contrast, BERT-Base was highly vulnerable, with a 93.75% attack success rate. The study highlights that while certain LLMs have developed effective defensive mechanisms, these often require substantial computational resources. Key quantitative results include attack success rates and robustness scores, with RoBERTa-Base achieving a perfect score of 1.", "rejected": "**Summary:**\n\nThis paper evaluates the differential robustness of three large language models (LLMs) — Flan-T5, BERT, and RoBERTa-Base — against adversarial attacks using systematic tests via TextFooler and BERTAttack. The study reveals significant differences in resilience among the models. RoBERTa-Base and Flan-T5 showed remarkable robustness, maintaining 100% accuracy even under sophisticated attacks. In contrast, BERT-Base was highly vulnerable, with TextFooler achieving a 93.75% success rate. Key quantitative results include RoBERTa-Base’s 100% attack success rate under BERTAttack and Flan-T5’s 10"}
