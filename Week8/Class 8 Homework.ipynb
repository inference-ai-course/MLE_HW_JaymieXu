{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 Assignment: Multimodal Summarization and Reward Modeling\n",
    "\n",
    "## Homework Introduction\n",
    "\n",
    "Effective summarization is critical in research because it distills large, complex documents into concise overviews that highlight key insights. Researchers often rely on summaries to quickly understand a paper’s contributions without reading every detail. However, automatically evaluating the quality of generated summaries is challenging. Traditional metrics like ROUGE and BERTScore rely on lexical overlap and can miss nuances like semantic correctness or coherence.\n",
    "\n",
    "Reward modeling offers a way to address this gap. In reinforcement learning from human feedback (RLHF), we train a reward model on examples of outputs labeled by humans. The reward model learns to predict which summary a person would prefer, serving as a proxy for human judgment. By training such a model on preference data, we can score new summaries according to human-aligned preferences, rather than just surface similarity.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "* Generate abstractive summaries of academic documents using LLaMA 3 (7B).\n",
    "* Collect two candidate summaries per paper and have annotators select the better summary.\n",
    "* Prepare the dataset of summary pairs and preference labels for reward model training.\n",
    "* Train a reward model (e.g., DeBERTa-v3) on the collected preference data.\n",
    "* Evaluate summaries using ROUGE and BERTScore, and compare these metrics to the reward model’s scores.\n",
    "\n",
    "## Project Design\n",
    "\n",
    "* **Data Collection:** Select 10 academic papers (including both text and figures) from arXiv or recent NLP conference proceedings.\n",
    "* **Summary Generation:** For each paper, use the LLaMA 3 (7B) model to generate *two* different summaries. Vary the prompting strategy or sampling parameters to produce diverse outputs.\n",
    "* **Human Annotation:** Have one or two human annotators compare each pair of summaries for a paper and choose the better one (e.g. more informative, coherent, factually consistent, etc.). Record which summary is preferred.\n",
    "* **Data Formatting:** Create a dataset (e.g. in JSONL format) of summary pairs and preference labels. Each entry should include the two summary texts and which one was chosen (for example, fields `chosen` and `rejected` as required by reward modeling tools).\n",
    "* **Reward Model Training:** Fine-tune a reward model (such as DeBERTa-v3) on this preference data. Use the chosen/rejected summary pairs so the model learns to assign higher scores to the preferred summaries.\n",
    "* **Evaluation:** Generate summaries (or summary pairs) for 10 new papers and score them using the trained reward model. Also compute ROUGE and BERTScore for these summaries.\n",
    "* **Comparison:** Analyze how the reward model’s scores align with ROUGE and BERTScore. Discuss examples where the reward model and the automatic metrics agree or disagree on which summary is better.\n",
    "\n",
    "## Starter Code\n",
    "\n",
    "* **Prompt Examples:** Prewritten prompt templates for LLaMA 3 summarization. For example: `\"Summarize the following research paper excerpt:\\n\\n[insert paper text here]\"`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Dataset Format:** Example code showing how to store summary pairs and labels. For instance, a JSONL file where each record has `\"chosen\"` and `\"rejected\"` summary fields (matching the RewardTrainer input format).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = []\n",
    "for pair in summary_pairs:\n",
    "    data.append({\n",
    "        \"chosen\": pair[\"preferred\"],\n",
    "        \"rejected\": pair[\"other\"]\n",
    "    })\n",
    "\n",
    "with open(\"reward_data.jsonl\", \"w\") as f:\n",
    "    for item in data:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Reward Training Loop:** Sample code (using Hugging Face `transformers` and `trl`) to fine-tune a reward model on the preference dataset. This should load the model (e.g. DeBERTa-v3) and train it on the chosen/rejected pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import RewardTrainer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-base\", num_labels=1)\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"reward_data.jsonl\", split=\"train\")\n",
    "\n",
    "def preprocess(example):\n",
    "    return tokenizer(example[\"chosen\"], example[\"rejected\"], truncation=True, padding=\"max_length\")\n",
    "\n",
    "dataset = dataset.map(preprocess, batched=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"reward_model\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Evaluation Script:** Example code to compute ROUGE and BERTScore (using the `evaluate` library) and to run the reward model scoring on a batch of summaries. The script can output metric scores and compare reward model rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "rouge = load(\"rouge\")\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "results_rouge = rouge.compute(predictions=generated_summaries, references=reference_summaries)\n",
    "results_bertscore = bertscore.compute(predictions=generated_summaries, references=reference_summaries, lang=\"en\")\n",
    "\n",
    "print(\"ROUGE:\", results_rouge)\n",
    "print(\"BERTScore:\", results_bertscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Environment Setup\n",
    "\n",
    "* Install required Python libraries: `transformers`, `datasets`, `evaluate`, `trl` (Hugging Face TRL), and `accelerate`.\n",
    "* (Optional) Install `peft` if you want to use parameter-efficient fine-tuning for the reward model.\n",
    "* Ensure you have GPU access for model training (e.g., use Google Colab Pro, AWS, or a local GPU).\n",
    "* Download or load the LLaMA 3 (7B) model checkpoint and a DeBERTa-v3 checkpoint (for example, via Hugging Face Hub).\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "* A JSONL file containing 20 summary pairs with preference labels (the dataset of chosen/rejected summaries).\n",
    "* The fine-tuned reward model weights (saved model file).\n",
    "* An evaluation notebook (or script) that computes ROUGE and BERTScore on the summaries and compares them to the reward model’s scores/rankings.\n",
    "\n",
    "## Exploration Tips\n",
    "\n",
    "* Experiment with alternative models if resources allow. For example, try Mixtral-8x7B (a Mixture-of-Experts LLM) or the DeepSeek-VL vision-language model for summarization. Compare their outputs.\n",
    "* Incorporate structured content into the prompts: e.g. include figure captions or table content when generating summaries to make the task truly multimodal.\n",
    "* Compare summaries on qualitative criteria (factual consistency, conciseness, readability, etc.) and see how these aspects correlate with the numeric scores from ROUGE/BERTScore and from the reward model.\n",
    "\n",
    "**Sources:** Summarization is often used to reduce long inputs and highlight key points. Evaluating summary quality is a known open challenge due to subjective references and aspects like coherence that metrics may miss. Reward modeling (from RLHF) involves training a model on human preference data so it can align generation with human judgments.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
