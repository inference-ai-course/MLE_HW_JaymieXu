{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60bc6e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.10/site-packages (4.56.0)\n",
      "Requirement already satisfied: accelerate in ./.local/lib/python3.10/site-packages (1.10.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.local/lib/python3.10/site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.4.28)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.local/lib/python3.10/site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.10/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.4.0a0+07cecf4168.nv24.5)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6f27ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b62fe03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float16 if device == \"mps\" else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67027143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836f79df0a2248daac21253e85821bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is Jaymie Xu? Jaymie Xu, also known as Jaymie Xue or Jaymie, is a Chinese-American singer-songwriter and musician. She gained significant attention in the music industry for her unique blend of pop, indie, and electronic genres.\n",
      "\n",
      "Key points about Jaymie Xu:\n",
      "\n",
      "1. Born on July 27, 1994, in Shanghai, China, she moved to the United States with her family when she was young.\n",
      "2. She began her musical journey at a young age, learning piano and violin.\n",
      "3. Jaymie released her debut EP \"Shine\" in 2016, which gained popularity through various online platforms.\n",
      "4. Her music often reflects themes of self-discovery, relationships, and personal growth.\n",
      "5. In addition to singing, Jaymie also plays multiple instruments, including piano, guitar, and drums.\n",
      "6. She has collaborated with other artists in the music industry, both locally and internationally.\n",
      "7. Jaymie has performed at various events and festivals, including SXSW (South by Southwest) and Coachella.\n",
      "8. Her style has been described as eclectic, blending traditional Chinese elements with modern pop sounds.\n",
      "\n",
      "Jaymie's career has seen her grow from an internet sensation to a recognized figure in the music scene, particularly among younger\n"
     ]
    }
   ],
   "source": [
    "ask_llm = pipeline(\n",
    "  task=\"text-generation\",\n",
    "  model=\"Qwen/Qwen2.5-3B-Instruct\",\n",
    "  device=device,\n",
    "  torch_dtype=dtype\n",
    ")\n",
    "\n",
    "print(ask_llm(\"Who is Jaymie Xu?\")[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145dc2e4",
   "metadata": {},
   "source": [
    "If only I were that successful lol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "869c27e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'completion'],\n",
       "        num_rows: 90\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data \n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_data = load_dataset('json', data_files = \"jaymie_xu_resume_train.json\")\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3e9dc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'What is Jaymie Xuâ€™s profession?',\n",
       " 'completion': 'Engine Programmer.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f13cf6e",
   "metadata": {},
   "source": [
    "As you can see, here we return with the long text, but for fine-tuning we need the data to be small and precise chunks, more like here we apply the tokenization to take the text and split it into smaller chunks. Each chunk is called a token and it the smallest unit of meaning that LLMs work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da03887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\"\n",
    ")\n",
    "def preprocess(sample):\n",
    "    sample = sample['prompt']+ '\\n' + sample['completion']\n",
    "    print(sample)\n",
    "    tokenized = tokenizer(\n",
    "        sample,\n",
    "        max_length = 128,\n",
    "        truncation = True,\n",
    "        padding = \"max_length\"    \n",
    "    )\n",
    "\n",
    "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "    return tokenized\n",
    "data = raw_data.map(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6aae75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'completion', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 90\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(data['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436ab97f",
   "metadata": {},
   "source": [
    "## LoRA\n",
    "\n",
    "now, let's move into the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90228ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7c30e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d1d59f51b1b4b68bb92796c865c98f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    device_map = device,\n",
    "    torch_dtype = torch.float16\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig (\n",
    "    \n",
    "    task_type = TaskType.CAUSAL_LM, \n",
    "    target_modules=['q_proj', \"k_proj\", \"v_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33200b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    num_train_epochs = 10, # we will go throught the dataset from start to finish 10 times\n",
    "    learning_rate=0.001, \n",
    "    logging_steps = 25, # we want to see the result in every 25 steps it runs \n",
    "    fp16 = False # float point set to 16 to speed it up, set to \"True\" if you are on GPU\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    args = train_args,\n",
    "    model = model, \n",
    "    train_dataset=data[\"train\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b17f16de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 00:37, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.461700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.336700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.257800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=120, training_loss=0.7455982049306233, metrics={'train_runtime': 39.8666, 'train_samples_per_second': 22.575, 'train_steps_per_second': 3.01, 'total_flos': 1919656289894400.0, 'train_loss': 0.7455982049306233, 'epoch': 10.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed1aafcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./my-qwen/tokenizer_config.json',\n",
       " './my-qwen/special_tokens_map.json',\n",
       " './my-qwen/chat_template.jinja',\n",
       " './my-qwen/vocab.json',\n",
       " './my-qwen/merges.txt',\n",
       " './my-qwen/added_tokens.json',\n",
       " './my-qwen/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "trainer.save_model(\"./my-qwen\")\n",
    "tokenizer.save_pretrained(\"./my-qwen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7449af4",
   "metadata": {},
   "source": [
    "Now let's test it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71e8b4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615b626ba592474d8754d4d26d8e7fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is Jaymie Xu? An engine programming specialist at Ubisoft.\n"
     ]
    }
   ],
   "source": [
    "ask_llm = pipeline(\n",
    "  task=\"text-generation\",\n",
    "  model=\"./my-qwen\",\n",
    "  tokenizer='./my-qwen',\n",
    "  device=device,\n",
    "  torch_dtype=dtype\n",
    ")\n",
    "\n",
    "print(ask_llm(\"Who is Jaymie Xu?\")[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e322d0e",
   "metadata": {},
   "source": [
    "Well close enough. not anymore now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
