{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 Assignment: Extending the Voice Agent with Function Calling.\n",
    "This week, we dive into a crucial capability of modern AI agents: function calling. Real-world agents are not limited to generating textâ€”they interact with tools, run computations, query databases, and more. In this assignment, you will extend the multi-turn voice agent you built in Week 3 with the ability to automatically execute tools based on natural language commands.\n",
    "\n",
    "Using Llama 3 as your core LLM, youâ€™ll teach the model to recognize when a user wants to search arXiv papers or perform a math calculation, and respond by outputting a structured function call. Your agent will then parse that function call, execute the appropriate tool, and return the result to the user via text-to-speech. This is a major step toward building a fully autonomous research assistant that can act on intentâ€”not just reply with facts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "* **Function Calling with LLMs:** Learn how to use function/tool calling by prompting LlamaÂ 3 to output structured calls (e.g., JSON) for external functions.\n",
    "* **Intent Parsing and Tool Mapping:** Practice parsing user queries to determine the intent (e.g., search a database or perform a calculation) and mapping that intent to specific tool functions like `search_arxiv(query)` and `calculate(expression)`.\n",
    "* **Integrating Tools into the Agent:** Extend the WeekÂ 3 multi-turn voice agent pipeline (ASR â†’ LLM â†’ TTS) so that the LLM can trigger code execution. The agent should automatically call the right function based on the LLMâ€™s output, and then speak the returned result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Project Design\n",
    "\n",
    "Reuse the voice-chat pipeline from WeekÂ 3 and enhance the LLM step to support calling external tools. The key tasks include:\n",
    "\n",
    "* **Tool Functions:** Implement two helper functions:\n",
    "\n",
    "  * `search_arxiv(query: str) -> str`: Simulates or performs an arXiv search and returns a relevant passage or summary for the query.\n",
    "  * `calculate(expression: str) -> str`: Evaluates a mathematical expression (using `sympy` or `eval`) and returns the result as text.\n",
    "\n",
    "* **Prompt Engineering:** Modify the LlamaÂ 3 system/user prompts so the model knows to generate structured function-call outputs when appropriate. For example, instruct it that if the userâ€™s question can be answered by searching arXiv or doing math, it should output a JSON-like call, such as:\n",
    "\n",
    "  ```json\n",
    "  {\"function\": \"calculate\", \"arguments\": {\"expression\": \"2+2\"}}\n",
    "  ```\n",
    "\n",
    "  or\n",
    "\n",
    "  ```json\n",
    "  {\"function\": \"search_arxiv\", \"arguments\": {\"query\": \"quantum entanglement\"}}\n",
    "  ```\n",
    "\n",
    "  Otherwise it should respond normally in text.\n",
    "\n",
    "* **Detecting and Calling Tools:** After the LLM generates a response, check if it is a function call. Parse the JSON output from the LLM to extract the function name and arguments. If it is a call, invoke the corresponding Python function (`search_arxiv` or `calculate`) with those arguments and capture its result. Use this result as the assistantâ€™s reply (to be spoken by TTS). If the LLM output is normal text, just use it as the assistantâ€™s response without calling any function.\n",
    "\n",
    "* **Fallback Behavior:** Ensure the voice agent handles all cases. If the LLMâ€™s output cannot be parsed as a function call (or if the named function is unknown), fall back to replying with a standard text response or an error message as appropriate.\n",
    "\n",
    "## Starter Code\n",
    "\n",
    "We provide snippets to help you get started:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function tool stubs (starter implementations)\n",
    "def search_arxiv(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Simulate an arXiv search or return a dummy passage for the given query.\n",
    "    In a real system, this might query the arXiv API and extract a summary.\n",
    "    \"\"\"\n",
    "    # Example placeholder implementation:\n",
    "    return f\"[arXiv snippet related to '{query}']\"\n",
    "\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"\n",
    "    Evaluate a mathematical expression and return the result as a string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from sympy import sympify\n",
    "        result = sympify(expression)  # use sympy for safe evaluation\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dialogue engine: function-routing logic\n",
    "import json\n",
    "\n",
    "def route_llm_output(llm_output: str) -> str:\n",
    "    \"\"\"\n",
    "    Route LLM response to the correct tool if it's a function call, else return the text.\n",
    "    Expects LLM output in JSON format like {'function': ..., 'arguments': {...}}.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        output = json.loads(llm_output)\n",
    "        func_name = output.get(\"function\")\n",
    "        args = output.get(\"arguments\", {})\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        # Not a JSON function call; return the text directly\n",
    "        return llm_output\n",
    "\n",
    "    if func_name == \"search_arxiv\":\n",
    "        query = args.get(\"query\", \"\")\n",
    "        return search_arxiv(query)\n",
    "    elif func_name == \"calculate\":\n",
    "        expr = args.get(\"expression\", \"\")\n",
    "        return calculate(expr)\n",
    "    else:\n",
    "        return f\"Error: Unknown function '{func_name}'\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example FastAPI endpoint (sketch)\n",
    "from fastapi import FastAPI\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/api/voice-query/\")\n",
    "async def voice_query_endpoint(request: dict):\n",
    "    # Assume request has 'text': the user's query string\n",
    "    user_text = request.get(\"text\", \"\")\n",
    "    # Call Llama 3 model (instructed to output function calls when needed)\n",
    "    llm_response = llama3_chat_model(user_text)\n",
    "    # Process LLM output and possibly call tools\n",
    "    reply_text = route_llm_output(llm_response)\n",
    "    # Convert reply_text to speech (TTS) and return it\n",
    "    return {\"response\": reply_text}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The above code outlines where to plug in your LLM call and audio I/O. Integrate the function-calling logic into your existing voice agent framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Deliverables\n",
    "\n",
    "* **Codebase:** Submit your updated voice agent code with function calling integration. Document any new modules or changes clearly.\n",
    "* **Test Logs:** Provide sample logs for at least three queries, showing:\n",
    "\n",
    "  1. The userâ€™s query text.\n",
    "  2. The raw LLM response (JSON function call or normal text).\n",
    "  3. Any function call made and its output.\n",
    "  4. The final assistant response.\n",
    "* **Demo Video:** A 1â€“2 minute demo of the voice agent. Show the agent handling:\n",
    "\n",
    "  * A math query (invoking `calculate`).\n",
    "  * An arXiv search query (invoking `search_arxiv`).\n",
    "  * A normal query (no function call).\n",
    "\n",
    "## Exploration Tips\n",
    "\n",
    "* **Extend Tools:** Try adding new tools (e.g. a weather lookup or translation). Define their function signatures and integrate them into your agent.\n",
    "* **Tool Registry:** Create a dictionary or registry of function names to callables to simplify routing logic when you have multiple tools.\n",
    "* **Other LLMs:** Experiment with other models that support function calling (e.g. GPT-4 with its function calling API). Compare how their output format and reliability differ from LlamaÂ 3.\n",
    "* **Error Handling:** Make sure your agent handles invalid inputs gracefully (e.g. a malformed math expression should not crash the agent).\n",
    "* **Chained Calls (Advanced):** As a challenge, allow the agent to use one toolâ€™s output as context for another. For example, it could `search_arxiv` for a value and then `calculate` something with that value.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
